diff --git a/configure-libc-partial-order-debug.sh b/configure-libc-partial-order-debug.sh
new file mode 100755
index 0000000000..163e153b2a
--- /dev/null
+++ b/configure-libc-partial-order-debug.sh
@@ -0,0 +1 @@
+CFLAGS="-Wno-maybe-uninitialized -fno-stack-protector -O1 -ggdb -fno-omit-frame-pointer -DMVEE_USE_TOTALPARTIAL_AGENT" ../configure --enable-stackguard-randomization --enable-obsolete-rpc --enable-pt_chown --with-selinux --enable-lock-elision=no --enable-addons=nptl --prefix=$HOME/glibc-build  --sysconfdir=/etc/
diff --git a/configure-libc-woc.sh b/configure-libc-woc.sh
new file mode 100755
index 0000000000..4b8814d585
--- /dev/null
+++ b/configure-libc-woc.sh
@@ -0,0 +1 @@
+../configure --enable-stackguard-randomization --enable-obsolete-rpc --enable-pt_chown --with-selinux --enable-lock-elision=no --enable-addons=nptl --prefix=$HOME/glibc-build --sysconfdir=$HOME/glibc-build/etc/
diff --git a/csu/Versions b/csu/Versions
index 43010c3443..4ece9632fe 100644
--- a/csu/Versions
+++ b/csu/Versions
@@ -2,6 +2,15 @@ libc {
   GLIBC_2.0 {
     # helper functions
     __libc_init_first; __libc_start_main;
+    mvee_atomic_preop;
+    mvee_atomic_postop;
+    mvee_atomic_preop_internal;
+    mvee_atomic_postop_internal;
+    mvee_invalidate_buffer;
+    mvee_all_heaps_aligned;
+    mvee_should_sync_tid;
+    mvee_should_futex_unlock;
+	mvee_xcheck;
   }
   GLIBC_2.1 {
     # New special glibc functions.
diff --git a/csu/libc-start.c b/csu/libc-start.c
index 12468c5a89..9f6d55ee57 100644
--- a/csu/libc-start.c
+++ b/csu/libc-start.c
@@ -108,6 +108,14 @@ apply_irel (void)
 
 #include <libc-start.h>
 
+#ifdef USE_MVEE_LIBC
+# ifdef MVEE_USE_TOTALPARTIAL_AGENT
+#  include "mvee-totalpartial-agent.c"
+# else
+#  include "mvee-woc-agent.c"
+# endif
+#endif
+
 STATIC int LIBC_START_MAIN (int (*main) (int, char **, char **
 					 MAIN_AUXVEC_DECL),
 			    int argc,
@@ -287,6 +295,11 @@ LIBC_START_MAIN (int (*main) (int, char **, char ** MAIN_AUXVEC_DECL),
 #ifndef SHARED
   _dl_debug_initialize (0, LM_ID_BASE);
 #endif
+
+  (void) syscall(MVEE_RUNS_UNDER_MVEE_CONTROL, &mvee_sync_enabled, &mvee_infinite_loop, 
+				 NULL, NULL, &mvee_master_variant);
+  mvee_libc_initialized = 1;
+
 #ifdef HAVE_CLEANUP_JMP_BUF
   /* Memory for the cancellation buffer.  */
   struct pthread_unwind_buf unwind_buf;
diff --git a/csu/mvee-totalpartial-agent.c b/csu/mvee-totalpartial-agent.c
new file mode 100644
index 0000000000..a80ae2a58f
--- /dev/null
+++ b/csu/mvee-totalpartial-agent.c
@@ -0,0 +1,630 @@
+static volatile unsigned int          mvee_lock_owner               = 0;
+static unsigned char                  mvee_sync_enabled             = 0;
+static unsigned char                  mvee_libc_initialized         = 0;
+static unsigned char                  mvee_master_variant           = 0;
+static unsigned char                  mvee_buffer_valid             = 0;
+static unsigned short                 mvee_num_variants             = 0;
+static unsigned short                 mvee_my_variant_num           = 0;
+static struct mvee_buffer_info*       mvee_lock_buffer_info         = NULL;
+static struct mvee_buffer_entry*      mvee_lock_buffer              = NULL;
+static struct mvee_callstack_entry*   mvee_callstack_buffer         = NULL;
+static __thread unsigned int          mvee_master_thread_id         = 0;
+static __thread unsigned long         mvee_prev_flush_cnt           = 0;
+static __thread unsigned long         mvee_lock_buffer_prev_pos     = 0;
+#ifdef MVEE_CHECK_LOCK_TYPE
+static __thread unsigned long         mvee_original_call_site       = 0;
+#define INLINEIFNODEBUG __attribute__((noinline))
+#else
+#define INLINEIFNODEBUG inline
+#endif
+
+
+#define likely(x)       __builtin_expect((x),1)
+#define unlikely(x)     __builtin_expect((x),0)
+
+extern void mvee_infinite_loop(void);
+
+// ========================================================================================================================
+// INITIALIZATION FUNCS
+// ========================================================================================================================
+
+static void mvee_check_buffer(void)
+{
+	if (unlikely(!mvee_master_thread_id))
+    {
+		mvee_master_thread_id = syscall(MVEE_GET_MASTERTHREAD_ID);
+
+		if (!mvee_buffer_valid)
+		{
+			mvee_buffer_valid = 1;
+
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+			unsigned long queue_ident = MVEE_LIBC_LOCK_BUFFER_PARTIAL;
+#else
+			unsigned long queue_ident = MVEE_LIBC_LOCK_BUFFER;
+#endif
+
+			// ask the MVEE to allocate the lock buffer
+			unsigned long slots = 0;
+			long tmp_id = syscall(MVEE_GET_SHARED_BUFFER, 0, queue_ident, &slots, sizeof(struct mvee_buffer_entry));
+
+			// we use some of the slots for buffer_info entries
+			slots       = (slots - mvee_num_variants * 64) / sizeof(struct mvee_buffer_entry) - 2;
+			
+			// Attach to the buffer
+			void* tmp_buffer      = (void*)syscall(__NR_shmat, tmp_id, NULL, 0);
+			mvee_lock_buffer_info = ((struct mvee_buffer_info*) tmp_buffer) + mvee_my_variant_num;
+			mvee_lock_buffer      = ((struct mvee_buffer_entry*) tmp_buffer) + mvee_num_variants;
+			mvee_lock_buffer_info->lock = 1;			
+			mvee_lock_buffer_info->size = slots;
+			mvee_lock_buffer_info->buffer_type = queue_ident;
+
+#ifdef MVEE_LOG_EIPS			
+			long callstack_buffer_id = syscall(MVEE_GET_SHARED_BUFFER, 1, queue_ident, NULL,
+											   mvee_num_variants * sizeof(struct mvee_callstack_entry),
+											   MVEE_STACK_DEPTH);
+			mvee_callstack_buffer = (struct mvee_callstack_entry*) syscall(__NR_shmat, callstack_buffer_id, NULL, 0);
+#endif
+		}
+    }
+}
+
+
+static INLINEIFNODEBUG int mvee_should_sync(void)
+{
+	if (unlikely(!mvee_libc_initialized))
+	{
+		long res = syscall(MVEE_RUNS_UNDER_MVEE_CONTROL, &mvee_sync_enabled, &mvee_infinite_loop, 
+						   &mvee_num_variants, &mvee_my_variant_num, &mvee_master_variant);
+		if (!(res < 0 && res > -4095))
+			mvee_check_buffer();
+		mvee_libc_initialized = 1;
+	}
+	return mvee_sync_enabled;
+}
+
+#define cpu_relax() asm volatile("rep; nop" ::: "memory")
+
+#define gcc_barrier() asm volatile("" ::: "memory")
+
+/*
+ * logs a (truncated) stack into the specified "eip" buffer. This stack is logged for EVERY variant,
+ * which greatly facilitates debugging. The MVEE can dump the contents of this buffer very efficiently, 
+ * including source lines/info
+ * 
+ * Do note that __builtin_return_address() can trap (SEGV) if the entire stack contains less than
+ * MVEE_MAX_STACK_DEPTH entries. The MVEE will however trap the SEGV and if it comes from this function,
+ * the offending instruction will be skipped.
+ */
+static void __attribute__ ((noinline)) mvee_log_stack(unsigned int pos, int start_depth)
+{
+#ifdef MVEE_LOG_EIPS
+	int entries_logged = 0;
+	int next_entry = start_depth;
+
+	// make sure that word_ptr and op_type are visible before we log the stack
+	__sync_synchronize();
+
+	while (entries_logged < MVEE_STACK_DEPTH)
+    {
+		unsigned long ret_addr = 0;
+		switch (next_entry)
+		{
+			// __builtin_* intrinics need const arguments so unfortunately, we have to do the following...
+#define DEF_CASE(x)														\
+			case x:														\
+				ret_addr = (unsigned long)__builtin_return_address(x);	\
+				break;
+			DEF_CASE(0);
+#ifdef MVEE_CHECK_LOCK_TYPE
+			case 1:
+				ret_addr = (unsigned long)mvee_original_call_site;
+				break;
+#else
+			DEF_CASE(1);
+#endif
+
+			DEF_CASE(2);
+			DEF_CASE(3);
+			DEF_CASE(4);
+			DEF_CASE(5);
+			DEF_CASE(6);
+			DEF_CASE(7);
+			DEF_CASE(8);
+			DEF_CASE(9);
+			DEF_CASE(10);
+		}
+		
+		mvee_callstack_buffer[pos * mvee_num_variants + mvee_my_variant_num].callee[entries_logged++] = ret_addr;
+		next_entry++;
+    }
+#endif
+}
+
+// ========================================================================================================================
+// ASSERTIONS
+// ========================================================================================================================
+
+static INLINEIFNODEBUG void mvee_assert_lock_not_owned(void)
+{
+#ifdef MVEE_CHECK_LOCK_TYPE
+	if (mvee_lock_owner)
+		*(volatile long*)0 = mvee_lock_owner;
+#endif
+}
+
+static INLINEIFNODEBUG void mvee_assert_lock_owned(void)
+{
+#ifdef MVEE_CHECK_LOCK_TYPE
+	if (!mvee_lock_owner || 
+		mvee_lock_owner != mvee_master_thread_id)
+	{
+		*(volatile long*)0 = mvee_lock_owner;
+	}
+#endif
+}
+
+static INLINEIFNODEBUG void mvee_assert_operation_matches
+(
+	unsigned int pos, 
+	unsigned long slave_word_ptr, 
+	unsigned short slave_op_type
+)
+{
+#ifdef MVEE_CHECK_LOCK_TYPE
+	unsigned long master_word_ptr = mvee_lock_buffer[pos].word_ptr;
+	unsigned short master_op_type = mvee_lock_buffer[pos].operation_type;
+
+	if ((master_word_ptr & 0xfff) != (slave_word_ptr & 0xfff))
+	{
+		mvee_log_stack(pos, 1);
+		syscall(__NR_gettid, 1337, 10000001, 61, slave_word_ptr, pos);
+	}
+	
+	if (master_op_type != slave_op_type)
+	{
+		mvee_log_stack(pos, 1);
+		syscall(__NR_gettid, 1337, 10000001, 60, 0, pos);
+		syscall(__NR_gettid, 1337, 10000001, 59, master_op_type, slave_op_type);
+	}
+#endif
+}
+
+static INLINEIFNODEBUG void mvee_assert_at_end_of_buffer(unsigned int pos)
+{
+	if (pos != mvee_lock_buffer_info->size)
+		*(volatile int*) 0 = 0x0bad1dea;
+}
+
+static INLINEIFNODEBUG void mvee_assert_same_callee(unsigned int pos)
+{
+// check call site, should be ASLR proof
+#if defined(MVEE_CHECK_LOCK_TYPE) && defined(MVEE_LOG_EIPS)
+	unsigned long parent_eip = mvee_callstack_buffer[pos * mvee_num_variants + mvee_my_variant_num].callee[0];
+	unsigned long our_eip = (unsigned long)mvee_original_call_site;
+
+	if ((parent_eip & 0xfff) != (our_eip & 0xfff))
+		syscall(__NR_gettid, 1337, 10000001, 90, mvee_lock_buffer[pos].operation_type, parent_eip, our_eip);
+#endif
+}
+
+// ========================================================================================================================
+// MASTER LOGIC
+// ========================================================================================================================
+
+static INLINEIFNODEBUG unsigned int mvee_write_lock_result_prepare(void)
+{
+	while (1)
+    {
+		if (orig_atomic_decrement_and_test(&mvee_lock_buffer_info->lock))
+		{
+			mvee_assert_lock_not_owned();
+			mvee_lock_owner = mvee_master_thread_id;
+			return mvee_lock_buffer_info->pos;
+		}
+      
+		while (mvee_lock_buffer_info->lock <= 0)
+			cpu_relax();
+    }
+
+	// unreachable
+	return mvee_lock_buffer_info->pos;
+}
+
+static INLINEIFNODEBUG void mvee_write_lock_result_finish(void)
+{
+	mvee_assert_lock_owned();
+	mvee_lock_owner = 0;
+	orig_atomic_increment(&mvee_lock_buffer_info->pos);
+	mvee_lock_buffer_info->lock = 1;
+}
+
+static INLINEIFNODEBUG void mvee_lock_buffer_flush(void)
+{
+	mvee_lock_buffer_info->flushing = 1;
+	atomic_full_barrier();
+
+	syscall(MVEE_FLUSH_SHARED_BUFFER, mvee_lock_buffer_info->buffer_type);
+	mvee_lock_buffer_info->pos = 0;
+	atomic_full_barrier();
+
+	mvee_lock_buffer_info->flush_cnt = (++mvee_prev_flush_cnt);
+	mvee_lock_buffer_info->flushing = 0;
+}
+
+static INLINEIFNODEBUG void mvee_write_lock_result_write(unsigned int pos, unsigned short op_type, void* word_ptr)
+{
+	while (1)
+	{
+		if (likely(pos < mvee_lock_buffer_info->size))
+		{
+			mvee_lock_buffer[pos].word_ptr = (unsigned long) word_ptr;
+			mvee_lock_buffer[pos].operation_type = op_type;
+
+#ifdef MVEE_CHECK_LOCK_TYPE
+			if (mvee_lock_owner != mvee_master_thread_id)
+				*(volatile long*)0 = mvee_lock_owner;
+#endif
+
+			mvee_log_stack(pos, 1);
+
+			// This must be stored last. The slave assumes that when
+			// master_thread_id becomes non-zero, the word_ptr and operation_type
+			// fields are valid too
+			mvee_lock_buffer[pos].master_thread_id = mvee_master_thread_id;
+			break;
+		}
+		else
+		{      
+			// we log the tid of the flushing thread into the last slot
+			mvee_lock_buffer[pos].master_thread_id = mvee_master_thread_id;
+			mvee_lock_buffer_flush();
+			pos = 0;
+		}
+	}
+}
+
+// ========================================================================================================================
+// SLAVE LOGIC
+// ========================================================================================================================
+
+static INLINEIFNODEBUG unsigned char mvee_op_is_tagged(unsigned long pos)
+{
+	return mvee_lock_buffer[pos].tags[mvee_my_variant_num];
+}
+
+static INLINEIFNODEBUG unsigned char mvee_pos_still_valid(void)
+{
+	if (mvee_lock_buffer_info->flushing || 
+		mvee_lock_buffer_info->flush_cnt != mvee_prev_flush_cnt)
+		return 0;
+	return 1;
+}
+
+//
+// Wait until all relevant replication operations in the range [start_pos, end_pos[
+// have been tagged.
+// 
+// if @wait_for_all_ops == 1, this waits until _ALL_ operations get tagged
+// if @wait_for_all_ops == 0, this waits until all operations on the same 
+// current_word_ptr get tagged
+//
+static INLINEIFNODEBUG void mvee_wait_for_preceding_ops
+(
+	unsigned int start_pos, 
+	unsigned int end_pos, 
+	unsigned char wait_for_all_ops, 
+	unsigned long current_word_ptr
+)
+{
+	unsigned char all_tagged;
+	unsigned int temppos;
+
+	while (true)
+	{
+		all_tagged = 1;
+
+		for (temppos = start_pos; temppos < end_pos; ++temppos)
+		{		    
+			unsigned long word_ptr = mvee_lock_buffer[temppos].word_ptr;
+
+			if (!word_ptr ||
+				(!mvee_op_is_tagged(temppos) && (wait_for_all_ops || word_ptr == current_word_ptr)))
+			{
+				all_tagged = 0;
+				start_pos = temppos;
+				break;
+			}
+		}
+
+		if (all_tagged)
+		{
+			mvee_lock_buffer_prev_pos = temppos;
+			break;
+		}
+		
+		syscall(__NR_sched_yield);
+	}
+}
+
+static INLINEIFNODEBUG void mvee_read_lock_result_wait(unsigned short op_type, void* word_ptr)
+{
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+	unsigned long master_word_ptr;
+	unsigned int start_pos = 0;
+	unsigned int current_pos = mvee_lock_buffer_info->pos;
+
+	if (mvee_lock_buffer_prev_pos > current_pos)
+		start_pos = mvee_lock_buffer_prev_pos;
+	else
+		start_pos = current_pos;
+
+	while(true)
+	{
+		// STEP 1: FIND THE CORRESPONDING MASTER WORD POINTER
+		//
+		// Our algorithm relies on the fact that all variants are semantically
+		// equivalent. If the first non-replicated operation we find is an
+		// operation on address &A, then we know that address &A in the master
+		// is equivalent to address <word_ptr> in the slave.
+		//
+		// Caching: We start the search one item beyond the previously
+		// replicated operation by this thread. The index of the previously
+		// replicated operation is stored in mvee_lock_buffer_prev_pos.  It
+		// remains valid until the queue is flushed.  When the queue is flushed,
+		// we increment the flush counter.
+		//
+
+		// check if the queue has been flushed since the last operation we've replicated
+		while (unlikely(!mvee_pos_still_valid()))
+		{
+			// it has been flushed, update flush cnt and reset the 
+			// position of our previously replicated operation
+			mvee_prev_flush_cnt = mvee_lock_buffer_info->flush_cnt;
+			mvee_lock_buffer_prev_pos = start_pos = 0;
+			syscall(__NR_sched_yield);
+		}
+
+		master_word_ptr = 0;
+
+		for (current_pos = start_pos; current_pos <= mvee_lock_buffer_info->size; ++current_pos)
+		{
+			unsigned int tid = mvee_lock_buffer[current_pos].master_thread_id;
+
+			// no tid => the slaves are running ahead of the master
+			// or the master stores are not visible yet
+			if (!tid)
+			{
+				break;
+			}
+			else if (tid != mvee_master_thread_id || 
+					 mvee_op_is_tagged(current_pos))
+			{
+				continue;
+			}
+
+			// if tid is visible, then this will be too
+			master_word_ptr = mvee_lock_buffer[current_pos].word_ptr;
+
+			if (mvee_pos_still_valid())
+			{				
+				// this will only happen if we're at the end of the queue
+				// at which point we log a pseudo-operation with master_word_ptr == 0
+				if (!master_word_ptr)
+				{
+					// assert that we're at the end of the buffer			   
+					mvee_assert_at_end_of_buffer(current_pos);
+					// wait for everything before us to complete
+					mvee_wait_for_preceding_ops(start_pos, mvee_lock_buffer_info->size, 1, 0);
+					mvee_lock_buffer_flush();
+					current_pos = mvee_lock_buffer_prev_pos = 0;
+					break;
+				}
+				else
+				{					
+					// We found a non-replicated operation for our thread at position
+					// <current_pos>. If the buffer has not been flushed between the moment
+					// we determined the start position of our search and the moment
+					// we found the non-replicated operation, we can now safely assume that
+					// it is not going to be flushed.
+					mvee_assert_operation_matches(current_pos, (unsigned long) word_ptr, op_type);
+					break;
+				}
+			}
+			else
+			{
+				// Some other thread flushed the buffer while we were searching for the
+				// next non-replicated position.
+				master_word_ptr = 0;
+				current_pos = 0;
+				break;
+			}
+		}
+
+		if (master_word_ptr)
+			break;
+
+		// if we get to this point, it means that the slave 
+		// has caught up with the master
+		// => we restart the iteration but this time
+		// we start at the position we were at
+		start_pos = current_pos;
+		syscall(__NR_sched_yield);
+	}
+
+	// STEP 2: FIND PRECEDING OPERATIONS ON THIS LOCATION
+	mvee_wait_for_preceding_ops(mvee_lock_buffer_info->pos, current_pos, 0, master_word_ptr);
+	mvee_log_stack(current_pos, 1);
+
+#else // MVEE_TOTAL_ORDER_REPLICATION
+
+	// Super simple compared to the partial order agent. Just wait until
+	// the operation poited to by buffer_info->pos is for our thread...
+	while (true)
+    {
+		int current_pos = mvee_lock_buffer_info->pos;
+      
+		if (current_pos < mvee_lock_buffer_info->size)
+		{
+			if (mvee_lock_buffer[current_pos].master_thread_id == mvee_master_thread_id)
+			{
+				mvee_assert_operation_matches(current_pos, (unsigned long)word_ptr, op_type);
+				mvee_log_stack(current_pos, 1);
+				break;
+			}
+
+			syscall(__NR_sched_yield);
+		}
+		else
+		{
+			unsigned int tid = mvee_lock_buffer[current_pos].master_thread_id;
+
+			// we have to flush... figure out which thread does the flush
+			if (tid == mvee_master_thread_id)
+			{
+				mvee_lock_buffer_flush();
+			}
+			else
+			{
+				while (mvee_lock_buffer_info->pos == mvee_lock_buffer_info->size &&
+					   mvee_lock_buffer[current_pos].master_thread_id != mvee_master_thread_id)
+				{
+					syscall(__NR_sched_yield);
+				}
+			}
+		}
+    }
+
+#endif
+}
+
+static INLINEIFNODEBUG void mvee_read_lock_result_wake(void)
+{
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+	mvee_assert_same_callee(mvee_lock_buffer_prev_pos);
+#else
+	mvee_assert_same_callee(mvee_lock_buffer_info->pos);
+#endif
+
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+	
+	// Check if we can increase the pos so other threads get a smaller scan window
+	// Don't try to move beyond prev_pos. Otherwise, the buffer might get flushed while
+	// we're tagging our completed operation
+	for (unsigned int i = mvee_lock_buffer_info->pos; i < mvee_lock_buffer_prev_pos; ++i)
+    {
+		if (!mvee_op_is_tagged(i))
+		{
+			orig_atomic_max(&mvee_lock_buffer_info->pos, i);
+			break;
+		}
+	}
+  
+	// tag this slot
+	mvee_lock_buffer[mvee_lock_buffer_prev_pos].tags[mvee_my_variant_num] = 1;
+	
+	// make sure that our thread starts from prev_pos + 1 next time
+	mvee_lock_buffer_prev_pos++;
+
+#else // MVEE_TOTAL_ORDER_REPLICATION
+
+    mvee_lock_buffer_info->pos++;
+
+#endif // !MVEE_PARTIAL_ORDER_REPLICATION
+}
+
+// ========================================================================================================================
+// EXTERNAL APIS
+// ========================================================================================================================
+
+unsigned char mvee_atomic_preop_internal(unsigned short op_type, void* word_ptr)
+{
+	if (unlikely(!mvee_should_sync()))
+		return 0;
+	mvee_check_buffer();
+#ifdef MVEE_CHECK_LOCK_TYPE
+	if (!mvee_original_call_site)
+		mvee_original_call_site = (unsigned long)__builtin_return_address(0);
+#endif
+	if (likely(mvee_master_variant))
+    {
+		unsigned int pos = mvee_write_lock_result_prepare();
+		mvee_write_lock_result_write(pos, op_type, word_ptr);
+		return 1;
+    }
+	else
+    {
+		mvee_read_lock_result_wait(op_type, word_ptr);
+		return 2;
+    }
+}
+
+void mvee_atomic_postop_internal(unsigned char preop_result)
+{
+	if(likely(preop_result) == 1)
+		mvee_write_lock_result_finish();
+	else if (likely(preop_result) == 2)
+		mvee_read_lock_result_wake();
+#ifdef MVEE_CHECK_LOCK_TYPE
+	mvee_original_call_site = 0;
+#endif   
+}
+
+unsigned char mvee_atomic_preop(unsigned short op_type, void* word_ptr)
+{
+#ifdef MVEE_CHECK_LOCK_TYPE
+	mvee_original_call_site = (unsigned long)__builtin_return_address(0);
+#endif
+	return mvee_atomic_preop_internal(op_type + __MVEE_BASE_ATOMICS_MAX__, word_ptr);
+}
+
+void mvee_atomic_postop(unsigned char preop_result)
+{
+	mvee_atomic_postop_internal(preop_result);
+}
+
+void mvee_xcheck(unsigned long item)
+{
+	unsigned char tmp = mvee_atomic_preop_internal(ATOMIC_STORE, (void*) item);
+	mvee_atomic_postop_internal(tmp);
+}
+
+int mvee_should_sync_tid(void)
+{
+	return mvee_should_sync();
+}
+
+// the buffer initialization is NOT thread-safe. Therefore, it must be initialized
+// in single threaded context!!!
+void mvee_invalidate_buffer(void)
+{
+	mvee_buffer_valid = 0;
+	mvee_master_thread_id = 0;
+}
+
+/* MVEE PATCH:
+   Checks wether or not all variants got ALIGNMENT aligned heaps from
+   the previous mmap request. If some of them have not, ALL variants
+   have to bail out and fall back to another heap allocation method.
+   This ensures that the variants stay in sync with respect to future mm
+   requests.
+*/
+#define ALIGNMENT 0x4000000
+
+int
+mvee_all_heaps_aligned(char* heap, unsigned long alloc_size)
+{
+  // if we're not running under MVEE control,
+  // just check the alignment of the current heap
+  if (!mvee_master_thread_id)
+  {
+      if ((unsigned long)heap & (ALIGNMENT-1))
+		  return 0;
+      return 1;
+  }
+
+  // We ARE running under MVEE control
+  // => ask the MVEE to check the alignments
+  // of ALL heaps
+  return syscall(MVEE_ALL_HEAPS_ALIGNED, heap, ALIGNMENT, alloc_size);
+}
diff --git a/csu/mvee-woc-agent.c b/csu/mvee-woc-agent.c
new file mode 100644
index 0000000000..9e1f652962
--- /dev/null
+++ b/csu/mvee-woc-agent.c
@@ -0,0 +1,181 @@
+#define MVEE_SLAVE_YIELD
+#define MVEE_TOTAL_CLOCK_COUNT   2048
+#define MVEE_CLOCK_GROUP_SIZE    64
+#define MVEE_TOTAL_CLOCK_GROUPS  (MVEE_TOTAL_CLOCK_COUNT / MVEE_CLOCK_GROUP_SIZE)
+
+struct mvee_counter
+{
+  volatile unsigned long lock;
+  volatile unsigned long counter;
+  unsigned char padding[64 - 2 * sizeof(unsigned long)]; // prevents false sharing
+};
+
+struct mvee_op_entry
+{
+  volatile unsigned long  counter_and_idx; // the value we must see in mvee_counters[idx] before we can replay the operation
+};
+
+static unsigned char                  mvee_sync_enabled             = 0;
+static unsigned char                  mvee_libc_initialized         = 0;
+static unsigned char                  mvee_master_variant           = 0;
+static __thread unsigned long         mvee_thread_local_pos         = 0; // our position in the thread local queue
+static __thread  
+    struct mvee_op_entry*             mvee_thread_local_queue       = NULL;
+static __thread unsigned long         mvee_thread_local_queue_size  = 0; // nr of slots in the thread local queue
+static __thread unsigned short        mvee_prev_idx                 = 0;
+
+__attribute__((aligned (64)))
+static struct mvee_counter            mvee_counters[MVEE_TOTAL_CLOCK_COUNT + 1];
+
+#define likely(x)       __builtin_expect((x),1)
+#define unlikely(x)     __builtin_expect((x),0)
+#define gcc_barrier()  asm volatile("" ::: "memory")
+#ifndef arch_cpu_relax
+#define arch_cpu_relax()
+#endif
+
+extern void mvee_infinite_loop(void);
+
+void mvee_invalidate_buffer(void)
+{
+	mvee_thread_local_queue = NULL;
+}
+
+unsigned char mvee_atomic_preop_internal(volatile void* word_ptr)
+{
+	if (unlikely(!mvee_sync_enabled))
+		return 0;
+
+	if (unlikely(!mvee_thread_local_queue))
+    {
+		long mvee_thread_local_queue_id = syscall(MVEE_GET_SHARED_BUFFER, &mvee_counters, MVEE_LIBC_ATOMIC_BUFFER, &mvee_thread_local_queue_size, &mvee_thread_local_pos, NULL);
+		mvee_thread_local_queue_size   /= sizeof(struct mvee_op_entry);
+		mvee_thread_local_queue         = (void*)syscall(__NR_shmat, mvee_thread_local_queue_id, NULL, 0);     
+    }
+
+	if (unlikely(mvee_thread_local_pos >= mvee_thread_local_queue_size))
+    {
+		syscall(MVEE_FLUSH_SHARED_BUFFER, MVEE_LIBC_ATOMIC_BUFFER);
+		mvee_thread_local_pos = 0;
+    }
+
+	if (likely(mvee_master_variant))
+    {
+		// page number defines the clock group
+		// offset within page defines the clock within that group
+		mvee_prev_idx = (((((unsigned long)word_ptr >> 24) % MVEE_TOTAL_CLOCK_GROUPS) * (MVEE_CLOCK_GROUP_SIZE) 
+						  + ((((unsigned long)word_ptr & 4095) >> 6) % MVEE_CLOCK_GROUP_SIZE))
+						 & 0xFFF) + 1;
+
+		while (!__sync_bool_compare_and_swap(&mvee_counters[mvee_prev_idx].lock, 0, 1))
+			arch_cpu_relax();
+		
+		unsigned long pos = mvee_counters[mvee_prev_idx].counter;    
+
+		mvee_thread_local_queue[mvee_thread_local_pos++].counter_and_idx 
+			= (pos << 12) | mvee_prev_idx;
+
+		atomic_full_barrier();
+
+		return 1;
+    }
+	else
+    {
+		unsigned long counter_and_idx = 0;
+
+		while (unlikely(1))
+		{
+			counter_and_idx = mvee_thread_local_queue[mvee_thread_local_pos].counter_and_idx;
+
+			if (likely(counter_and_idx))
+				break;
+
+#ifdef MVEE_SLAVE_YIELD
+			syscall(__NR_sched_yield);
+#else
+			arch_cpu_relax();
+#endif
+		}
+
+		mvee_prev_idx = counter_and_idx & 0xFFF;
+		counter_and_idx &= ~0xFFF;
+
+		atomic_full_barrier();
+
+		while ((mvee_counters[mvee_prev_idx].counter << 12) != counter_and_idx)
+#ifdef MVEE_SLAVE_YIELD
+			syscall(__NR_sched_yield);
+#else
+		arch_cpu_relax();
+#endif
+
+		atomic_full_barrier();
+		
+		return 2;
+    }
+}
+
+void mvee_atomic_postop_internal(unsigned char preop_result)
+{
+	atomic_full_barrier();
+	
+	if(likely(preop_result) == 1)
+	{
+		gcc_barrier();
+		orig_atomic_increment(&mvee_counters[mvee_prev_idx].counter);
+		atomic_full_barrier();
+		mvee_counters[mvee_prev_idx].lock = 0;
+	}
+	else if (likely(preop_result) == 2)
+	{
+		gcc_barrier();
+		mvee_counters[mvee_prev_idx].counter++;
+		mvee_thread_local_pos++;
+	}
+}
+
+/* Checks if all variants got ALIGNMENT aligned heaps from
+   the previous mmap request. If some of them have not, ALL variants
+   have to bail out and fall back to another heap allocation method.
+   This ensures that the variants stay in sync with respect to future mm
+   requests.
+*/
+#define ALIGNMENT 0x4000000
+
+int
+mvee_all_heaps_aligned(char* heap, unsigned long alloc_size)
+{
+	// if we're not running under MVEE control,
+	// just check the alignment of the current heap
+	if (!mvee_thread_local_queue)
+	{
+		if ((unsigned long)heap & (ALIGNMENT-1))
+			return 0;
+		return 1;
+	}
+
+	// We ARE running under MVEE control
+	// => ask the MVEE to check the alignments
+	// of ALL heaps
+	return syscall(MVEE_ALL_HEAPS_ALIGNED, heap, ALIGNMENT, alloc_size);
+}
+
+int mvee_should_sync_tid(void)
+{
+	return mvee_sync_enabled ? 1 : 0;
+}
+
+unsigned char mvee_atomic_preop(unsigned short op_type, void* word_ptr)
+{
+	return mvee_atomic_preop_internal(word_ptr);
+}
+
+void mvee_atomic_postop(unsigned char preop_result)
+{
+	mvee_atomic_postop_internal(preop_result);
+}
+
+unsigned char mvee_should_futex_unlock(void)
+{
+	return (!mvee_master_variant && mvee_sync_enabled) ? 1 : 0;
+}
diff --git a/elf/rtld.c b/elf/rtld.c
index 553cfbd1b7..3b91bd4d1d 100644
--- a/elf/rtld.c
+++ b/elf/rtld.c
@@ -1300,6 +1300,18 @@ of this helper program; chances are you did not intend to run this program.\n\
       main_map->l_name = (char *) "";
       *user_entry = main_map->l_entry;
 
+     /* GHUMVEE patch: ask the MVEE for the "virtualized" argv[0].  This can be
+     necessary if we run compile-time diversified variants that print out their
+     argv[0] value. */                                                                                                                                                                                             
+	 char virtualized_argv0[4096];
+	 if (syscall(MVEE_GET_VIRTUALIZED_ARGV0, _dl_argv[0], virtualized_argv0, 4096) == 0)
+	 {
+		 // should we do this?
+		 if (_dl_argv[0])
+			 free(_dl_argv[0]);
+		 _dl_argv[0] = strdup(virtualized_argv0);
+	 }
+
 #ifdef HAVE_AUX_VECTOR
       /* Adjust the on-stack auxiliary vector so that it looks like the
 	 binary was executed directly.  */
diff --git a/gmon/gmon.c b/gmon/gmon.c
index dee64803ad..d5c8abf1ef 100644
--- a/gmon/gmon.c
+++ b/gmon/gmon.c
@@ -98,20 +98,20 @@ __moncontrol (int mode)
   struct gmonparam *p = &_gmonparam;
 
   /* Don't change the state if we ran into an error.  */
-  if (p->state == GMON_PROF_ERROR)
+  if (atomic_load_relaxed(&p->state) == GMON_PROF_ERROR)
     return;
 
   if (mode)
     {
       /* start */
       __profil((void *) p->kcount, p->kcountsize, p->lowpc, s_scale);
-      p->state = GMON_PROF_ON;
+      atomic_store_relaxed(&p->state, GMON_PROF_ON);
     }
   else
     {
       /* stop */
       __profil(NULL, 0, 0, 0);
-      p->state = GMON_PROF_OFF;
+      atomic_store_relaxed(&p->state, GMON_PROF_OFF);
     }
 }
 libc_hidden_def (__moncontrol)
@@ -155,7 +155,7 @@ __monstartup (u_long lowpc, u_long highpc)
     {
       ERR("monstartup: out of memory\n");
       p->tos = NULL;
-      p->state = GMON_PROF_ERROR;
+      atomic_store_relaxed(&p->state, GMON_PROF_ERROR);
       return;
     }
   p->tos = (struct tostruct *)cp;
@@ -419,11 +419,11 @@ write_gmon (void)
 void
 __write_profiling (void)
 {
-  int save = _gmonparam.state;
-  _gmonparam.state = GMON_PROF_OFF;
+  int save = atomic_load_relaxed(&_gmonparam.state);
+  atomic_store_relaxed(&_gmonparam.state, GMON_PROF_OFF);
   if (save == GMON_PROF_ON)
     write_gmon ();
-  _gmonparam.state = save;
+  atomic_store_relaxed(&_gmonparam.state, save);
 }
 #ifndef SHARED
 /* This symbol isn't used anywhere in the DSO and it is not exported.
@@ -440,7 +440,7 @@ _mcleanup (void)
 {
   __moncontrol (0);
 
-  if (_gmonparam.state != GMON_PROF_ERROR)
+  if (atomic_load_relaxed(&_gmonparam.state) != GMON_PROF_ERROR)
     write_gmon ();
 
   /* free the memory. */
diff --git a/gmon/mcount.c b/gmon/mcount.c
index 9d4a1a50fa..07705ec485 100644
--- a/gmon/mcount.c
+++ b/gmon/mcount.c
@@ -166,10 +166,10 @@ _MCOUNT_DECL(frompc, selfpc)	/* _mcount; may be static, inline, etc */
 
 	}
 done:
-	p->state = GMON_PROF_ON;
+	atomic_store_relaxed(&p->state, GMON_PROF_ON);
 	return;
 overflow:
-	p->state = GMON_PROF_ERROR;
+	atomic_store_relaxed(&p->state, GMON_PROF_ERROR);
 	return;
 }
 
diff --git a/include/atomic.h b/include/atomic.h
index 818b22b117..9a3c6fa4e4 100644
--- a/include/atomic.h
+++ b/include/atomic.h
@@ -86,73 +86,73 @@
 
 /* Atomically store NEWVAL in *MEM if *MEM is equal to OLDVAL.
    Return the old *MEM value.  */
-#if !defined atomic_compare_and_exchange_val_acq \
-    && defined __arch_compare_and_exchange_val_32_acq
-# define atomic_compare_and_exchange_val_acq(mem, newval, oldval) \
-  __atomic_val_bysize (__arch_compare_and_exchange_val,acq,		      \
+#if !defined orig_atomic_compare_and_exchange_val_acq \
+    && defined orig___arch_compare_and_exchange_val_32_acq
+# define orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval) \
+  __atomic_val_bysize (orig___arch_compare_and_exchange_val,acq,		      \
 		       mem, newval, oldval)
 #endif
 
 
-#ifndef catomic_compare_and_exchange_val_acq
-# ifdef __arch_c_compare_and_exchange_val_32_acq
-#  define catomic_compare_and_exchange_val_acq(mem, newval, oldval) \
-  __atomic_val_bysize (__arch_c_compare_and_exchange_val,acq,		      \
+#ifndef orig_catomic_compare_and_exchange_val_acq
+# ifdef orig___arch_c_compare_and_exchange_val_32_acq
+#  define orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval) \
+  __atomic_val_bysize (orig___arch_c_compare_and_exchange_val,acq,		      \
 		       mem, newval, oldval)
 # else
-#  define catomic_compare_and_exchange_val_acq(mem, newval, oldval) \
-  atomic_compare_and_exchange_val_acq (mem, newval, oldval)
+#  define orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval) \
+  orig_atomic_compare_and_exchange_val_acq (mem, newval, oldval)
 # endif
 #endif
 
 
-#ifndef catomic_compare_and_exchange_val_rel
-# ifndef atomic_compare_and_exchange_val_rel
-#  define catomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
-  catomic_compare_and_exchange_val_acq (mem, newval, oldval)
+#ifndef orig_catomic_compare_and_exchange_val_rel
+# ifndef orig_atomic_compare_and_exchange_val_rel
+#  define orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
+  orig_catomic_compare_and_exchange_val_acq (mem, newval, oldval)
 # else
-#  define catomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
-  atomic_compare_and_exchange_val_rel (mem, newval, oldval)
+#  define orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
+  orig_atomic_compare_and_exchange_val_rel (mem, newval, oldval)
 # endif
 #endif
 
 
-#ifndef atomic_compare_and_exchange_val_rel
-# define atomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
-  atomic_compare_and_exchange_val_acq (mem, newval, oldval)
+#ifndef orig_atomic_compare_and_exchange_val_rel
+# define orig_atomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
+  orig_atomic_compare_and_exchange_val_acq (mem, newval, oldval)
 #endif
 
 
 /* Atomically store NEWVAL in *MEM if *MEM is equal to OLDVAL.
    Return zero if *MEM was changed or non-zero if no exchange happened.  */
-#ifndef atomic_compare_and_exchange_bool_acq
-# ifdef __arch_compare_and_exchange_bool_32_acq
-#  define atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
-  __atomic_bool_bysize (__arch_compare_and_exchange_bool,acq,		      \
+#ifndef orig_atomic_compare_and_exchange_bool_acq
+# ifdef orig___arch_compare_and_exchange_bool_32_acq
+#  define orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
+  __atomic_bool_bysize (orig___arch_compare_and_exchange_bool,acq,		      \
 		        mem, newval, oldval)
 # else
-#  define atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
+#  define orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
   ({ /* Cannot use __oldval here, because macros later in this file might     \
 	call this macro with __oldval argument.	 */			      \
      __typeof (oldval) __atg3_old = (oldval);				      \
-     atomic_compare_and_exchange_val_acq (mem, newval, __atg3_old)	      \
+     orig_atomic_compare_and_exchange_val_acq (mem, newval, __atg3_old)	      \
        != __atg3_old;							      \
   })
 # endif
 #endif
 
 
-#ifndef catomic_compare_and_exchange_bool_acq
-# ifdef __arch_c_compare_and_exchange_bool_32_acq
-#  define catomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
-  __atomic_bool_bysize (__arch_c_compare_and_exchange_bool,acq,		      \
+#ifndef orig_catomic_compare_and_exchange_bool_acq
+# ifdef orig___arch_c_compare_and_exchange_bool_32_acq
+#  define orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
+  __atomic_bool_bysize (orig___arch_c_compare_and_exchange_bool,acq,		      \
 		        mem, newval, oldval)
 # else
-#  define catomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
+#  define orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
   ({ /* Cannot use __oldval here, because macros later in this file might     \
 	call this macro with __oldval argument.	 */			      \
      __typeof (oldval) __atg4_old = (oldval);				      \
-     catomic_compare_and_exchange_val_acq (mem, newval, __atg4_old)	      \
+     orig_catomic_compare_and_exchange_val_acq (mem, newval, __atg4_old)	      \
        != __atg4_old;							      \
   })
 # endif
@@ -169,24 +169,24 @@
      do									      \
        __atg5_oldval = *__atg5_memp;					      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg5_memp, __atg5_value, \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg5_memp, __atg5_value, \
 						   __atg5_oldval), 0));	      \
 									      \
      __atg5_oldval; })
 #endif
 
-#ifndef atomic_exchange_rel
-# define atomic_exchange_rel(mem, newvalue) atomic_exchange_acq (mem, newvalue)
+#ifndef orig_atomic_exchange_rel
+# define orig_atomic_exchange_rel(mem, newvalue) orig_atomic_exchange_acq (mem, newvalue)
 #endif
 
 
 /* Add VALUE to *MEM and return the old value of *MEM.  */
-#ifndef atomic_exchange_and_add_acq
-# ifdef atomic_exchange_and_add
-#  define atomic_exchange_and_add_acq(mem, value) \
-  atomic_exchange_and_add (mem, value)
+#ifndef orig_atomic_exchange_and_add_acq
+# ifdef orig_atomic_exchange_and_add
+#  define orig_atomic_exchange_and_add_acq(mem, value) \
+  orig_atomic_exchange_and_add (mem, value)
 # else
-#  define atomic_exchange_and_add_acq(mem, value) \
+#  define orig_atomic_exchange_and_add_acq(mem, value) \
   ({ __typeof (*(mem)) __atg6_oldval;					      \
      __typeof (mem) __atg6_memp = (mem);				      \
      __typeof (*(mem)) __atg6_value = (value);				      \
@@ -194,7 +194,7 @@
      do									      \
        __atg6_oldval = *__atg6_memp;					      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg6_memp,		      \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg6_memp,		      \
 						   __atg6_oldval	      \
 						   + __atg6_value,	      \
 						   __atg6_oldval), 0));	      \
@@ -203,18 +203,18 @@
 # endif
 #endif
 
-#ifndef atomic_exchange_and_add_rel
-# define atomic_exchange_and_add_rel(mem, value) \
-  atomic_exchange_and_add_acq(mem, value)
+#ifndef orig_atomic_exchange_and_add_rel
+# define orig_atomic_exchange_and_add_rel(mem, value) \
+  orig_atomic_exchange_and_add_acq(mem, value)
 #endif
 
-#ifndef atomic_exchange_and_add
-# define atomic_exchange_and_add(mem, value) \
-  atomic_exchange_and_add_acq(mem, value)
+#ifndef orig_atomic_exchange_and_add
+# define orig_atomic_exchange_and_add(mem, value) \
+  orig_atomic_exchange_and_add_acq(mem, value)
 #endif
 
-#ifndef catomic_exchange_and_add
-# define catomic_exchange_and_add(mem, value) \
+#ifndef orig_catomic_exchange_and_add
+# define orig_catomic_exchange_and_add(mem, value) \
   ({ __typeof (*(mem)) __atg7_oldv;					      \
      __typeof (mem) __atg7_memp = (mem);				      \
      __typeof (*(mem)) __atg7_value = (value);				      \
@@ -222,7 +222,7 @@
      do									      \
        __atg7_oldv = *__atg7_memp;					      \
      while (__builtin_expect						      \
-	    (catomic_compare_and_exchange_bool_acq (__atg7_memp,	      \
+	    (orig_catomic_compare_and_exchange_bool_acq (__atg7_memp,	      \
 						    __atg7_oldv		      \
 						    + __atg7_value,	      \
 						    __atg7_oldv), 0));	      \
@@ -231,8 +231,8 @@
 #endif
 
 
-#ifndef atomic_max
-# define atomic_max(mem, value) \
+#ifndef orig_atomic_max
+# define orig_atomic_max(mem, value) \
   do {									      \
     __typeof (*(mem)) __atg8_oldval;					      \
     __typeof (mem) __atg8_memp = (mem);					      \
@@ -242,14 +242,14 @@
       if (__atg8_oldval >= __atg8_value)				      \
 	break;								      \
     } while (__builtin_expect						      \
-	     (atomic_compare_and_exchange_bool_acq (__atg8_memp, __atg8_value,\
+	     (orig_atomic_compare_and_exchange_bool_acq (__atg8_memp, __atg8_value,\
 						    __atg8_oldval), 0));      \
   } while (0)
 #endif
 
 
-#ifndef catomic_max
-# define catomic_max(mem, value) \
+#ifndef orig_catomic_max
+# define orig_catomic_max(mem, value) \
   do {									      \
     __typeof (*(mem)) __atg9_oldv;					      \
     __typeof (mem) __atg9_memp = (mem);					      \
@@ -259,15 +259,15 @@
       if (__atg9_oldv >= __atg9_value)					      \
 	break;								      \
     } while (__builtin_expect						      \
-	     (catomic_compare_and_exchange_bool_acq (__atg9_memp,	      \
+	     (orig_catomic_compare_and_exchange_bool_acq (__atg9_memp,	      \
 						     __atg9_value,	      \
 						     __atg9_oldv), 0));	      \
   } while (0)
 #endif
 
 
-#ifndef atomic_min
-# define atomic_min(mem, value) \
+#ifndef orig_atomic_min
+# define orig_atomic_min(mem, value) \
   do {									      \
     __typeof (*(mem)) __atg10_oldval;					      \
     __typeof (mem) __atg10_memp = (mem);				      \
@@ -277,81 +277,81 @@
       if (__atg10_oldval <= __atg10_value)				      \
 	break;								      \
     } while (__builtin_expect						      \
-	     (atomic_compare_and_exchange_bool_acq (__atg10_memp,	      \
+	     (orig_atomic_compare_and_exchange_bool_acq (__atg10_memp,	      \
 						    __atg10_value,	      \
 						    __atg10_oldval), 0));     \
   } while (0)
 #endif
 
 
-#ifndef atomic_add
-# define atomic_add(mem, value) (void) atomic_exchange_and_add ((mem), (value))
+#ifndef orig_atomic_add
+# define orig_atomic_add(mem, value) (void) orig_atomic_exchange_and_add ((mem), (value))
 #endif
 
 
-#ifndef catomic_add
-# define catomic_add(mem, value) \
-  (void) catomic_exchange_and_add ((mem), (value))
+#ifndef orig_catomic_add
+# define orig_catomic_add(mem, value) \
+  (void) orig_catomic_exchange_and_add ((mem), (value))
 #endif
 
 
-#ifndef atomic_increment
-# define atomic_increment(mem) atomic_add ((mem), 1)
+#ifndef orig_atomic_increment
+# define orig_atomic_increment(mem) orig_atomic_add ((mem), 1)
 #endif
 
 
-#ifndef catomic_increment
-# define catomic_increment(mem) catomic_add ((mem), 1)
+#ifndef orig_catomic_increment
+# define orig_catomic_increment(mem) orig_catomic_add ((mem), 1)
 #endif
 
 
-#ifndef atomic_increment_val
-# define atomic_increment_val(mem) (atomic_exchange_and_add ((mem), 1) + 1)
+#ifndef orig_atomic_increment_val
+# define orig_atomic_increment_val(mem) (orig_atomic_exchange_and_add ((mem), 1) + 1)
 #endif
 
 
-#ifndef catomic_increment_val
-# define catomic_increment_val(mem) (catomic_exchange_and_add ((mem), 1) + 1)
+#ifndef orig_catomic_increment_val
+# define orig_catomic_increment_val(mem) (orig_catomic_exchange_and_add ((mem), 1) + 1)
 #endif
 
 
 /* Add one to *MEM and return true iff it's now zero.  */
-#ifndef atomic_increment_and_test
-# define atomic_increment_and_test(mem) \
-  (atomic_exchange_and_add ((mem), 1) + 1 == 0)
+#ifndef orig_atomic_increment_and_test
+# define orig_atomic_increment_and_test(mem) \
+  (orig_atomic_exchange_and_add ((mem), 1) + 1 == 0)
 #endif
 
 
-#ifndef atomic_decrement
-# define atomic_decrement(mem) atomic_add ((mem), -1)
+#ifndef orig_atomic_decrement
+# define orig_atomic_decrement(mem) orig_atomic_add ((mem), -1)
 #endif
 
 
-#ifndef catomic_decrement
-# define catomic_decrement(mem) catomic_add ((mem), -1)
+#ifndef orig_catomic_decrement
+# define orig_catomic_decrement(mem) orig_catomic_add ((mem), -1)
 #endif
 
 
-#ifndef atomic_decrement_val
-# define atomic_decrement_val(mem) (atomic_exchange_and_add ((mem), -1) - 1)
+#ifndef orig_atomic_decrement_val
+# define orig_atomic_decrement_val(mem) (orig_atomic_exchange_and_add ((mem), -1) - 1)
 #endif
 
 
-#ifndef catomic_decrement_val
-# define catomic_decrement_val(mem) (catomic_exchange_and_add ((mem), -1) - 1)
+#ifndef orig_catomic_decrement_val
+# define orig_catomic_decrement_val(mem) (orig_catomic_exchange_and_add ((mem), -1) - 1)
 #endif
 
 
 /* Subtract 1 from *MEM and return true iff it's now zero.  */
-#ifndef atomic_decrement_and_test
-# define atomic_decrement_and_test(mem) \
-  (atomic_exchange_and_add ((mem), -1) == 1)
+#ifndef orig_atomic_decrement_and_test
+# define orig_atomic_decrement_and_test(mem) \
+  (orig_atomic_exchange_and_add ((mem), -1) == 1)
 #endif
 
 
 /* Decrement *MEM if it is > 0, and return the old value.  */
-#ifndef atomic_decrement_if_positive
-# define atomic_decrement_if_positive(mem) \
+#ifndef orig_atomic_decrement_if_positive
+# define orig_atomic_decrement_if_positive(mem) \
   ({ __typeof (*(mem)) __atg11_oldval;					      \
      __typeof (mem) __atg11_memp = (mem);				      \
 									      \
@@ -362,35 +362,35 @@
 	   break;							      \
        }								      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg11_memp,	      \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg11_memp,	      \
 						   __atg11_oldval - 1,	      \
 						   __atg11_oldval), 0));      \
      __atg11_oldval; })
 #endif
 
 
-#ifndef atomic_add_negative
-# define atomic_add_negative(mem, value)				      \
+#ifndef orig_atomic_add_negative
+# define orig_atomic_add_negative(mem, value)				      \
   ({ __typeof (value) __atg12_value = (value);				      \
-     atomic_exchange_and_add (mem, __atg12_value) < -__atg12_value; })
+     orig_atomic_exchange_and_add (mem, __atg12_value) < -__atg12_value; })
 #endif
 
 
-#ifndef atomic_add_zero
-# define atomic_add_zero(mem, value)					      \
+#ifndef orig_atomic_add_zero
+# define orig_atomic_add_zero(mem, value)					      \
   ({ __typeof (value) __atg13_value = (value);				      \
-     atomic_exchange_and_add (mem, __atg13_value) == -__atg13_value; })
+     orig_atomic_exchange_and_add (mem, __atg13_value) == -__atg13_value; })
 #endif
 
 
-#ifndef atomic_bit_set
-# define atomic_bit_set(mem, bit) \
-  (void) atomic_bit_test_set(mem, bit)
+#ifndef orig_atomic_bit_set
+# define orig_atomic_bit_set(mem, bit) \
+  (void) orig_atomic_bit_test_set(mem, bit)
 #endif
 
 
-#ifndef atomic_bit_test_set
-# define atomic_bit_test_set(mem, bit) \
+#ifndef orig_atomic_bit_test_set
+# define orig_atomic_bit_test_set(mem, bit) \
   ({ __typeof (*(mem)) __atg14_old;					      \
      __typeof (mem) __atg14_memp = (mem);				      \
      __typeof (*(mem)) __atg14_mask = ((__typeof (*(mem))) 1 << (bit));	      \
@@ -398,7 +398,7 @@
      do									      \
        __atg14_old = (*__atg14_memp);					      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg14_memp,	      \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg14_memp,	      \
 						   __atg14_old | __atg14_mask,\
 						   __atg14_old), 0));	      \
 									      \
@@ -406,8 +406,8 @@
 #endif
 
 /* Atomically *mem &= mask.  */
-#ifndef atomic_and
-# define atomic_and(mem, mask) \
+#ifndef orig_atomic_and
+# define orig_atomic_and(mem, mask) \
   do {									      \
     __typeof (*(mem)) __atg15_old;					      \
     __typeof (mem) __atg15_memp = (mem);				      \
@@ -416,14 +416,14 @@
     do									      \
       __atg15_old = (*__atg15_memp);					      \
     while (__builtin_expect						      \
-	   (atomic_compare_and_exchange_bool_acq (__atg15_memp,		      \
+	   (orig_atomic_compare_and_exchange_bool_acq (__atg15_memp,		      \
 						  __atg15_old & __atg15_mask, \
 						  __atg15_old), 0));	      \
   } while (0)
 #endif
 
-#ifndef catomic_and
-# define catomic_and(mem, mask) \
+#ifndef orig_catomic_and
+# define orig_catomic_and(mem, mask) \
   do {									      \
     __typeof (*(mem)) __atg20_old;					      \
     __typeof (mem) __atg20_memp = (mem);				      \
@@ -432,15 +432,15 @@
     do									      \
       __atg20_old = (*__atg20_memp);					      \
     while (__builtin_expect						      \
-	   (catomic_compare_and_exchange_bool_acq (__atg20_memp,	      \
+	   (orig_catomic_compare_and_exchange_bool_acq (__atg20_memp,	      \
 						   __atg20_old & __atg20_mask,\
 						   __atg20_old), 0));	      \
   } while (0)
 #endif
 
 /* Atomically *mem &= mask and return the old value of *mem.  */
-#ifndef atomic_and_val
-# define atomic_and_val(mem, mask) \
+#ifndef orig_atomic_and_val
+# define orig_atomic_and_val(mem, mask) \
   ({ __typeof (*(mem)) __atg16_old;					      \
      __typeof (mem) __atg16_memp = (mem);				      \
      __typeof (*(mem)) __atg16_mask = (mask);				      \
@@ -448,7 +448,7 @@
      do									      \
        __atg16_old = (*__atg16_memp);					      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg16_memp,	      \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg16_memp,	      \
 						   __atg16_old & __atg16_mask,\
 						   __atg16_old), 0));	      \
 									      \
@@ -456,8 +456,8 @@
 #endif
 
 /* Atomically *mem |= mask and return the old value of *mem.  */
-#ifndef atomic_or
-# define atomic_or(mem, mask) \
+#ifndef orig_atomic_or
+# define orig_atomic_or(mem, mask) \
   do {									      \
     __typeof (*(mem)) __atg17_old;					      \
     __typeof (mem) __atg17_memp = (mem);				      \
@@ -466,14 +466,14 @@
     do									      \
       __atg17_old = (*__atg17_memp);					      \
     while (__builtin_expect						      \
-	   (atomic_compare_and_exchange_bool_acq (__atg17_memp,		      \
+	   (orig_atomic_compare_and_exchange_bool_acq (__atg17_memp,		      \
 						  __atg17_old | __atg17_mask, \
 						  __atg17_old), 0));	      \
   } while (0)
 #endif
 
-#ifndef catomic_or
-# define catomic_or(mem, mask) \
+#ifndef orig_catomic_or
+# define orig_catomic_or(mem, mask) \
   do {									      \
     __typeof (*(mem)) __atg18_old;					      \
     __typeof (mem) __atg18_memp = (mem);				      \
@@ -482,15 +482,15 @@
     do									      \
       __atg18_old = (*__atg18_memp);					      \
     while (__builtin_expect						      \
-	   (catomic_compare_and_exchange_bool_acq (__atg18_memp,	      \
+	   (orig_catomic_compare_and_exchange_bool_acq (__atg18_memp,	      \
 						   __atg18_old | __atg18_mask,\
 						   __atg18_old), 0));	      \
   } while (0)
 #endif
 
 /* Atomically *mem |= mask and return the old value of *mem.  */
-#ifndef atomic_or_val
-# define atomic_or_val(mem, mask) \
+#ifndef orig_atomic_or_val
+# define orig_atomic_or_val(mem, mask) \
   ({ __typeof (*(mem)) __atg19_old;					      \
      __typeof (mem) __atg19_memp = (mem);				      \
      __typeof (*(mem)) __atg19_mask = (mask);				      \
@@ -498,7 +498,7 @@
      do									      \
        __atg19_old = (*__atg19_memp);					      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg19_memp,	      \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg19_memp,	      \
 						   __atg19_old | __atg19_mask,\
 						   __atg19_old), 0));	      \
 									      \
@@ -520,8 +520,8 @@
 #endif
 
 
-#ifndef atomic_forced_read
-# define atomic_forced_read(x) \
+#ifndef orig_atomic_forced_read
+# define orig_atomic_forced_read(x) \
   ({ __typeof (x) __x; __asm ("" : "=r" (__x) : "0" (x)); __x; })
 #endif
 
@@ -572,82 +572,82 @@ void __atomic_link_error (void);
 # define atomic_thread_fence_seq_cst() \
   __atomic_thread_fence (__ATOMIC_SEQ_CST)
 
-# define atomic_load_relaxed(mem) \
+# define orig_atomic_load_relaxed(mem) \
   ({ __atomic_check_size_ls((mem));					      \
      __atomic_load_n ((mem), __ATOMIC_RELAXED); })
-# define atomic_load_acquire(mem) \
+# define orig_atomic_load_acquire(mem) \
   ({ __atomic_check_size_ls((mem));					      \
      __atomic_load_n ((mem), __ATOMIC_ACQUIRE); })
 
-# define atomic_store_relaxed(mem, val) \
+# define orig_atomic_store_relaxed(mem, val) \
   do {									      \
     __atomic_check_size_ls((mem));					      \
     __atomic_store_n ((mem), (val), __ATOMIC_RELAXED);			      \
   } while (0)
-# define atomic_store_release(mem, val) \
+# define orig_atomic_store_release(mem, val) \
   do {									      \
     __atomic_check_size_ls((mem));					      \
     __atomic_store_n ((mem), (val), __ATOMIC_RELEASE);			      \
   } while (0)
 
 /* On failure, this CAS has memory_order_relaxed semantics.  */
-# define atomic_compare_exchange_weak_relaxed(mem, expected, desired) \
+# define orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_compare_exchange_n ((mem), (expected), (desired), 1,		      \
     __ATOMIC_RELAXED, __ATOMIC_RELAXED); })
-# define atomic_compare_exchange_weak_acquire(mem, expected, desired) \
+# define orig_atomic_compare_exchange_weak_acquire(mem, expected, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_compare_exchange_n ((mem), (expected), (desired), 1,		      \
     __ATOMIC_ACQUIRE, __ATOMIC_RELAXED); })
-# define atomic_compare_exchange_weak_release(mem, expected, desired) \
+# define orig_atomic_compare_exchange_weak_release(mem, expected, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_compare_exchange_n ((mem), (expected), (desired), 1,		      \
     __ATOMIC_RELEASE, __ATOMIC_RELAXED); })
 
-# define atomic_exchange_relaxed(mem, desired) \
+# define orig_atomic_exchange_relaxed(mem, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_exchange_n ((mem), (desired), __ATOMIC_RELAXED); })
-# define atomic_exchange_acquire(mem, desired) \
+# define orig_atomic_exchange_acquire(mem, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_exchange_n ((mem), (desired), __ATOMIC_ACQUIRE); })
-# define atomic_exchange_release(mem, desired) \
+# define orig_atomic_exchange_release(mem, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_exchange_n ((mem), (desired), __ATOMIC_RELEASE); })
 
-# define atomic_fetch_add_relaxed(mem, operand) \
+# define orig_atomic_fetch_add_relaxed(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_add ((mem), (operand), __ATOMIC_RELAXED); })
-# define atomic_fetch_add_acquire(mem, operand) \
+# define orig_atomic_fetch_add_acquire(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_add ((mem), (operand), __ATOMIC_ACQUIRE); })
-# define atomic_fetch_add_release(mem, operand) \
+# define orig_atomic_fetch_add_release(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_add ((mem), (operand), __ATOMIC_RELEASE); })
-# define atomic_fetch_add_acq_rel(mem, operand) \
+# define orig_atomic_fetch_add_acq_rel(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_add ((mem), (operand), __ATOMIC_ACQ_REL); })
 
-# define atomic_fetch_and_relaxed(mem, operand) \
+# define orig_atomic_fetch_and_relaxed(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_and ((mem), (operand), __ATOMIC_RELAXED); })
-# define atomic_fetch_and_acquire(mem, operand) \
+# define orig_atomic_fetch_and_acquire(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_and ((mem), (operand), __ATOMIC_ACQUIRE); })
-# define atomic_fetch_and_release(mem, operand) \
+# define orig_atomic_fetch_and_release(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_and ((mem), (operand), __ATOMIC_RELEASE); })
 
-# define atomic_fetch_or_relaxed(mem, operand) \
+# define orig_atomic_fetch_or_relaxed(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_or ((mem), (operand), __ATOMIC_RELAXED); })
-# define atomic_fetch_or_acquire(mem, operand) \
+# define orig_atomic_fetch_or_acquire(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_or ((mem), (operand), __ATOMIC_ACQUIRE); })
-# define atomic_fetch_or_release(mem, operand) \
+# define orig_atomic_fetch_or_release(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_or ((mem), (operand), __ATOMIC_RELEASE); })
 
-# define atomic_fetch_xor_release(mem, operand) \
+# define orig_atomic_fetch_xor_release(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_xor ((mem), (operand), __ATOMIC_RELEASE); })
 
@@ -666,25 +666,25 @@ void __atomic_link_error (void);
 #  define atomic_thread_fence_seq_cst() atomic_full_barrier ()
 # endif
 
-# ifndef atomic_load_relaxed
-#  define atomic_load_relaxed(mem) \
+# ifndef orig_atomic_load_relaxed
+#  define orig_atomic_load_relaxed(mem) \
    ({ __typeof ((__typeof (*(mem))) *(mem)) __atg100_val;		      \
    __asm ("" : "=r" (__atg100_val) : "0" (*(mem)));			      \
    __atg100_val; })
 # endif
-# ifndef atomic_load_acquire
-#  define atomic_load_acquire(mem) \
+# ifndef orig_atomic_load_acquire
+#  define orig_atomic_load_acquire(mem) \
    ({ __typeof (*(mem)) __atg101_val = atomic_load_relaxed (mem);	      \
    atomic_thread_fence_acquire ();					      \
    __atg101_val; })
 # endif
 
-# ifndef atomic_store_relaxed
+# ifndef orig_atomic_store_relaxed
 /* XXX Use inline asm here?  */
-#  define atomic_store_relaxed(mem, val) do { *(mem) = (val); } while (0)
+#  define orig_atomic_store_relaxed(mem, val) do { *(mem) = (val); } while (0)
 # endif
-# ifndef atomic_store_release
-#  define atomic_store_release(mem, val) \
+# ifndef orig_atomic_store_release
+#  define orig_atomic_store_release(mem, val) \
    do {									      \
      atomic_thread_fence_release ();					      \
      atomic_store_relaxed ((mem), (val));				      \
@@ -695,107 +695,107 @@ void __atomic_link_error (void);
 /* XXX This potentially has one branch more than necessary, but archs
    currently do not define a CAS that returns both the previous value and
    the success flag.  */
-# ifndef atomic_compare_exchange_weak_acquire
-#  define atomic_compare_exchange_weak_acquire(mem, expected, desired) \
+# ifndef orig_atomic_compare_exchange_weak_acquire
+#  define orig_atomic_compare_exchange_weak_acquire(mem, expected, desired) \
    ({ typeof (*(expected)) __atg102_expected = *(expected);		      \
    *(expected) =							      \
-     atomic_compare_and_exchange_val_acq ((mem), (desired), *(expected));     \
+     orig_atomic_compare_and_exchange_val_acq ((mem), (desired), *(expected));     \
    *(expected) == __atg102_expected; })
 # endif
-# ifndef atomic_compare_exchange_weak_relaxed
+# ifndef orig_atomic_compare_exchange_weak_relaxed
 /* XXX Fall back to CAS with acquire MO because archs do not define a weaker
    CAS.  */
-#  define atomic_compare_exchange_weak_relaxed(mem, expected, desired) \
-   atomic_compare_exchange_weak_acquire ((mem), (expected), (desired))
+#  define orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired) \
+   orig_atomic_compare_exchange_weak_acquire ((mem), (expected), (desired))
 # endif
-# ifndef atomic_compare_exchange_weak_release
-#  define atomic_compare_exchange_weak_release(mem, expected, desired) \
+# ifndef orig_atomic_compare_exchange_weak_release
+#  define orig_atomic_compare_exchange_weak_release(mem, expected, desired) \
    ({ typeof (*(expected)) __atg103_expected = *(expected);		      \
    *(expected) =							      \
-     atomic_compare_and_exchange_val_rel ((mem), (desired), *(expected));     \
+     orig_atomic_compare_and_exchange_val_rel ((mem), (desired), *(expected));     \
    *(expected) == __atg103_expected; })
 # endif
 
 /* XXX Fall back to acquire MO because archs do not define a weaker
    atomic_exchange.  */
-# ifndef atomic_exchange_relaxed
-#  define atomic_exchange_relaxed(mem, val) \
-   atomic_exchange_acq ((mem), (val))
+# ifndef orig_atomic_exchange_relaxed
+#  define orig_atomic_exchange_relaxed(mem, val) \
+   orig_atomic_exchange_acq ((mem), (val))
 # endif
-# ifndef atomic_exchange_acquire
-#  define atomic_exchange_acquire(mem, val) \
-   atomic_exchange_acq ((mem), (val))
+# ifndef orig_atomic_exchange_acquire
+#  define orig_atomic_exchange_acquire(mem, val) \
+   orig_atomic_exchange_acq ((mem), (val))
 # endif
-# ifndef atomic_exchange_release
-#  define atomic_exchange_release(mem, val) \
-   atomic_exchange_rel ((mem), (val))
+# ifndef orig_atomic_exchange_release
+#  define orig_atomic_exchange_release(mem, val) \
+   orig_atomic_exchange_rel ((mem), (val))
 # endif
 
-# ifndef atomic_fetch_add_acquire
-#  define atomic_fetch_add_acquire(mem, operand) \
-   atomic_exchange_and_add_acq ((mem), (operand))
+# ifndef orig_atomic_fetch_add_acquire
+#  define orig_atomic_fetch_add_acquire(mem, operand) \
+   orig_atomic_exchange_and_add_acq ((mem), (operand))
 # endif
-# ifndef atomic_fetch_add_relaxed
+# ifndef orig_atomic_fetch_add_relaxed
 /* XXX Fall back to acquire MO because the MO semantics of
    atomic_exchange_and_add are not documented; the generic version falls back
    to atomic_exchange_and_add_acq if atomic_exchange_and_add is not defined,
    and vice versa.  */
-#  define atomic_fetch_add_relaxed(mem, operand) \
-   atomic_fetch_add_acquire ((mem), (operand))
+#  define orig_atomic_fetch_add_relaxed(mem, operand) \
+   orig_atomic_fetch_add_acquire ((mem), (operand))
 # endif
-# ifndef atomic_fetch_add_release
-#  define atomic_fetch_add_release(mem, operand) \
-   atomic_exchange_and_add_rel ((mem), (operand))
+# ifndef orig_atomic_fetch_add_release
+#  define orig_atomic_fetch_add_release(mem, operand) \
+   orig_atomic_exchange_and_add_rel ((mem), (operand))
 # endif
-# ifndef atomic_fetch_add_acq_rel
-#  define atomic_fetch_add_acq_rel(mem, operand) \
+# ifndef orig_atomic_fetch_add_acq_rel
+#  define orig_atomic_fetch_add_acq_rel(mem, operand) \
    ({ atomic_thread_fence_release ();					      \
-   atomic_exchange_and_add_acq ((mem), (operand)); })
+   orig_atomic_exchange_and_add_acq ((mem), (operand)); })
 # endif
 
 /* XXX Fall back to acquire MO because archs do not define a weaker
    atomic_and_val.  */
-# ifndef atomic_fetch_and_relaxed
-#  define atomic_fetch_and_relaxed(mem, operand) \
-   atomic_fetch_and_acquire ((mem), (operand))
+# ifndef orig_atomic_fetch_and_relaxed
+#  define orig_atomic_fetch_and_relaxed(mem, operand) \
+   orig_atomic_fetch_and_acquire ((mem), (operand))
 # endif
 /* XXX The default for atomic_and_val has acquire semantics, but this is not
    documented.  */
-# ifndef atomic_fetch_and_acquire
-#  define atomic_fetch_and_acquire(mem, operand) \
-   atomic_and_val ((mem), (operand))
+# ifndef orig_atomic_fetch_and_acquire
+#  define orig_atomic_fetch_and_acquire(mem, operand) \
+   orig_atomic_and_val ((mem), (operand))
 # endif
-# ifndef atomic_fetch_and_release
+# ifndef orig_atomic_fetch_and_release
 /* XXX This unnecessarily has acquire MO.  */
-#  define atomic_fetch_and_release(mem, operand) \
+#  define orig_atomic_fetch_and_release(mem, operand) \
    ({ atomic_thread_fence_release ();					      \
-   atomic_and_val ((mem), (operand)); })
+   orig_atomic_and_val ((mem), (operand)); })
 # endif
 
 /* XXX The default for atomic_or_val has acquire semantics, but this is not
    documented.  */
-# ifndef atomic_fetch_or_acquire
-#  define atomic_fetch_or_acquire(mem, operand) \
-   atomic_or_val ((mem), (operand))
+# ifndef orig_atomic_fetch_or_acquire
+#  define orig_atomic_fetch_or_acquire(mem, operand) \
+   orig_atomic_or_val ((mem), (operand))
 # endif
 /* XXX Fall back to acquire MO because archs do not define a weaker
    atomic_or_val.  */
-# ifndef atomic_fetch_or_relaxed
-#  define atomic_fetch_or_relaxed(mem, operand) \
-   atomic_fetch_or_acquire ((mem), (operand))
+# ifndef orig_atomic_fetch_or_relaxed
+#  define orig_atomic_fetch_or_relaxed(mem, operand) \
+   orig_atomic_fetch_or_acquire ((mem), (operand))
 # endif
 /* XXX Contains an unnecessary acquire MO because archs do not define a weaker
    atomic_or_val.  */
-# ifndef atomic_fetch_or_release
-#  define atomic_fetch_or_release(mem, operand) \
+# ifndef orig_atomic_fetch_or_release
+#  define orig_atomic_fetch_or_release(mem, operand) \
    ({ atomic_thread_fence_release ();					      \
-   atomic_fetch_or_acquire ((mem), (operand)); })
+   orig_atomic_fetch_or_acquire ((mem), (operand)); })
 # endif
 
-# ifndef atomic_fetch_xor_release
+# ifndef orig_atomic_fetch_xor_release
 /* Failing the atomic_compare_exchange_weak_release reloads the value in
    __atg104_expected, so we need only do the XOR again and retry.  */
-# define atomic_fetch_xor_release(mem, operand) \
+# define orig_atomic_fetch_xor_release(mem, operand) \
   ({ __typeof (mem) __atg104_memp = (mem);				      \
      __typeof (*(mem)) __atg104_expected = (*__atg104_memp);		      \
      __typeof (*(mem)) __atg104_desired;				      \
@@ -804,7 +804,7 @@ void __atomic_link_error (void);
      do									      \
        __atg104_desired = __atg104_expected ^ __atg104_op;		      \
      while (__glibc_unlikely						      \
-	    (atomic_compare_exchange_weak_release (			      \
+	    (orig_atomic_compare_exchange_weak_release (			      \
 	       __atg104_memp, &__atg104_expected, __atg104_desired)	      \
 	     == 0));							      \
      __atg104_expected; })
diff --git a/inet/getnameinfo.c b/inet/getnameinfo.c
index acb2565b5f..6fc5365f7c 100644
--- a/inet/getnameinfo.c
+++ b/inet/getnameinfo.c
@@ -88,8 +88,6 @@ nrl_domainname (void)
 {
   static int not_first;
 
-  if (! not_first)
-    {
       __libc_lock_define_initialized (static, lock);
       __libc_lock_lock (lock);
 
@@ -179,7 +177,6 @@ nrl_domainname (void)
 	}
 
       __libc_lock_unlock (lock);
-    }
 
   return domain;
 };
diff --git a/malloc/arena.c b/malloc/arena.c
index cecdb7f4c4..034b449804 100644
--- a/malloc/arena.c
+++ b/malloc/arena.c
@@ -154,7 +154,7 @@ __malloc_fork_lock_parent (void)
   for (mstate ar_ptr = &main_arena;; )
     {
       __libc_lock_lock (ar_ptr->mutex);
-      ar_ptr = ar_ptr->next;
+      ar_ptr = atomic_load_relaxed(&ar_ptr->next);
       if (ar_ptr == &main_arena)
         break;
     }
@@ -169,7 +169,7 @@ __malloc_fork_unlock_parent (void)
   for (mstate ar_ptr = &main_arena;; )
     {
       __libc_lock_unlock (ar_ptr->mutex);
-      ar_ptr = ar_ptr->next;
+      ar_ptr = atomic_load_relaxed(&ar_ptr->next);
       if (ar_ptr == &main_arena)
         break;
     }
@@ -195,10 +195,10 @@ __malloc_fork_unlock_child (void)
         {
 	  /* This arena is no longer attached to any thread.  */
 	  ar_ptr->attached_threads = 0;
-          ar_ptr->next_free = free_list;
+	  ar_ptr->next_free = free_list;
           free_list = ar_ptr;
         }
-      ar_ptr = ar_ptr->next;
+      ar_ptr = atomic_load_relaxed(&ar_ptr->next);
       if (ar_ptr == &main_arena)
         break;
     }
@@ -453,7 +453,7 @@ static heap_info *
 new_heap (size_t size, size_t top_pad)
 {
   size_t pagesize = GLRO (dl_pagesize);
-  char *p1, *p2;
+  char *p1, *p2, *prev_heap_area;
   unsigned long ul;
   heap_info *h;
 
@@ -472,12 +472,13 @@ new_heap (size_t size, size_t top_pad)
      mapping (on Linux, this is the case for all non-writable mappings
      anyway). */
   p2 = MAP_FAILED;
-  if (aligned_heap_area)
+  prev_heap_area = atomic_load_acquire(&aligned_heap_area);
+  if (prev_heap_area)
     {
-      p2 = (char *) MMAP (aligned_heap_area, HEAP_MAX_SIZE, PROT_NONE,
+      p2 = (char *) MMAP (prev_heap_area, HEAP_MAX_SIZE, PROT_NONE,
                           MAP_NORESERVE);
-      aligned_heap_area = NULL;
-      if (p2 != MAP_FAILED && ((unsigned long) p2 & (HEAP_MAX_SIZE - 1)))
+      atomic_store_release(&aligned_heap_area, NULL);
+      if (p2 != MAP_FAILED && !mvee_all_heaps_aligned(p2, HEAP_MAX_SIZE))
         {
           __munmap (p2, HEAP_MAX_SIZE);
           p2 = MAP_FAILED;
@@ -485,6 +486,7 @@ new_heap (size_t size, size_t top_pad)
     }
   if (p2 == MAP_FAILED)
     {
+	  (void) mvee_all_heaps_aligned(0, HEAP_MAX_SIZE);
       p1 = (char *) MMAP (0, HEAP_MAX_SIZE << 1, PROT_NONE, MAP_NORESERVE);
       if (p1 != MAP_FAILED)
         {
@@ -493,9 +495,8 @@ new_heap (size_t size, size_t top_pad)
           ul = p2 - p1;
           if (ul)
             __munmap (p1, ul);
-          else
-            aligned_heap_area = p2 + HEAP_MAX_SIZE;
           __munmap (p2 + HEAP_MAX_SIZE, HEAP_MAX_SIZE - ul);
+		  atomic_store_release(&aligned_heap_area, p2 + HEAP_MAX_SIZE);
         }
       else
         {
@@ -505,7 +506,7 @@ new_heap (size_t size, size_t top_pad)
           if (p2 == MAP_FAILED)
             return 0;
 
-          if ((unsigned long) p2 & (HEAP_MAX_SIZE - 1))
+          if (!mvee_all_heaps_aligned(p2, HEAP_MAX_SIZE))
             {
               __munmap (p2, HEAP_MAX_SIZE);
               return 0;
@@ -587,8 +588,8 @@ shrink_heap (heap_info *h, long diff)
 
 #define delete_heap(heap) \
   do {									      \
-      if ((char *) (heap) + HEAP_MAX_SIZE == aligned_heap_area)		      \
-        aligned_heap_area = NULL;					      \
+	if ((char *) (heap) + HEAP_MAX_SIZE == atomic_load_acquire(&aligned_heap_area)) \
+	  atomic_store_release(&aligned_heap_area, NULL);							\
       __munmap ((char *) (heap), HEAP_MAX_SIZE);			      \
     } while (0)
 
@@ -640,7 +641,7 @@ heap_trim (heap_info *heap, size_t pad)
      page.  */
   top_size = chunksize (top_chunk);
   if ((unsigned long)(top_size) <
-      (unsigned long)(mp_.trim_threshold))
+      (unsigned long)(atomic_load_relaxed(&mp_.trim_threshold)))
     return 0;
 
   top_area = top_size - MINSIZE - 1;
@@ -660,6 +661,7 @@ heap_trim (heap_info *heap, size_t pad)
 
   /* Success. Adjust top accordingly. */
   set_head (top_chunk, (top_size - extra) | PREV_INUSE);
+
   /*check_chunk(ar_ptr, top_chunk);*/
   return 1;
 }
@@ -723,12 +725,13 @@ _int_new_arena (size_t size)
   __libc_lock_lock (list_lock);
 
   /* Add the new arena to the global list.  */
-  a->next = main_arena.next;
+  struct malloc_state* tmp = atomic_load_relaxed(&main_arena.next);
+  atomic_store_relaxed(&a->next, tmp);
   /* FIXME: The barrier is an attempt to synchronize with read access
      in reused_arena, which does not acquire list_lock while
      traversing the list.  */
   atomic_write_barrier ();
-  main_arena.next = a;
+  atomic_store_relaxed(&main_arena.next, a);
 
   __libc_lock_unlock (list_lock);
 
@@ -757,9 +760,7 @@ static mstate
 get_free_list (void)
 {
   mstate replaced_arena = thread_arena;
-  mstate result = free_list;
-  if (result != NULL)
-    {
+  mstate result;
       __libc_lock_lock (free_list_lock);
       result = free_list;
       if (result != NULL)
@@ -780,7 +781,6 @@ get_free_list (void)
           __libc_lock_lock (result->mutex);
 	  thread_arena = result;
         }
-    }
 
   return result;
 }
@@ -814,26 +814,26 @@ reused_arena (mstate avoid_arena)
   mstate result;
   /* FIXME: Access to next_to_use suffers from data races.  */
   static mstate next_to_use;
-  if (next_to_use == NULL)
-    next_to_use = &main_arena;
+  if (atomic_load_relaxed(&next_to_use) == NULL)
+    atomic_store_relaxed(&next_to_use, &main_arena);
 
   /* Iterate over all arenas (including those linked from
      free_list).  */
-  result = next_to_use;
+  result = atomic_load_relaxed(&next_to_use);
   do
     {
       if (!__libc_lock_trylock (result->mutex))
         goto out;
 
       /* FIXME: This is a data race, see _int_new_arena.  */
-      result = result->next;
+      result = atomic_load_relaxed(&result->next);
     }
-  while (result != next_to_use);
+  while (result != atomic_load_relaxed(&next_to_use));
 
   /* Avoid AVOID_ARENA as we have already failed to allocate memory
      in that arena and it is currently locked.   */
   if (result == avoid_arena)
-    result = result->next;
+    result = atomic_load_relaxed(&result->next);
 
   /* No arena available without contention.  Wait for the next in line.  */
   LIBC_PROBE (memory_arena_reuse_wait, 3, &result->mutex, result, avoid_arena);
@@ -864,7 +864,8 @@ out:
 
   LIBC_PROBE (memory_arena_reuse, 2, result, avoid_arena);
   thread_arena = result;
-  next_to_use = result->next;
+  mstate tmp = atomic_load_relaxed(&result->next);
+  atomic_store_relaxed(&next_to_use, tmp);
 
   return result;
 }
@@ -880,24 +881,24 @@ arena_get2 (size_t size, mstate avoid_arena)
   if (a == NULL)
     {
       /* Nothing immediately available, so generate a new arena.  */
-      if (narenas_limit == 0)
+      if (atomic_load_relaxed(&narenas_limit) == 0)
         {
           if (mp_.arena_max != 0)
-            narenas_limit = mp_.arena_max;
-          else if (narenas > mp_.arena_test)
+            atomic_store_release(&narenas_limit, mp_.arena_max);
+          else if (atomic_load_relaxed(&narenas) > mp_.arena_test)
             {
               int n = __get_nprocs ();
 
               if (n >= 1)
-                narenas_limit = NARENAS_FROM_NCORES (n);
+                atomic_store_release(&narenas_limit, NARENAS_FROM_NCORES (n));
               else
                 /* We have no information about the system.  Assume two
                    cores.  */
-                narenas_limit = NARENAS_FROM_NCORES (2);
+                atomic_store_release(&narenas_limit, NARENAS_FROM_NCORES (2));
             }
         }
     repeat:;
-      size_t n = narenas;
+      size_t n = atomic_load_relaxed(&narenas);
       /* NB: the following depends on the fact that (size_t)0 - 1 is a
          very large number and that the underflow is OK.  If arena_max
          is set the value of arena_test is irrelevant.  If arena_test
@@ -905,7 +906,7 @@ arena_get2 (size_t size, mstate avoid_arena)
          narenas_limit is 0.  There is no possibility for narenas to
          be too big for the test to always fail since there is not
          enough address space to create that many arenas.  */
-      if (__glibc_unlikely (n <= narenas_limit - 1))
+      if (__glibc_unlikely (n <= atomic_load_relaxed(&narenas_limit) - 1))
         {
           if (catomic_compare_and_exchange_bool_acq (&narenas, n + 1, n))
             goto repeat;
diff --git a/malloc/hooks.c b/malloc/hooks.c
index a2b93e5446..064d1293a3 100644
--- a/malloc/hooks.c
+++ b/malloc/hooks.c
@@ -27,7 +27,7 @@
 static void *
 malloc_hook_ini (size_t sz, const void *caller)
 {
-  __malloc_hook = NULL;
+  atomic_store_relaxed(&__malloc_hook, NULL);
   ptmalloc_init ();
   return __libc_malloc (sz);
 }
@@ -35,8 +35,8 @@ malloc_hook_ini (size_t sz, const void *caller)
 static void *
 realloc_hook_ini (void *ptr, size_t sz, const void *caller)
 {
-  __malloc_hook = NULL;
-  __realloc_hook = NULL;
+  atomic_store_relaxed(&__malloc_hook, NULL);
+  atomic_store_relaxed(&__realloc_hook, NULL);
   ptmalloc_init ();
   return __libc_realloc (ptr, sz);
 }
@@ -44,7 +44,7 @@ realloc_hook_ini (void *ptr, size_t sz, const void *caller)
 static void *
 memalign_hook_ini (size_t alignment, size_t sz, const void *caller)
 {
-  __memalign_hook = NULL;
+  atomic_store_relaxed(&__memalign_hook, NULL);
   ptmalloc_init ();
   return __libc_memalign (alignment, sz);
 }
@@ -57,10 +57,10 @@ void
 __malloc_check_init (void)
 {
   using_malloc_checking = 1;
-  __malloc_hook = malloc_check;
-  __free_hook = free_check;
-  __realloc_hook = realloc_check;
-  __memalign_hook = memalign_check;
+  atomic_store_relaxed(&__malloc_hook, malloc_check);
+  atomic_store_relaxed(&__free_hook, free_check);
+  atomic_store_relaxed(&__realloc_hook, realloc_check);
+  atomic_store_relaxed(&__memalign_hook, memalign_check);
 }
 
 /* A simple, standard set of debugging hooks.  Overhead is `only' one
@@ -456,10 +456,10 @@ malloc_set_state (void *msptr)
      cannot be more than one thread when we reach this point.  */
 
   /* Disable the malloc hooks (and malloc checking).  */
-  __malloc_hook = NULL;
-  __realloc_hook = NULL;
-  __free_hook = NULL;
-  __memalign_hook = NULL;
+  atomic_store_relaxed(&__malloc_hook, NULL);
+  atomic_store_relaxed(&__realloc_hook, NULL);
+  atomic_store_relaxed(&__free_hook, NULL);
+  atomic_store_relaxed(&__memalign_hook, NULL);
   using_malloc_checking = 0;
 
   /* Patch the dumped heap.  We no longer try to integrate into the
diff --git a/malloc/malloc.c b/malloc/malloc.c
index f7cd29bc2f..137b054761 100644
--- a/malloc/malloc.c
+++ b/malloc/malloc.c
@@ -1,3 +1,4 @@
+/* -*- Mode:C++; c-file-style: "gnu"; c-basic-offset: 2; tab-width: 8; indent-tabs-mode: nil; -*- */
 /* Malloc implementation for multiple threads without lock contention.
    Copyright (C) 1996-2020 Free Software Foundation, Inc.
    This file is part of the GNU C Library.
@@ -1591,6 +1592,27 @@ typedef struct malloc_chunk *mfastbinptr;
 
 #define FASTBIN_CONSOLIDATION_THRESHOLD  (65536UL)
 
+/*
+   Since the lowest 2 bits in max_fast don't matter in size comparisons,
+   they are used as flags.
+ */
+
+/*
+   FASTCHUNKS_BIT held in max_fast indicates that there are probably
+   some fastbin chunks. It is set true on entering a chunk into any
+   fastbin, and cleared only in malloc_consolidate.
+
+   The truth value is inverted so that have_fastchunks will be true
+   upon startup (since statics are zero-filled), simplifying
+   initialization checks.
+ */
+
+#define FASTCHUNKS_BIT        (1U)
+
+#define have_fastchunks(M)     ((atomic_load_acquire(&(M)->flags) & FASTCHUNKS_BIT) == 0)
+#define clear_fastchunks(M)    catomic_or (&(M)->flags, FASTCHUNKS_BIT)
+#define set_fastchunks(M)      catomic_and (&(M)->flags, ~FASTCHUNKS_BIT)
+
 /*
    NONCONTIGUOUS_BIT indicates that MORECORE does not return contiguous
    regions.  Otherwise, contiguity is exploited in merging together,
@@ -1696,17 +1718,28 @@ struct malloc_state
   INTERNAL_SIZE_T max_system_mem;
 };
 
+/* 
+ * stijn: This data structure is a huge pain if it is modified at run time.
+ * Run-time updates of any of these fields have radical implications on the
+ * allocation behavior, many of which result in divergences in the MVEE.
+ *
+ * Some fields are generally only modified by mallopt, which -- I assume -- 
+ * people will only use in a single-threaded context.
+ * Other fields (e.g., trim_threshold and mmap_threshold) are also modified
+ * on regular code paths, which means we have to wrap and order accesses
+ * to said fields.
+ */
 struct malloc_par
 {
   /* Tunable parameters */
-  unsigned long trim_threshold;
+  unsigned long trim_threshold; /* stijn: racy access on regular code paths */
   INTERNAL_SIZE_T top_pad;
-  INTERNAL_SIZE_T mmap_threshold;
+  INTERNAL_SIZE_T mmap_threshold; /* stijn: racy access on regular code paths */
   INTERNAL_SIZE_T arena_test;
   INTERNAL_SIZE_T arena_max;
 
   /* Memory map support */
-  int n_mmaps;
+  int n_mmaps; /* stijn: racy access on regular code paths */
   int n_mmaps_max;
   int max_n_mmaps;
   /* the mmap_threshold is dynamic, until the user sets
@@ -1715,11 +1748,11 @@ struct malloc_par
   int no_dyn_threshold;
 
   /* Statistics */
-  INTERNAL_SIZE_T mmapped_mem;
-  INTERNAL_SIZE_T max_mmapped_mem;
+  INTERNAL_SIZE_T mmapped_mem; /* stijn: racy access on regular code paths */
+  INTERNAL_SIZE_T max_mmapped_mem; /* stijn: racy access on regular code paths */
 
   /* First address handed out by MORECORE/sbrk.  */
-  char *sbrk_base;
+  char *sbrk_base; /* stijn: racy - but probably impossible to trigger the race... */
 
 #if USE_TCACHE
   /* Maximum number of buckets to use.  */
@@ -2138,7 +2171,7 @@ do_check_malloc_state (mstate av)
 
   for (i = 0; i < NFASTBINS; ++i)
     {
-      p = fastbin (av, i);
+      p = atomic_load_acquire(&fastbin (av, i));
 
       /* The following test can only be performed for the main arena.
          While mallopt calls malloc_consolidate to get rid of all fast
@@ -2284,8 +2317,8 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av)
    */
 
   if (av == NULL
-      || ((unsigned long) (nb) >= (unsigned long) (mp_.mmap_threshold)
-	  && (mp_.n_mmaps < mp_.n_mmaps_max)))
+      || ((unsigned long) (nb) >= (unsigned long) (atomic_load_relaxed(&mp_.mmap_threshold))
+		  && (atomic_load_acquire(&mp_.n_mmaps) < mp_.n_mmaps_max)))
     {
       char *mm;           /* return value from mmap call*/
 
@@ -3105,13 +3138,14 @@ __libc_free (void *mem)
     {
       /* See if the dynamic brk/mmap threshold needs adjusting.
 	 Dumped fake mmapped chunks do not affect the threshold.  */
-      if (!mp_.no_dyn_threshold
-          && chunksize_nomask (p) > mp_.mmap_threshold
+      if (!atomic_load_relaxed(&mp_.no_dyn_threshold)
+          && chunksize_nomask (p) > atomic_load_relaxed(&mp_.mmap_threshold)
           && chunksize_nomask (p) <= DEFAULT_MMAP_THRESHOLD_MAX
 	  && !DUMPED_MAIN_ARENA_CHUNK (p))
         {
-          mp_.mmap_threshold = chunksize (p);
-          mp_.trim_threshold = 2 * mp_.mmap_threshold;
+          atomic_store_relaxed(&mp_.mmap_threshold, chunksize (p));
+          unsigned long new_trim_threshold = 2 * atomic_load_relaxed(&mp_.mmap_threshold);
+          atomic_store_relaxed(&mp_.trim_threshold, new_trim_threshold);
           LIBC_PROBE (memory_mallopt_free_dyn_thresholds, 2,
                       mp_.mmap_threshold, mp_.trim_threshold);
         }
@@ -3507,7 +3541,6 @@ __libc_calloc (size_t n, size_t elem_size)
 /*
    ------------------------------ malloc ------------------------------
  */
-
 static void *
 _int_malloc (mstate av, size_t bytes)
 {
@@ -3579,7 +3612,7 @@ _int_malloc (mstate av, size_t bytes)
       idx = fastbin_index (nb);
       mfastbinptr *fb = &fastbin (av, idx);
       mchunkptr pp;
-      victim = *fb;
+      victim = atomic_load_acquire(fb);
 
       if (victim != NULL)
 	{
@@ -3603,7 +3636,7 @@ _int_malloc (mstate av, size_t bytes)
 
 		  /* While bin not empty and tcache not full, copy chunks.  */
 		  while (tcache->counts[tc_idx] < mp_.tcache_count
-			 && (tc_victim = *fb) != NULL)
+			 && (tc_victim = atomic_load_acquire(fb)) != NULL)
 		    {
 		      if (SINGLE_THREAD_P)
 			*fb = tc_victim->fd;
@@ -4256,7 +4289,7 @@ _int_free (mstate av, mchunkptr p, int have_lock)
     fb = &fastbin (av, idx);
 
     /* Atomically link P to its fastbin: P->FD = *FB; *FB = P;  */
-    mchunkptr old = *fb, old2;
+    mchunkptr old = atomic_load_acquire(fb), old2;
 
     if (SINGLE_THREAD_P)
       {
@@ -4402,7 +4435,7 @@ _int_free (mstate av, mchunkptr p, int have_lock)
       if (av == &main_arena) {
 #ifndef MORECORE_CANNOT_TRIM
 	if ((unsigned long)(chunksize(av->top)) >=
-	    (unsigned long)(mp_.trim_threshold))
+	    (unsigned long)(atomic_load_relaxed(&mp_.trim_threshold)))
 	  systrim(mp_.top_pad, av);
 #endif
       } else {
@@ -4822,7 +4855,7 @@ __malloc_trim (size_t s)
       result |= mtrim (ar_ptr, s);
       __libc_lock_unlock (ar_ptr->mutex);
 
-      ar_ptr = ar_ptr->next;
+      ar_ptr = atomic_load_relaxed(&ar_ptr->next);
     }
   while (ar_ptr != &main_arena);
 
@@ -4896,7 +4929,7 @@ int_mallinfo (mstate av, struct mallinfo *m)
 
   for (i = 0; i < NFASTBINS; ++i)
     {
-      for (p = fastbin (av, i); p != 0; p = p->fd)
+      for (p = atomic_load_acquire(&fastbin (av, i)); p != 0; p = p->fd)
         {
           ++nfastblocks;
           fastavail += chunksize (p);
@@ -4924,8 +4957,8 @@ int_mallinfo (mstate av, struct mallinfo *m)
   m->fsmblks += fastavail;
   if (av == &main_arena)
     {
-      m->hblks = mp_.n_mmaps;
-      m->hblkhd = mp_.mmapped_mem;
+      m->hblks = atomic_load_acquire(&mp_.n_mmaps);
+      m->hblkhd = atomic_load_relaxed(&mp_.mmapped_mem);
       m->usmblks = 0;
       m->keepcost = chunksize (av->top);
     }
@@ -4949,7 +4982,7 @@ __libc_mallinfo (void)
       int_mallinfo (ar_ptr, &m);
       __libc_lock_unlock (ar_ptr->mutex);
 
-      ar_ptr = ar_ptr->next;
+      ar_ptr = atomic_load_relaxed(&ar_ptr->next);
     }
   while (ar_ptr != &main_arena);
 
@@ -4965,7 +4998,7 @@ __malloc_stats (void)
 {
   int i;
   mstate ar_ptr;
-  unsigned int in_use_b = mp_.mmapped_mem, system_b = in_use_b;
+  unsigned int in_use_b = atomic_load_acquire(&mp_.mmapped_mem), system_b = in_use_b;
 
   if (__malloc_initialized < 0)
     ptmalloc_init ();
@@ -4989,16 +5022,16 @@ __malloc_stats (void)
       system_b += mi.arena;
       in_use_b += mi.uordblks;
       __libc_lock_unlock (ar_ptr->mutex);
-      ar_ptr = ar_ptr->next;
+      ar_ptr = atomic_load_relaxed(&ar_ptr->next);
       if (ar_ptr == &main_arena)
         break;
     }
   fprintf (stderr, "Total (incl. mmap):\n");
   fprintf (stderr, "system bytes     = %10u\n", system_b);
   fprintf (stderr, "in use bytes     = %10u\n", in_use_b);
-  fprintf (stderr, "max mmap regions = %10u\n", (unsigned int) mp_.max_n_mmaps);
+  fprintf (stderr, "max mmap regions = %10u\n", (unsigned int) atomic_load_relaxed(&mp_.max_n_mmaps));
   fprintf (stderr, "max mmap bytes   = %10lu\n",
-           (unsigned long) mp_.max_mmapped_mem);
+           (unsigned long) atomic_load_relaxed(&mp_.max_mmapped_mem));
   stderr->_flags2 = old_flags2;
   _IO_funlockfile (stderr);
 }
@@ -5429,7 +5462,7 @@ __malloc_info (int options, FILE *fp)
 
       for (size_t i = 0; i < NFASTBINS; ++i)
 	{
-	  mchunkptr p = fastbin (ar_ptr, i);
+	  mchunkptr p = atomic_load_acquire(&fastbin (ar_ptr, i));
 	  if (p != NULL)
 	    {
 	      size_t nthissize = 0;
@@ -5554,7 +5587,7 @@ __malloc_info (int options, FILE *fp)
 	}
 
       fputs ("</heap>\n", fp);
-      ar_ptr = ar_ptr->next;
+      ar_ptr = atomic_load_relaxed(&ar_ptr->next);
     }
   while (ar_ptr != &main_arena);
 
@@ -5568,7 +5601,7 @@ __malloc_info (int options, FILE *fp)
 	   "<aspace type=\"mprotect\" size=\"%zu\"/>\n"
 	   "</malloc>\n",
 	   total_nfastblocks, total_fastavail, total_nblocks, total_avail,
-	   mp_.n_mmaps, mp_.mmapped_mem,
+		   atomic_load_relaxed(&mp_.n_mmaps), atomic_load_relaxed(&mp_.mmapped_mem),
 	   total_system, total_max_system,
 	   total_aspace, total_aspace_mprotect);
 
diff --git a/malloc/mcheck.c b/malloc/mcheck.c
index 9cd10d7a5e..a6f6656ed5 100644
--- a/malloc/mcheck.c
+++ b/malloc/mcheck.c
@@ -189,12 +189,12 @@ freehook (void *ptr, const void *caller)
       flood (ptr, FREEFLOOD, hdr->size);
       ptr = hdr->block;
     }
-  __free_hook = old_free_hook;
+  atomic_store_relaxed(&__free_hook, old_free_hook);
   if (old_free_hook != NULL)
     (*old_free_hook)(ptr, caller);
   else
     free (ptr);
-  __free_hook = freehook;
+  atomic_store_relaxed(&__free_hook, freehook);
 }
 
 static void *
@@ -211,13 +211,13 @@ mallochook (size_t size, const void *caller)
       return NULL;
     }
 
-  __malloc_hook = old_malloc_hook;
+  atomic_store_relaxed(&__malloc_hook, old_malloc_hook);
   if (old_malloc_hook != NULL)
     hdr = (struct hdr *) (*old_malloc_hook)(sizeof (struct hdr) + size + 1,
                                             caller);
   else
     hdr = (struct hdr *) malloc (sizeof (struct hdr) + size + 1);
-  __malloc_hook = mallochook;
+  atomic_store_relaxed(&__malloc_hook, mallochook);
   if (hdr == NULL)
     return NULL;
 
@@ -249,12 +249,12 @@ memalignhook (size_t alignment, size_t size,
       return NULL;
     }
 
-  __memalign_hook = old_memalign_hook;
+  atomic_store_relaxed(&__memalign_hook, old_memalign_hook);
   if (old_memalign_hook != NULL)
     block = (*old_memalign_hook)(alignment, slop + size + 1, caller);
   else
     block = memalign (alignment, slop + size + 1);
-  __memalign_hook = memalignhook;
+  atomic_store_relaxed(&__memalign_hook, memalignhook);
   if (block == NULL)
     return NULL;
 
@@ -305,10 +305,10 @@ reallochook (void *ptr, size_t size, const void *caller)
       osize = 0;
       hdr = NULL;
     }
-  __free_hook = old_free_hook;
-  __malloc_hook = old_malloc_hook;
-  __memalign_hook = old_memalign_hook;
-  __realloc_hook = old_realloc_hook;
+  atomic_store_relaxed(&__free_hook, old_free_hook);
+  atomic_store_relaxed(&__malloc_hook, old_malloc_hook);
+  atomic_store_relaxed(&__memalign_hook, old_memalign_hook);
+  atomic_store_relaxed(&__realloc_hook, old_realloc_hook);
   if (old_realloc_hook != NULL)
     hdr = (struct hdr *) (*old_realloc_hook)((void *) hdr,
                                              sizeof (struct hdr) + size + 1,
@@ -316,10 +316,10 @@ reallochook (void *ptr, size_t size, const void *caller)
   else
     hdr = (struct hdr *) realloc ((void *) hdr,
                                   sizeof (struct hdr) + size + 1);
-  __free_hook = freehook;
-  __malloc_hook = mallochook;
-  __memalign_hook = memalignhook;
-  __realloc_hook = reallochook;
+  atomic_store_relaxed(&__free_hook, freehook);
+  atomic_store_relaxed(&__malloc_hook, mallochook);
+  atomic_store_relaxed(&__memalign_hook, memalignhook);
+  atomic_store_relaxed(&__realloc_hook, reallochook);
   if (hdr == NULL)
     return NULL;
 
@@ -383,15 +383,15 @@ mcheck (void (*func) (enum mcheck_status))
       p = malloc_opt_barrier (p);
       free (p);
 
-      old_free_hook = __free_hook;
-      __free_hook = freehook;
-      old_malloc_hook = __malloc_hook;
-      __malloc_hook = mallochook;
-      old_memalign_hook = __memalign_hook;
-      __memalign_hook = memalignhook;
-      old_realloc_hook = __realloc_hook;
-      __realloc_hook = reallochook;
-      mcheck_used = 1;
+      old_free_hook = atomic_load_relaxed(&__free_hook);
+      atomic_store_relaxed(&__free_hook, freehook);
+      old_malloc_hook = atomic_load_relaxed(&__malloc_hook);
+      atomic_store_relaxed(&__malloc_hook, mallochook);
+      old_memalign_hook = atomic_load_relaxed(&__memalign_hook);
+      atomic_store_relaxed(&__memalign_hook, memalignhook);
+      old_realloc_hook = atomic_load_relaxed(&__realloc_hook);
+	  atomic_store_relaxed(&__realloc_hook, reallochook);      
+	  mcheck_used = 1;
     }
 
   return mcheck_used ? 0 : -1;
diff --git a/malloc/mtrace.c b/malloc/mtrace.c
index 7e7719df97..2b8f7a90af 100644
--- a/malloc/mtrace.c
+++ b/malloc/mtrace.c
@@ -130,30 +130,30 @@ static void * tr_memalignhook (size_t, size_t, const void *);
 static __always_inline void
 set_default_hooks (void)
 {
-  __free_hook = tr_old_free_hook;
-  __malloc_hook = tr_old_malloc_hook;
-  __realloc_hook = tr_old_realloc_hook;
-  __memalign_hook = tr_old_memalign_hook;
+  atomic_store_relaxed(&__free_hook, tr_old_free_hook);
+  atomic_store_relaxed(&__malloc_hook, tr_old_malloc_hook);
+  atomic_store_relaxed(&__realloc_hook, tr_old_realloc_hook);
+  atomic_store_relaxed(&__memalign_hook, tr_old_memalign_hook);
 }
 
 /* Set all of the tracing hooks used for mtrace.  */
 static __always_inline void
 set_trace_hooks (void)
 {
-  __free_hook = tr_freehook;
-  __malloc_hook = tr_mallochook;
-  __realloc_hook = tr_reallochook;
-  __memalign_hook = tr_memalignhook;
+  atomic_store_relaxed(&__free_hook, tr_freehook);
+  atomic_store_relaxed(&__malloc_hook, tr_mallochook);
+  atomic_store_relaxed(&__realloc_hook, tr_reallochook);
+  atomic_store_relaxed(&__memalign_hook, tr_memalignhook);
 }
 
 /* Save the current set of hooks as the default hooks.  */
 static __always_inline void
 save_default_hooks (void)
 {
-  tr_old_free_hook = __free_hook;
-  tr_old_malloc_hook = __malloc_hook;
-  tr_old_realloc_hook = __realloc_hook;
-  tr_old_memalign_hook = __memalign_hook;
+  tr_old_free_hook = atomic_load_relaxed(&__free_hook);
+  tr_old_malloc_hook = atomic_load_relaxed(&__malloc_hook);
+  tr_old_realloc_hook = atomic_load_relaxed(&__realloc_hook);
+  tr_old_memalign_hook = atomic_load_relaxed(&__memalign_hook);
 }
 
 static void
diff --git a/nptl/allocatestack.c b/nptl/allocatestack.c
index 110ba18f5d..41bc487c7e 100644
--- a/nptl/allocatestack.c
+++ b/nptl/allocatestack.c
@@ -212,7 +212,7 @@ get_cached_stack (size_t *sizep, void **memp)
     }
 
   /* Don't allow setxid until cloned.  */
-  result->setxid_futex = -1;
+  atomic_store_relaxed(&result->setxid_futex, -1);
 
   /* Dequeue the entry.  */
   stack_list_del (&result->list);
@@ -231,7 +231,7 @@ get_cached_stack (size_t *sizep, void **memp)
   *memp = result->stackblock;
 
   /* Cancellation handling is back to the default.  */
-  result->cancelhandling = 0;
+  atomic_store_relaxed(&result->cancelhandling, 0);
   result->cleanup = NULL;
 
   /* No pending event.  */
@@ -496,7 +496,7 @@ allocate_stack (const struct pthread_attr *attr, struct pthread **pdp,
 #endif
 
       /* Don't allow setxid until cloned.  */
-      pd->setxid_futex = -1;
+      atomic_store_relaxed(&pd->setxid_futex, -1);
 
       /* Allocate the DTV for this thread.  */
       if (_dl_allocate_tls (TLS_TPADJ (pd)) == NULL)
@@ -616,7 +616,7 @@ allocate_stack (const struct pthread_attr *attr, struct pthread **pdp,
 #endif
 
 	  /* Don't allow setxid until cloned.  */
-	  pd->setxid_futex = -1;
+	  atomic_store_relaxed(&pd->setxid_futex, -1);
 
 	  /* Allocate the DTV for this thread.  */
 	  if (_dl_allocate_tls (TLS_TPADJ (pd)) == NULL)
@@ -908,7 +908,7 @@ __reclaim_stacks (void)
       if (curp != self)
 	{
 	  /* This marks the stack as free.  */
-	  curp->tid = 0;
+		atomic_store_relaxed(&curp->tid, 0);
 
 	  /* Account for the size of the stack.  */
 	  stack_cache_actsize += curp->stackblock_size;
@@ -969,18 +969,18 @@ setxid_mark_thread (struct xid_command *cmdp, struct pthread *t)
   int ch;
 
   /* Wait until this thread is cloned.  */
-  if (t->setxid_futex == -1
+  if (atomic_load_relaxed(&t->setxid_futex) == -1
       && ! atomic_compare_and_exchange_bool_acq (&t->setxid_futex, -2, -1))
     do
       futex_wait_simple (&t->setxid_futex, -2, FUTEX_PRIVATE);
-    while (t->setxid_futex == -2);
+    while (atomic_load_relaxed(&t->setxid_futex) == -2);
 
   /* Don't let the thread exit before the setxid handler runs.  */
-  t->setxid_futex = 0;
+  atomic_store_release(&t->setxid_futex, 0);
 
   do
     {
-      ch = t->cancelhandling;
+		ch = atomic_load_relaxed(&t->cancelhandling);
 
       /* If the thread is exiting right now, ignore it.  */
       if ((ch & EXITING_BITMASK) != 0)
@@ -989,7 +989,7 @@ setxid_mark_thread (struct xid_command *cmdp, struct pthread *t)
 	     progress.  */
 	  if ((ch & SETXID_BITMASK) == 0)
 	    {
-	      t->setxid_futex = 1;
+			atomic_store_release(&t->setxid_futex, 1);
 	      futex_wake (&t->setxid_futex, 1, FUTEX_PRIVATE);
 	    }
 	  return;
@@ -1007,7 +1007,7 @@ setxid_unmark_thread (struct xid_command *cmdp, struct pthread *t)
 
   do
     {
-      ch = t->cancelhandling;
+		ch = atomic_load_relaxed(&t->cancelhandling);
       if ((ch & SETXID_BITMASK) == 0)
 	return;
     }
@@ -1015,7 +1015,7 @@ setxid_unmark_thread (struct xid_command *cmdp, struct pthread *t)
 					       ch & ~SETXID_BITMASK, ch));
 
   /* Release the futex just in case.  */
-  t->setxid_futex = 1;
+  atomic_store_release(&t->setxid_futex, 1);
   futex_wake (&t->setxid_futex, 1, FUTEX_PRIVATE);
 }
 
@@ -1023,7 +1023,7 @@ setxid_unmark_thread (struct xid_command *cmdp, struct pthread *t)
 static int
 setxid_signal_thread (struct xid_command *cmdp, struct pthread *t)
 {
-  if ((t->cancelhandling & SETXID_BITMASK) == 0)
+	if ((atomic_load_relaxed(&t->cancelhandling) & SETXID_BITMASK) == 0)
     return 0;
 
   int val;
diff --git a/nptl/cancellation.c b/nptl/cancellation.c
index 826071321e..c77b2285db 100644
--- a/nptl/cancellation.c
+++ b/nptl/cancellation.c
@@ -32,7 +32,7 @@ attribute_hidden
 __pthread_enable_asynccancel (void)
 {
   struct pthread *self = THREAD_SELF;
-  int oldval = THREAD_GETMEM (self, cancelhandling);
+  int oldval = THREAD_ATOMIC_GETMEM (self, cancelhandling);
 
   while (1)
     {
@@ -47,7 +47,7 @@ __pthread_enable_asynccancel (void)
 	{
 	  if (CANCEL_ENABLED_AND_CANCELED_AND_ASYNCHRONOUS (newval))
 	    {
-	      THREAD_SETMEM (self, result, PTHREAD_CANCELED);
+	      THREAD_ATOMIC_SETMEM (self, result, PTHREAD_CANCELED);
 	      __do_cancel ();
 	    }
 
@@ -75,7 +75,7 @@ __pthread_disable_asynccancel (int oldtype)
   struct pthread *self = THREAD_SELF;
   int newval;
 
-  int oldval = THREAD_GETMEM (self, cancelhandling);
+  int oldval = THREAD_ATOMIC_GETMEM (self, cancelhandling);
 
   while (1)
     {
@@ -99,6 +99,6 @@ __pthread_disable_asynccancel (int oldtype)
     {
       futex_wait_simple ((unsigned int *) &self->cancelhandling, newval,
 			 FUTEX_PRIVATE);
-      newval = THREAD_GETMEM (self, cancelhandling);
+      newval = THREAD_ATOMIC_GETMEM (self, cancelhandling);
     }
 }
diff --git a/nptl/cleanup_defer.c b/nptl/cleanup_defer.c
index 8ad9a90c50..da254904ba 100644
--- a/nptl/cleanup_defer.c
+++ b/nptl/cleanup_defer.c
@@ -31,7 +31,7 @@ __pthread_register_cancel_defer (__pthread_unwind_buf_t *buf)
   ibuf->priv.data.prev = THREAD_GETMEM (self, cleanup_jmp_buf);
   ibuf->priv.data.cleanup = THREAD_GETMEM (self, cleanup);
 
-  int cancelhandling = THREAD_GETMEM (self, cancelhandling);
+  int cancelhandling = THREAD_ATOMIC_GETMEM (self, cancelhandling);
 
   /* Disable asynchronous cancellation for now.  */
   if (__glibc_unlikely (cancelhandling & CANCELTYPE_BITMASK))
@@ -69,7 +69,7 @@ __pthread_unregister_cancel_restore (__pthread_unwind_buf_t *buf)
 
   int cancelhandling;
   if (ibuf->priv.data.canceltype != PTHREAD_CANCEL_DEFERRED
-      && ((cancelhandling = THREAD_GETMEM (self, cancelhandling))
+      && ((cancelhandling = THREAD_ATOMIC_GETMEM (self, cancelhandling))
 	  & CANCELTYPE_BITMASK) == 0)
     {
       while (1)
diff --git a/nptl/cleanup_defer_compat.c b/nptl/cleanup_defer_compat.c
index 33e47888f2..1a2e91331b 100644
--- a/nptl/cleanup_defer_compat.c
+++ b/nptl/cleanup_defer_compat.c
@@ -29,7 +29,7 @@ _pthread_cleanup_push_defer (struct _pthread_cleanup_buffer *buffer,
   buffer->__arg = arg;
   buffer->__prev = THREAD_GETMEM (self, cleanup);
 
-  int cancelhandling = THREAD_GETMEM (self, cancelhandling);
+  int cancelhandling = THREAD_ATOMIC_GETMEM (self, cancelhandling);
 
   /* Disable asynchronous cancellation for now.  */
   if (__glibc_unlikely (cancelhandling & CANCELTYPE_BITMASK))
@@ -66,7 +66,7 @@ _pthread_cleanup_pop_restore (struct _pthread_cleanup_buffer *buffer,
 
   int cancelhandling;
   if (__builtin_expect (buffer->__canceltype != PTHREAD_CANCEL_DEFERRED, 0)
-      && ((cancelhandling = THREAD_GETMEM (self, cancelhandling))
+      && ((cancelhandling = THREAD_ATOMIC_GETMEM (self, cancelhandling))
 	  & CANCELTYPE_BITMASK) == 0)
     {
       while (1)
diff --git a/nptl/nptl-init.c b/nptl/nptl-init.c
index 1877248014..dc1e0768bd 100644
--- a/nptl/nptl-init.c
+++ b/nptl/nptl-init.c
@@ -152,7 +152,7 @@ sigcancel_handler (int sig, siginfo_t *si, void *ctx)
 
   struct pthread *self = THREAD_SELF;
 
-  int oldval = THREAD_GETMEM (self, cancelhandling);
+  int oldval = THREAD_ATOMIC_GETMEM (self, cancelhandling);
   while (1)
     {
       /* We are canceled now.  When canceled by another thread this flag
@@ -216,14 +216,14 @@ sighandler_setxid (int sig, siginfo_t *si, void *ctx)
   int flags, newval;
   do
     {
-      flags = THREAD_GETMEM (self, cancelhandling);
+      flags = THREAD_ATOMIC_GETMEM (self, cancelhandling);
       newval = THREAD_ATOMIC_CMPXCHG_VAL (self, cancelhandling,
 					  flags & ~SETXID_BITMASK, flags);
     }
   while (flags != newval);
 
   /* And release the futex.  */
-  self->setxid_futex = 1;
+  atomic_store_release(&self->setxid_futex, 1);
   futex_wake (&self->setxid_futex, 1, FUTEX_PRIVATE);
 
   if (atomic_decrement_val (&__xidcmd->cntr) == 0)
diff --git a/nptl/pthreadP.h b/nptl/pthreadP.h
index 7e0ab8ef42..6c3dddbadb 100644
--- a/nptl/pthreadP.h
+++ b/nptl/pthreadP.h
@@ -259,10 +259,10 @@ extern int __pthread_debug attribute_hidden;
 /* Cancellation test.  */
 #define CANCELLATION_P(self) \
   do {									      \
-    int cancelhandling = THREAD_GETMEM (self, cancelhandling);		      \
+    int cancelhandling = THREAD_ATOMIC_GETMEM (self, cancelhandling);		      \
     if (CANCEL_ENABLED_AND_CANCELED (cancelhandling))			      \
       {									      \
-	THREAD_SETMEM (self, result, PTHREAD_CANCELED);			      \
+	THREAD_ATOMIC_SETMEM (self, result, PTHREAD_CANCELED);			      \
 	__do_cancel ();							      \
       }									      \
   } while (0)
diff --git a/nptl/pthread_cancel.c b/nptl/pthread_cancel.c
index 8e7be996e9..516c44f580 100644
--- a/nptl/pthread_cancel.c
+++ b/nptl/pthread_cancel.c
@@ -43,7 +43,7 @@ __pthread_cancel (pthread_t th)
   do
     {
     again:
-      oldval = pd->cancelhandling;
+		oldval = atomic_load_relaxed(&pd->cancelhandling);
       newval = oldval | CANCELING_BITMASK | CANCELED_BITMASK;
 
       /* Avoid doing unnecessary work.  The atomic operation can
diff --git a/nptl/pthread_cond_common.c b/nptl/pthread_cond_common.c
index 3251c7f0ec..f8986d0aed 100644
--- a/nptl/pthread_cond_common.c
+++ b/nptl/pthread_cond_common.c
@@ -53,7 +53,7 @@ static void __attribute__ ((unused))
 __condvar_add_g1_start_relaxed (pthread_cond_t *cond, unsigned int val)
 {
   atomic_store_relaxed (&cond->__data.__g1_start,
-      atomic_load_relaxed (&cond->__data.__g1_start) + val);
+      orig_atomic_load_relaxed (&cond->__data.__g1_start) + val);
 }
 
 #else
diff --git a/nptl/pthread_create.c b/nptl/pthread_create.c
index d3fd58730c..c1ac327765 100644
--- a/nptl/pthread_create.c
+++ b/nptl/pthread_create.c
@@ -405,7 +405,7 @@ START_THREAD_DEFN
   /* If the parent was running cancellation handlers while creating
      the thread the new thread inherited the signal mask.  Reset the
      cancellation signal mask.  */
-  if (__glibc_unlikely (pd->parent_cancelhandling & CANCELING_BITMASK))
+  if (__glibc_unlikely (atomic_load_relaxed(&pd->parent_cancelhandling) & CANCELING_BITMASK))
     {
       INTERNAL_SYSCALL_DECL (err);
       sigset_t mask;
@@ -571,7 +571,7 @@ START_THREAD_DEFN
   if (IS_DETACHED (pd))
     /* Free the TCB.  */
     __free_tcb (pd);
-  else if (__glibc_unlikely (pd->cancelhandling & SETXID_BITMASK))
+  else if (__glibc_unlikely (atomic_load_relaxed(&pd->cancelhandling) & SETXID_BITMASK))
     {
       /* Some other thread might call any of the setXid functions and expect
 	 us to reply.  In this case wait until we did that.  */
@@ -581,10 +581,10 @@ START_THREAD_DEFN
 	   condition used in the surrounding loop (cancelhandling).  We need
 	   to check and document why this is correct.  */
 	futex_wait_simple (&pd->setxid_futex, 0, FUTEX_PRIVATE);
-      while (pd->cancelhandling & SETXID_BITMASK);
+      while (atomic_load_relaxed(&pd->cancelhandling) & SETXID_BITMASK);
 
       /* Reset the value so that the stack can be reused.  */
-      pd->setxid_futex = 0;
+      atomic_store_release(&pd->setxid_futex, 0);
     }
 
   /* We cannot call '_exit' here.  '_exit' will terminate the process.
@@ -728,7 +728,8 @@ __pthread_create_2_1 (pthread_t *newthread, const pthread_attr_t *attr,
 
   /* Inform start_thread (above) about cancellation state that might
      translate into inherited signal state.  */
-  pd->parent_cancelhandling = THREAD_GETMEM (THREAD_SELF, cancelhandling);
+  int tmp_cancelhandling = THREAD_ATOMIC_GETMEM(THREAD_SELF, cancelhandling);
+  atomic_store_relaxed(&pd->parent_cancelhandling, tmp_cancelhandling);
 
   /* Determine scheduling parameters for the thread.  */
   if (__builtin_expect ((iattr->flags & ATTR_FLAG_NOTINHERITSCHED) != 0, 0)
@@ -752,7 +753,7 @@ __pthread_create_2_1 (pthread_t *newthread, const pthread_attr_t *attr,
         collect_default_sched (pd);
     }
 
-  if (__glibc_unlikely (__nptl_nthreads == 1))
+  if (__glibc_unlikely (atomic_load_relaxed(&__nptl_nthreads) == 1))
     _IO_enable_locks ();
 
   /* Pass the descriptor to the caller.  */
diff --git a/nptl/pthread_detach.c b/nptl/pthread_detach.c
index 3ac3f76887..b225442448 100644
--- a/nptl/pthread_detach.c
+++ b/nptl/pthread_detach.c
@@ -46,7 +46,7 @@ __pthread_detach (pthread_t th)
   else
     /* Check whether the thread terminated meanwhile.  In this case we
        will just free the TCB.  */
-    if ((pd->cancelhandling & EXITING_BITMASK) != 0)
+	  if ((atomic_load_relaxed(&pd->cancelhandling) & EXITING_BITMASK) != 0)
       /* Note that the code in __free_tcb makes sure each thread
 	 control block is freed only once.  */
       __free_tcb (pd);
diff --git a/nptl/pthread_getspecific.c b/nptl/pthread_getspecific.c
index 569dae47c6..ccc18945fb 100644
--- a/nptl/pthread_getspecific.c
+++ b/nptl/pthread_getspecific.c
@@ -57,7 +57,7 @@ __pthread_getspecific (pthread_key_t key)
     {
       uintptr_t seq = data->seq;
 
-      if (__glibc_unlikely (seq != __pthread_keys[key].seq))
+      if (__glibc_unlikely (seq != atomic_load_relaxed(&__pthread_keys[key].seq)))
 	result = data->data = NULL;
     }
 
diff --git a/nptl/pthread_join_common.c b/nptl/pthread_join_common.c
index a96ceafde4..04bc0e2528 100644
--- a/nptl/pthread_join_common.c
+++ b/nptl/pthread_join_common.c
@@ -45,7 +45,7 @@ clockwait_tid (pid_t *tidp, clockid_t clockid, const struct timespec *abstime)
     return EINVAL;
 
   /* Repeat until thread terminated.  */
-  while ((tid = *tidp) != 0)
+  while ((tid = *tidp) != 0 || mvee_should_sync_tid())
     {
       struct timespec rt;
 
@@ -70,9 +70,11 @@ clockwait_tid (pid_t *tidp, clockid_t clockid, const struct timespec *abstime)
       /* If *tidp == tid, wait until thread terminates or the wait times out.
          The kernel up to version 3.16.3 does not use the private futex
          operations for futex wake-up when the clone terminates.  */
-      if (lll_futex_timed_wait_cancel (tidp, tid, &rt, LLL_SHARED)
+      if (lll_futex_mvee_timed_wait_cancel (tidp, tid, &rt, LLL_SHARED)
 	  == -ETIMEDOUT)
         return ETIMEDOUT;
+      else if (*tidp == 0)
+        break;
     }
 
   return 0;
@@ -102,10 +104,10 @@ __pthread_clockjoin_ex (pthread_t threadid, void **thread_return,
 
   if ((pd == self
        || (self->joinid == pd
-	   && (pd->cancelhandling
+		   && (atomic_load_relaxed(&pd->cancelhandling)
 	       & (CANCELING_BITMASK | CANCELED_BITMASK | EXITING_BITMASK
 		  | TERMINATED_BITMASK)) == 0))
-      && !CANCEL_ENABLED_AND_CANCELED (self->cancelhandling))
+      && !CANCEL_ENABLED_AND_CANCELED (atomic_load_relaxed(&self->cancelhandling)))
     /* This is a deadlock situation.  The threads are waiting for each
        other to finish.  Note that this is a "may" error.  To be 100%
        sure we catch this error we would have to lock the data
@@ -141,8 +143,12 @@ __pthread_clockjoin_ex (pthread_t threadid, void **thread_return,
 	  pid_t tid;
 	  /* We need acquire MO here so that we synchronize with the
 	     kernel's store to 0 when the clone terminates. (see above)  */
-	  while ((tid = atomic_load_acquire (&pd->tid)) != 0)
-	    lll_futex_wait_cancel (&pd->tid, tid, LLL_SHARED);
+	  while ((tid = atomic_load_acquire (&pd->tid)) != 0 || mvee_should_sync_tid())
+    {
+	    lll_futex_mvee_wait_cancel (&pd->tid, tid, LLL_SHARED);
+      if (pd->tid == 0)
+        break;
+    }
 	}
 
       pthread_cleanup_pop (0);
@@ -152,7 +158,7 @@ __pthread_clockjoin_ex (pthread_t threadid, void **thread_return,
   if (__glibc_likely (result == 0))
     {
       /* We mark the thread as terminated and as joined.  */
-      pd->tid = -1;
+      atomic_store_relaxed(&pd->tid, -1);
 
       /* Store the return value if the caller is interested.  */
       if (thread_return != NULL)
diff --git a/nptl/pthread_key_create.c b/nptl/pthread_key_create.c
index a584db412b..e5d06bc846 100644
--- a/nptl/pthread_key_create.c
+++ b/nptl/pthread_key_create.c
@@ -27,7 +27,7 @@ __pthread_key_create (pthread_key_t *key, void (*destr) (void *))
   /* Find a slot in __pthread_keys which is unused.  */
   for (size_t cnt = 0; cnt < PTHREAD_KEYS_MAX; ++cnt)
     {
-      uintptr_t seq = __pthread_keys[cnt].seq;
+      uintptr_t seq = atomic_load_relaxed(&__pthread_keys[cnt].seq);
 
       if (KEY_UNUSED (seq) && KEY_USABLE (seq)
 	  /* We found an unused slot.  Try to allocate it.  */
diff --git a/nptl/pthread_key_delete.c b/nptl/pthread_key_delete.c
index da9283eb01..8be64aab88 100644
--- a/nptl/pthread_key_delete.c
+++ b/nptl/pthread_key_delete.c
@@ -28,7 +28,7 @@ __pthread_key_delete (pthread_key_t key)
 
   if (__glibc_likely (key < PTHREAD_KEYS_MAX))
     {
-      unsigned int seq = __pthread_keys[key].seq;
+      unsigned int seq = atomic_load_relaxed(&__pthread_keys[key].seq);
 
       if (__builtin_expect (! KEY_UNUSED (seq), 1)
 	  && ! atomic_compare_and_exchange_bool_acq (&__pthread_keys[key].seq,
diff --git a/nptl/pthread_mutex_getprioceiling.c b/nptl/pthread_mutex_getprioceiling.c
index 19fefadcab..5f226a6eab 100644
--- a/nptl/pthread_mutex_getprioceiling.c
+++ b/nptl/pthread_mutex_getprioceiling.c
@@ -30,8 +30,8 @@ pthread_mutex_getprioceiling (const pthread_mutex_t *mutex, int *prioceiling)
 			 & PTHREAD_MUTEX_PRIO_PROTECT_NP) == 0, 0))
     return EINVAL;
 
-  *prioceiling = (mutex->__data.__lock & PTHREAD_MUTEX_PRIO_CEILING_MASK)
-		 >> PTHREAD_MUTEX_PRIO_CEILING_SHIFT;
+  *prioceiling = (atomic_load_relaxed((int*)&mutex->__data.__lock) & PTHREAD_MUTEX_PRIO_CEILING_MASK)
+	  >> PTHREAD_MUTEX_PRIO_CEILING_SHIFT;
 
   return 0;
 }
diff --git a/nptl/pthread_mutex_lock.c b/nptl/pthread_mutex_lock.c
index 7858abd528..ab961803da 100644
--- a/nptl/pthread_mutex_lock.c
+++ b/nptl/pthread_mutex_lock.c
@@ -184,7 +184,7 @@ __pthread_mutex_lock_full (pthread_mutex_t *mutex)
 	 see comments at ENQUEUE_MUTEX.  */
       __asm ("" ::: "memory");
 
-      oldval = mutex->__data.__lock;
+      oldval = atomic_load_relaxed(&mutex->__data.__lock);
       /* This is set to FUTEX_WAITERS iff we might have shared the
 	 FUTEX_WAITERS flag with other threads, and therefore need to keep it
 	 set to avoid lost wake-ups.  We have the same requirement in the
@@ -297,7 +297,7 @@ __pthread_mutex_lock_full (pthread_mutex_t *mutex)
 							oldval)
 		  != 0)
 		{
-		  oldval = mutex->__data.__lock;
+			oldval = atomic_load_relaxed(&mutex->__data.__lock);
 		  continue;
 		}
 	      oldval |= FUTEX_WAITERS;
@@ -312,7 +312,7 @@ __pthread_mutex_lock_full (pthread_mutex_t *mutex)
 	  /* Block using the futex and reload current lock value.  */
 	  lll_futex_wait (&mutex->__data.__lock, oldval,
 			  PTHREAD_ROBUST_MUTEX_PSHARED (mutex));
-	  oldval = mutex->__data.__lock;
+	  oldval = atomic_load_relaxed(&mutex->__data.__lock);
 	}
 
       /* We have acquired the mutex; check if it is still consistent.  */
@@ -372,7 +372,7 @@ __pthread_mutex_lock_full (pthread_mutex_t *mutex)
 	    __asm ("" ::: "memory");
 	  }
 
-	oldval = mutex->__data.__lock;
+	oldval = atomic_load_relaxed(&mutex->__data.__lock);
 
 	/* Check whether we already hold the mutex.  */
 	if (__glibc_unlikely ((oldval & FUTEX_TID_MASK) == id))
@@ -433,7 +433,7 @@ __pthread_mutex_lock_full (pthread_mutex_t *mutex)
 				 private);
 	      }
 
-	    oldval = mutex->__data.__lock;
+	    oldval = atomic_load_relaxed(&mutex->__data.__lock);
 
 	    assert (robust || (oldval & FUTEX_OWNER_DIED) == 0);
 	  }
@@ -508,7 +508,7 @@ __pthread_mutex_lock_full (pthread_mutex_t *mutex)
 	int kind = atomic_load_relaxed (&(mutex->__data.__kind))
 	  & PTHREAD_MUTEX_KIND_MASK_NP;
 
-	oldval = mutex->__data.__lock;
+	oldval = atomic_load_relaxed(&mutex->__data.__lock);
 
 	/* Check whether we already hold the mutex.  */
 	if (mutex->__data.__owner == id)
diff --git a/nptl/pthread_mutex_setprioceiling.c b/nptl/pthread_mutex_setprioceiling.c
index 521da72622..79895885f7 100644
--- a/nptl/pthread_mutex_setprioceiling.c
+++ b/nptl/pthread_mutex_setprioceiling.c
@@ -60,7 +60,7 @@ pthread_mutex_setprioceiling (pthread_mutex_t *mutex, int prioceiling,
 	locked = true;
     }
 
-  int oldval = mutex->__data.__lock;
+  int oldval = atomic_load_relaxed(&mutex->__data.__lock);
   if (! locked)
     do
       {
@@ -110,9 +110,9 @@ pthread_mutex_setprioceiling (pthread_mutex_t *mutex, int prioceiling,
 
   int newlock = 0;
   if (locked)
-    newlock = (mutex->__data.__lock & ~PTHREAD_MUTEX_PRIO_CEILING_MASK);
-  mutex->__data.__lock = newlock
-			 | (prioceiling << PTHREAD_MUTEX_PRIO_CEILING_SHIFT);
+	  newlock = (atomic_load_relaxed(&mutex->__data.__lock) & ~PTHREAD_MUTEX_PRIO_CEILING_MASK);
+  atomic_store_relaxed(&mutex->__data.__lock, newlock
+			   | (prioceiling << PTHREAD_MUTEX_PRIO_CEILING_SHIFT));
   atomic_full_barrier ();
 
   lll_futex_wake (&mutex->__data.__lock, INT_MAX,
diff --git a/nptl/pthread_mutex_timedlock.c b/nptl/pthread_mutex_timedlock.c
index 8ae814b984..4250f97ecd 100644
--- a/nptl/pthread_mutex_timedlock.c
+++ b/nptl/pthread_mutex_timedlock.c
@@ -148,7 +148,7 @@ __pthread_mutex_clocklock_common (pthread_mutex_t *mutex,
 	 see comments at ENQUEUE_MUTEX.  */
       __asm ("" ::: "memory");
 
-      oldval = mutex->__data.__lock;
+      oldval = atomic_load_relaxed(&mutex->__data.__lock);
       /* This is set to FUTEX_WAITERS iff we might have shared the
 	 FUTEX_WAITERS flag with other threads, and therefore need to keep it
 	 set to avoid lost wake-ups.  We have the same requirement in the
@@ -255,7 +255,7 @@ __pthread_mutex_clocklock_common (pthread_mutex_t *mutex,
 							oldval)
 		  != 0)
 		{
-		  oldval = mutex->__data.__lock;
+			oldval = atomic_load_relaxed(&mutex->__data.__lock);
 		  continue;
 		}
 	      oldval |= FUTEX_WAITERS;
@@ -275,7 +275,7 @@ __pthread_mutex_clocklock_common (pthread_mutex_t *mutex,
 	  if (err == -ETIMEDOUT)
 	    return -err;
 	  /* Reload current lock value.  */
-	  oldval = mutex->__data.__lock;
+	  oldval = atomic_load_relaxed(&mutex->__data.__lock);
 	}
 
       /* We have acquired the mutex; check if it is still consistent.  */
@@ -335,7 +335,7 @@ __pthread_mutex_clocklock_common (pthread_mutex_t *mutex,
 	    __asm ("" ::: "memory");
 	  }
 
-	oldval = mutex->__data.__lock;
+	oldval = atomic_load_relaxed(&mutex->__data.__lock);
 
 	/* Check whether we already hold the mutex.  */
 	if (__glibc_unlikely ((oldval & FUTEX_TID_MASK) == id))
@@ -402,7 +402,7 @@ __pthread_mutex_clocklock_common (pthread_mutex_t *mutex,
 	    else if (e != 0)
 	      return e;
 
-	    oldval = mutex->__data.__lock;
+	    oldval = atomic_load_relaxed(&mutex->__data.__lock);
 
 	    assert (robust || (oldval & FUTEX_OWNER_DIED) == 0);
 	  }
@@ -472,7 +472,7 @@ __pthread_mutex_clocklock_common (pthread_mutex_t *mutex,
 	int kind = atomic_load_relaxed (&(mutex->__data.__kind))
 	  & PTHREAD_MUTEX_KIND_MASK_NP;
 
-	oldval = mutex->__data.__lock;
+	oldval = atomic_load_relaxed(&mutex->__data.__lock);
 
 	/* Check whether we already hold the mutex.  */
 	if (mutex->__data.__owner == id)
diff --git a/nptl/pthread_mutex_trylock.c b/nptl/pthread_mutex_trylock.c
index b9896f420e..af8f3e131f 100644
--- a/nptl/pthread_mutex_trylock.c
+++ b/nptl/pthread_mutex_trylock.c
@@ -99,7 +99,7 @@ __pthread_mutex_trylock (pthread_mutex_t *mutex)
 	 see comments at ENQUEUE_MUTEX.  */
       __asm ("" ::: "memory");
 
-      oldval = mutex->__data.__lock;
+      oldval = atomic_load_relaxed(&mutex->__data.__lock);
       do
 	{
 	again:
@@ -244,7 +244,7 @@ __pthread_mutex_trylock (pthread_mutex_t *mutex)
 	    __asm ("" ::: "memory");
 	  }
 
-	oldval = mutex->__data.__lock;
+	oldval = atomic_load_relaxed(&mutex->__data.__lock);
 
 	/* Check whether we already hold the mutex.  */
 	if (__glibc_unlikely ((oldval & FUTEX_TID_MASK) == id))
@@ -313,7 +313,7 @@ __pthread_mutex_trylock (pthread_mutex_t *mutex)
 		return EBUSY;
 	      }
 
-	    oldval = mutex->__data.__lock;
+	    oldval = atomic_load_relaxed(&mutex->__data.__lock);
 	  }
 
 	if (__glibc_unlikely (oldval & FUTEX_OWNER_DIED))
@@ -385,7 +385,7 @@ __pthread_mutex_trylock (pthread_mutex_t *mutex)
 	int kind = atomic_load_relaxed (&(mutex->__data.__kind))
 	  & PTHREAD_MUTEX_KIND_MASK_NP;
 
-	oldval = mutex->__data.__lock;
+	oldval = atomic_load_relaxed(&mutex->__data.__lock);
 
 	/* Check whether we already hold the mutex.  */
 	if (mutex->__data.__owner == id)
diff --git a/nptl/pthread_mutex_unlock.c b/nptl/pthread_mutex_unlock.c
index 2b4abb8ebe..3016323084 100644
--- a/nptl/pthread_mutex_unlock.c
+++ b/nptl/pthread_mutex_unlock.c
@@ -104,7 +104,7 @@ __pthread_mutex_unlock_full (pthread_mutex_t *mutex, int decr)
     {
     case PTHREAD_MUTEX_ROBUST_RECURSIVE_NP:
       /* Recursive mutex.  */
-      if ((mutex->__data.__lock & FUTEX_TID_MASK)
+		if ((atomic_load_relaxed(&mutex->__data.__lock) & FUTEX_TID_MASK)
 	  == THREAD_GETMEM (THREAD_SELF, tid)
 	  && __builtin_expect (mutex->__data.__owner
 			       == PTHREAD_MUTEX_INCONSISTENT, 0))
@@ -128,7 +128,7 @@ __pthread_mutex_unlock_full (pthread_mutex_t *mutex, int decr)
     case PTHREAD_MUTEX_ROBUST_ERRORCHECK_NP:
     case PTHREAD_MUTEX_ROBUST_NORMAL_NP:
     case PTHREAD_MUTEX_ROBUST_ADAPTIVE_NP:
-      if ((mutex->__data.__lock & FUTEX_TID_MASK)
+		if ((atomic_load_relaxed(&mutex->__data.__lock) & FUTEX_TID_MASK)
 	  != THREAD_GETMEM (THREAD_SELF, tid)
 	  || ! lll_islocked (mutex->__data.__lock))
 	return EPERM;
@@ -191,7 +191,7 @@ __pthread_mutex_unlock_full (pthread_mutex_t *mutex, int decr)
 
     case PTHREAD_MUTEX_PI_ROBUST_RECURSIVE_NP:
       /* Recursive mutex.  */
-      if ((mutex->__data.__lock & FUTEX_TID_MASK)
+		if ((atomic_load_relaxed(&mutex->__data.__lock) & FUTEX_TID_MASK)
 	  == THREAD_GETMEM (THREAD_SELF, tid)
 	  && __builtin_expect (mutex->__data.__owner
 			       == PTHREAD_MUTEX_INCONSISTENT, 0))
@@ -218,7 +218,7 @@ __pthread_mutex_unlock_full (pthread_mutex_t *mutex, int decr)
     case PTHREAD_MUTEX_PI_ROBUST_ERRORCHECK_NP:
     case PTHREAD_MUTEX_PI_ROBUST_NORMAL_NP:
     case PTHREAD_MUTEX_PI_ROBUST_ADAPTIVE_NP:
-      if ((mutex->__data.__lock & FUTEX_TID_MASK)
+		if ((atomic_load_relaxed(&mutex->__data.__lock) & FUTEX_TID_MASK)
 	  != THREAD_GETMEM (THREAD_SELF, tid)
 	  || ! lll_islocked (mutex->__data.__lock))
 	return EPERM;
@@ -306,7 +306,7 @@ __pthread_mutex_unlock_full (pthread_mutex_t *mutex, int decr)
     case PTHREAD_MUTEX_PP_ERRORCHECK_NP:
       /* Error checking mutex.  */
       if (mutex->__data.__owner != THREAD_GETMEM (THREAD_SELF, tid)
-	  || (mutex->__data.__lock & ~ PTHREAD_MUTEX_PRIO_CEILING_MASK) == 0)
+		  || (atomic_load_relaxed(&mutex->__data.__lock) & ~ PTHREAD_MUTEX_PRIO_CEILING_MASK) == 0)
 	return EPERM;
       /* FALLTHROUGH */
 
diff --git a/nptl/pthread_setcancelstate.c b/nptl/pthread_setcancelstate.c
index 4d7f413e19..01f4ab8f6c 100644
--- a/nptl/pthread_setcancelstate.c
+++ b/nptl/pthread_setcancelstate.c
@@ -31,7 +31,7 @@ __pthread_setcancelstate (int state, int *oldstate)
 
   self = THREAD_SELF;
 
-  int oldval = THREAD_GETMEM (self, cancelhandling);
+  int oldval = THREAD_ATOMIC_GETMEM (self, cancelhandling);
   while (1)
     {
       int newval = (state == PTHREAD_CANCEL_DISABLE
diff --git a/nptl/pthread_setcanceltype.c b/nptl/pthread_setcanceltype.c
index fcaae8abc7..622292774f 100644
--- a/nptl/pthread_setcanceltype.c
+++ b/nptl/pthread_setcanceltype.c
@@ -29,7 +29,7 @@ __pthread_setcanceltype (int type, int *oldtype)
 
   volatile struct pthread *self = THREAD_SELF;
 
-  int oldval = THREAD_GETMEM (self, cancelhandling);
+  int oldval = THREAD_ATOMIC_GETMEM (self, cancelhandling);
   while (1)
     {
       int newval = (type == PTHREAD_CANCEL_ASYNCHRONOUS
diff --git a/nptl/pthread_setspecific.c b/nptl/pthread_setspecific.c
index c24045399c..d4f209d00c 100644
--- a/nptl/pthread_setspecific.c
+++ b/nptl/pthread_setspecific.c
@@ -37,7 +37,7 @@ __pthread_setspecific (pthread_key_t key, const void *value)
   if (__glibc_likely (key < PTHREAD_KEY_2NDLEVEL_SIZE))
     {
       /* Verify the key is sane.  */
-      if (KEY_UNUSED ((seq = __pthread_keys[key].seq)))
+      if (KEY_UNUSED ((seq = atomic_load_relaxed(&__pthread_keys[key].seq))))
 	/* Not valid.  */
 	return EINVAL;
 
diff --git a/stdlib/cxa_atexit.c b/stdlib/cxa_atexit.c
index f0c29c6672..9df971f4a4 100644
--- a/stdlib/cxa_atexit.c
+++ b/stdlib/cxa_atexit.c
@@ -55,7 +55,7 @@ __internal_atexit (void (*func) (void *), void *arg, void *d,
   new->func.cxa.fn = (void (*) (void *, int)) func;
   new->func.cxa.arg = arg;
   new->func.cxa.dso_handle = d;
-  new->flavor = ef_cxa;
+  atomic_store_relaxed(&new->flavor, ef_cxa);
   __libc_lock_unlock (__exit_funcs_lock);
   return 0;
 }
@@ -93,7 +93,7 @@ __new_exitfn (struct exit_function_list **listp)
   for (l = *listp; l != NULL; p = l, l = l->next)
     {
       for (i = l->idx; i > 0; --i)
-	if (l->fns[i - 1].flavor != ef_free)
+		  if (atomic_load_relaxed(&l->fns[i - 1].flavor) != ef_free)
 	  break;
 
       if (i > 0)
@@ -135,7 +135,7 @@ __new_exitfn (struct exit_function_list **listp)
   /* Mark entry as used, but we don't know the flavor now.  */
   if (r != NULL)
     {
-      r->flavor = ef_us;
+		atomic_store_relaxed(&r->flavor, ef_us);
       ++__new_exitfn_called;
     }
 
diff --git a/stdlib/cxa_finalize.c b/stdlib/cxa_finalize.c
index 1936aabc85..516414d46f 100644
--- a/stdlib/cxa_finalize.c
+++ b/stdlib/cxa_finalize.c
@@ -38,7 +38,7 @@ __cxa_finalize (void *d)
       struct exit_function *f;
 
       for (f = &funcs->fns[funcs->idx - 1]; f >= &funcs->fns[0]; --f)
-	if ((d == NULL || d == f->func.cxa.dso_handle) && f->flavor == ef_cxa)
+		  if ((d == NULL || d == f->func.cxa.dso_handle) && atomic_load_relaxed(&f->flavor) == ef_cxa)
 	  {
 	    const uint64_t check = __new_exitfn_called;
 	    void (*cxafn) (void *arg, int status) = f->func.cxa.fn;
@@ -73,7 +73,7 @@ __cxa_finalize (void *d)
 	       run in effective LIFO order.  This should probably be fixed in a
 	       future implementation to ensure the functions do not run in
 	       parallel.  */
-	    f->flavor = ef_free;
+	    atomic_store_relaxed(&f->flavor, ef_free);
 
 #ifdef PTR_DEMANGLE
 	    PTR_DEMANGLE (cxafn);
@@ -97,7 +97,7 @@ __cxa_finalize (void *d)
 
       for (f = &funcs->fns[funcs->idx - 1]; f >= &funcs->fns[0]; --f)
 	if (d == NULL || d == f->func.cxa.dso_handle)
-	  f->flavor = ef_free;
+		atomic_store_relaxed(&f->flavor, ef_free);
     }
 
   /* Remove the registered fork handlers.  We do not have to
diff --git a/stdlib/exit.c b/stdlib/exit.c
index 7bca1cdc14..55cb3e1510 100644
--- a/stdlib/exit.c
+++ b/stdlib/exit.c
@@ -74,7 +74,7 @@ __run_exit_handlers (int status, struct exit_function_list **listp,
 
 	  /* Unlock the list while we call a foreign function.  */
 	  __libc_lock_unlock (__exit_funcs_lock);
-	  switch (f->flavor)
+	  switch (atomic_load_relaxed(&f->flavor))
 	    {
 	      void (*atfct) (void);
 	      void (*onfct) (int status, void *arg);
@@ -100,7 +100,7 @@ __run_exit_handlers (int status, struct exit_function_list **listp,
 	    case ef_cxa:
 	      /* To avoid dlclose/exit race calling cxafct twice (BZ 22180),
 		 we must mark this function as ef_free.  */
-	      f->flavor = ef_free;
+			atomic_store_relaxed(&f->flavor, ef_free);
 	      cxafct = f->func.cxa.fn;
 #ifdef PTR_DEMANGLE
 	      PTR_DEMANGLE (cxafct);
diff --git a/stdlib/on_exit.c b/stdlib/on_exit.c
index f64a3dd54f..98750cd609 100644
--- a/stdlib/on_exit.c
+++ b/stdlib/on_exit.c
@@ -44,7 +44,7 @@ __on_exit (void (*func) (int status, void *arg), void *arg)
 #endif
   new->func.on.fn = func;
   new->func.on.arg = arg;
-  new->flavor = ef_on;
+  atomic_store_relaxed(&new->flavor, ef_on);
   __libc_lock_unlock (__exit_funcs_lock);
   return 0;
 }
diff --git a/sysdeps/arm/Makefile b/sysdeps/arm/Makefile
index ad2042b93a..299145fd3e 100644
--- a/sysdeps/arm/Makefile
+++ b/sysdeps/arm/Makefile
@@ -47,7 +47,7 @@ aeabi_routines = aeabi_assert aeabi_localeconv aeabi_errno_addr \
 		 aeabi_memmove aeabi_memset \
 		 aeabi_read_tp libc-aeabi_read_tp
 
-sysdep_routines += $(aeabi_constants) $(aeabi_routines)
+sysdep_routines += $(aeabi_constants) $(aeabi_routines) mvee_infinite_loop
 static-only-routines += $(aeabi_constants) aeabi_read_tp
 shared-only-routines += libc-aeabi_read_tp
 
diff --git a/sysdeps/arm/atomic-machine.h b/sysdeps/arm/atomic-machine.h
index a1057e0d6b..3b2f439caf 100644
--- a/sysdeps/arm/atomic-machine.h
+++ b/sysdeps/arm/atomic-machine.h
@@ -56,68 +56,68 @@ void __arm_link_error (void);
    a pattern to do this efficiently.  */
 #ifdef __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4
 
-# define atomic_exchange_acq(mem, value)                                \
-  __atomic_val_bysize (__arch_exchange, int, mem, value, __ATOMIC_ACQUIRE)
+# define orig_atomic_exchange_acq(mem, value)                                \
+  __atomic_val_bysize (orig___arch_exchange, int, mem, value, __ATOMIC_ACQUIRE)
 
-# define atomic_exchange_rel(mem, value)                                \
-  __atomic_val_bysize (__arch_exchange, int, mem, value, __ATOMIC_RELEASE)
+# define orig_atomic_exchange_rel(mem, value)                                \
+  __atomic_val_bysize (orig___arch_exchange, int, mem, value, __ATOMIC_RELEASE)
 
 /* Atomic exchange (without compare).  */
 
-# define __arch_exchange_8_int(mem, newval, model)      \
+# define orig___arch_exchange_8_int(mem, newval, model)      \
   (__arm_link_error (), (typeof (*mem)) 0)
 
-# define __arch_exchange_16_int(mem, newval, model)     \
+# define orig___arch_exchange_16_int(mem, newval, model)     \
   (__arm_link_error (), (typeof (*mem)) 0)
 
-# define __arch_exchange_32_int(mem, newval, model)     \
+# define orig___arch_exchange_32_int(mem, newval, model)     \
   __atomic_exchange_n (mem, newval, model)
 
-# define __arch_exchange_64_int(mem, newval, model)     \
+# define orig___arch_exchange_64_int(mem, newval, model)     \
   (__arm_link_error (), (typeof (*mem)) 0)
 
 /* Compare and exchange with "acquire" semantics, ie barrier after.  */
 
-# define atomic_compare_and_exchange_bool_acq(mem, new, old)    \
-  __atomic_bool_bysize (__arch_compare_and_exchange_bool, int,  \
+# define orig_atomic_compare_and_exchange_bool_acq(mem, new, old)    \
+  __atomic_bool_bysize (orig___arch_compare_and_exchange_bool, int,  \
                         mem, new, old, __ATOMIC_ACQUIRE)
 
-# define atomic_compare_and_exchange_val_acq(mem, new, old)     \
-  __atomic_val_bysize (__arch_compare_and_exchange_val, int,    \
+# define orig_atomic_compare_and_exchange_val_acq(mem, new, old)     \
+  __atomic_val_bysize (orig___arch_compare_and_exchange_val, int,    \
                        mem, new, old, __ATOMIC_ACQUIRE)
 
 /* Compare and exchange with "release" semantics, ie barrier before.  */
 
-# define atomic_compare_and_exchange_val_rel(mem, new, old)      \
-  __atomic_val_bysize (__arch_compare_and_exchange_val, int,    \
+# define orig_atomic_compare_and_exchange_val_rel(mem, new, old)      \
+  __atomic_val_bysize (orig___arch_compare_and_exchange_val, int,    \
                        mem, new, old, __ATOMIC_RELEASE)
 
 /* Compare and exchange.
    For all "bool" routines, we return FALSE if exchange succesful.  */
 
-# define __arch_compare_and_exchange_bool_8_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_bool_8_int(mem, newval, oldval, model) \
   ({__arm_link_error (); 0; })
 
-# define __arch_compare_and_exchange_bool_16_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_bool_16_int(mem, newval, oldval, model) \
   ({__arm_link_error (); 0; })
 
-# define __arch_compare_and_exchange_bool_32_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_bool_32_int(mem, newval, oldval, model) \
   ({                                                                    \
     typeof (*mem) __oldval = (oldval);                                  \
     !__atomic_compare_exchange_n (mem, (void *) &__oldval, newval, 0,   \
                                   model, __ATOMIC_RELAXED);             \
   })
 
-# define __arch_compare_and_exchange_bool_64_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_bool_64_int(mem, newval, oldval, model) \
   ({__arm_link_error (); 0; })
 
-# define __arch_compare_and_exchange_val_8_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_val_8_int(mem, newval, oldval, model) \
   ({__arm_link_error (); oldval; })
 
-# define __arch_compare_and_exchange_val_16_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_val_16_int(mem, newval, oldval, model) \
   ({__arm_link_error (); oldval; })
 
-# define __arch_compare_and_exchange_val_32_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_val_32_int(mem, newval, oldval, model) \
   ({                                                                    \
     typeof (*mem) __oldval = (oldval);                                  \
     __atomic_compare_exchange_n (mem, (void *) &__oldval, newval, 0,    \
@@ -125,11 +125,11 @@ void __arm_link_error (void);
     __oldval;                                                           \
   })
 
-# define __arch_compare_and_exchange_val_64_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_val_64_int(mem, newval, oldval, model) \
   ({__arm_link_error (); oldval; })
 
 #else
-# define __arch_compare_and_exchange_val_32_acq(mem, newval, oldval) \
+# define orig___arch_compare_and_exchange_val_32_acq(mem, newval, oldval) \
   __arm_assisted_compare_and_exchange_val_32_acq ((mem), (newval), (oldval))
 #endif
 
@@ -153,3 +153,805 @@ void __arm_link_error (void);
 # define __arm_assisted_compare_and_exchange_val_32_acq(mem, newval, oldval) \
   ({ __arm_link_error (); oldval; })
 #endif
+
+/*--------------------------------------------------------------------------------
+                                  MVEE PATCHES
+--------------------------------------------------------------------------------*/
+#define USE_MVEE_LIBC
+
+#ifdef USE_MVEE_LIBC
+// NOTE: This is different from the base value for x86 because the ARM syscall
+// instruction can only encode small immediates as syscall numbers
+#define MVEE_FAKE_SYSCALL_BASE          0x6FF 
+#define MVEE_GET_MASTERTHREAD_ID        MVEE_FAKE_SYSCALL_BASE + 3
+#define MVEE_GET_SHARED_BUFFER          MVEE_FAKE_SYSCALL_BASE + 4
+#define MVEE_FLUSH_SHARED_BUFFER        MVEE_FAKE_SYSCALL_BASE + 5
+#define MVEE_SET_INFINITE_LOOP_PTR      MVEE_FAKE_SYSCALL_BASE + 6
+#define MVEE_TOGGLESYNC                 MVEE_FAKE_SYSCALL_BASE + 7
+#define MVEE_SET_SHARED_BUFFER_POS_PTR  MVEE_FAKE_SYSCALL_BASE + 8
+#define MVEE_RUNS_UNDER_MVEE_CONTROL    MVEE_FAKE_SYSCALL_BASE + 9
+#define MVEE_GET_THREAD_NUM             MVEE_FAKE_SYSCALL_BASE + 10
+#define MVEE_SET_SYNC_PRIMITIVES_PTR    MVEE_FAKE_SYSCALL_BASE + 12
+#define MVEE_ALL_HEAPS_ALIGNED          MVEE_FAKE_SYSCALL_BASE + 13
+#define MVEE_GET_VIRTUALIZED_ARGV0      MVEE_FAKE_SYSCALL_BASE + 17
+#define MVEE_LIBC_LOCK_BUFFER           3
+#define MVEE_LIBC_MALLOC_DEBUG_BUFFER   11
+#define MVEE_LIBC_ATOMIC_BUFFER         13
+#define MVEE_FUTEX_WAIT_TID             30
+
+enum mvee_alloc_types
+{
+	LIBC_MALLOC,
+	LIBC_FREE,
+	LIBC_REALLOC,
+	LIBC_MEMALIGN,
+	LIBC_CALLOC,
+	MALLOC_TRIM,
+	HEAP_TRIM,
+	MALLOC_CONSOLIDATE,
+	ARENA_GET2,
+	_INT_MALLOC,
+	_INT_FREE,
+	_INT_REALLOC
+};
+
+//
+// Atomic operations list. Keep this in sync with MVEE/Inc/MVEE_shm.h
+//
+enum mvee_atomics
+{
+    // LOAD OPERATIONS FIRST!!! DO NOT CHANGE THIS CONVENTION
+    ATOMIC_FORCED_READ,
+    ATOMIC_LOAD,
+    // THE FOLLOWING IS NOT AN ACTUAL ATOMIC OPERATION, IT JUST DENOTES THE END OF THE LOAD-ONLY ATOMICS!!!
+    ATOMIC_LOAD_MAX,
+    // STORES AFTER LOADS
+    CATOMIC_AND,
+    CATOMIC_OR,
+    CATOMIC_EXCHANGE_AND_ADD,
+    CATOMIC_ADD,
+    CATOMIC_INCREMENT,
+    CATOMIC_DECREMENT,
+    CATOMIC_MAX,
+    ATOMIC_COMPARE_AND_EXCHANGE_VAL,
+    ATOMIC_COMPARE_AND_EXCHANGE_BOOL,
+    ATOMIC_EXCHANGE,
+    ATOMIC_EXCHANGE_AND_ADD,
+    ATOMIC_INCREMENT_AND_TEST,
+    ATOMIC_DECREMENT_AND_TEST,
+	ATOMIC_ADD_NEGATIVE,
+    ATOMIC_ADD_ZERO,
+    ATOMIC_ADD,
+	ATOMIC_OR,
+	ATOMIC_OR_VAL,
+    ATOMIC_INCREMENT,
+    ATOMIC_DECREMENT,
+    ATOMIC_BIT_TEST_SET,
+    ATOMIC_BIT_SET,
+    ATOMIC_AND,
+	ATOMIC_AND_VAL,
+    ATOMIC_STORE,
+	ATOMIC_MIN,
+    ATOMIC_MAX,
+    ATOMIC_DECREMENT_IF_POSITIVE,
+	ATOMIC_FETCH_ADD,
+	ATOMIC_FETCH_AND,
+	ATOMIC_FETCH_OR,
+	ATOMIC_FETCH_XOR,
+    ___UNKNOWN_LOCK_TYPE___,
+    __MVEE_ATOMICS_MAX__
+};
+
+#define MVEE_ROUND_UP(x, multiple)				\
+	((x + (multiple - 1)) & ~(multiple -1))
+#define MVEE_MIN(a, b) ((a > b) ? (b) : (a))
+
+# ifdef MVEE_USE_TOTALPARTIAL_AGENT
+#  warning "The total and partial order agents have not been tested on ARM and will probably break!"
+#  include "mvee-totalpartial-agent.h"
+# else
+#  include "mvee-woc-agent.h"
+# endif
+
+#endif
+
+// We don't use our sync agent in the dynamic loader so just use the original atomics everywhere
+#if IS_IN (rtld) || !defined(USE_MVEE_LIBC)
+
+//
+// generic atomics (include/atomic.h and sysdeps/arch/atomic-machine.h)
+//
+#define __arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_val_acq(mem, newval, oldval) orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_val_rel(mem, newval, oldval) orig_atomic_compare_and_exchange_val_rel(mem, newval, oldval)
+#define atomic_compare_and_exchange_bool_acq(mem, newval, oldval) orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_bool_rel(mem, newval, oldval) orig_atomic_compare_and_exchange_bool_rel(mem, newval, oldval)
+#define atomic_exchange_acq(mem, newvalue) orig_atomic_exchange_acq(mem, newvalue)
+#define atomic_exchange_rel(mem, newvalue) orig_atomic_exchange_rel(mem, newvalue)
+#define atomic_exchange_and_add(mem, value) orig_atomic_exchange_and_add(mem, value)
+#define atomic_exchange_and_add_acq(mem, value) orig_atomic_exchange_and_add_acq(mem, value)
+#define atomic_exchange_and_add_rel(mem, value) orig_atomic_exchange_and_add_rel(mem, value)
+#define atomic_add(mem, value) orig_atomic_add(mem, value)
+#define atomic_increment(mem) orig_atomic_increment(mem)
+#define atomic_increment_and_test(mem) orig_atomic_increment_and_test(mem)
+#define atomic_increment_val(mem) orig_atomic_increment_val(mem)
+#define atomic_decrement(mem) orig_atomic_decrement(mem)
+#define atomic_decrement_and_test(mem) orig_atomic_decrement_and_test(mem)
+#define atomic_decrement_val(mem) orig_atomic_decrement_val(mem)
+#define atomic_add_negative(mem, value) orig_atomic_add_negative(mem, value)
+#define atomic_add_zero(mem, value) orig_atomic_add_zero(mem, value)
+#define atomic_bit_set(mem, bit) orig_atomic_bit_set(mem, bit)
+#define atomic_bit_test_set(mem, bit) orig_atomic_bit_test_set(mem, bit)
+#define atomic_and(mem, mask) orig_atomic_and(mem, mask)
+#define atomic_or(mem, mask) orig_atomic_or(mem, mask)
+#define atomic_max(mem, value) orig_atomic_max(mem, value)
+#define atomic_min(mem, value) orig_atomic_min(mem, value)
+#define atomic_decrement_if_positive(mem) orig_atomic_decrement_if_positive(mem)
+#define atomic_and_val(mem, mask) orig_atomic_and_val(mem, mask)
+#define atomic_or_val(mem, mask) orig_atomic_or_val(mem, mask)
+#define atomic_forced_read(x) orig_atomic_forced_read(x)
+#define catomic_compare_and_exchange_val_acq(mem, newval, oldval) orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval)
+#define catomic_compare_and_exchange_val_rel(mem, newval, oldval) orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval)
+#define catomic_compare_and_exchange_bool_acq(mem, newval, oldval) orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval)
+#define catomic_compare_and_exchange_bool_rel(mem, newval, oldval) orig_catomic_compare_and_exchange_bool_rel(mem, newval, oldval)
+#define catomic_exchange_and_add(mem, value) orig_catomic_exchange_and_add(mem, value)
+#define catomic_add(mem, value) orig_catomic_add(mem, value)
+#define catomic_increment(mem) orig_catomic_increment(mem)
+#define catomic_increment_val(mem) orig_catomic_increment_val(mem)
+#define catomic_decrement(mem) orig_catomic_decrement(mem)
+#define catomic_decrement_val(mem) orig_catomic_decrement_val(mem)
+#define catomic_and(mem, mask) orig_catomic_and(mem, mask)
+#define catomic_or(mem, mask) orig_catomic_or(mem, mask)
+#define catomic_max(mem, value) orig_catomic_max(mem, value)
+
+//
+// C11-style atomics (include/atomic.h)
+//
+#define atomic_load_relaxed(mem) orig_atomic_load_relaxed(mem)
+#define atomic_load_acquire(mem) orig_atomic_load_acquire(mem)
+#define atomic_store_relaxed(mem, val) orig_atomic_store_relaxed(mem, val)
+#define atomic_store_release(mem, val) orig_atomic_store_release(mem, val)
+#define atomic_compare_exchange_weak_relaxed(mem, expected, desired) orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired)
+#define atomic_compare_exchange_weak_acquire(mem, expected, desired) orig_atomic_compare_exchange_weak_acquire(mem, expected, desired) 
+#define atomic_compare_exchange_weak_release(mem, expected, desired) orig_atomic_compare_exchange_weak_release(mem, expected, desired)
+#define atomic_exchange_relaxed(mem, desired) orig_atomic_exchange_relaxed(mem, desired)
+#define atomic_exchange_acquire(mem, desired) orig_atomic_exchange_acquire(mem, desired)
+#define atomic_exchange_release(mem, desired) orig_atomic_exchange_release(mem, desired)
+#define atomic_fetch_add_relaxed(mem, operand) orig_atomic_fetch_add_relaxed(mem, operand)
+#define atomic_fetch_add_acquire(mem, operand) orig_atomic_fetch_add_acquire(mem, operand)
+#define atomic_fetch_add_release(mem, operand) orig_atomic_fetch_add_release(mem, operand)
+#define atomic_fetch_add_acq_rel(mem, operand) orig_atomic_fetch_add_acq_rel(mem, operand)
+#define atomic_fetch_and_relaxed(mem, operand) orig_atomic_fetch_and_relaxed(mem, operand)
+#define atomic_fetch_and_acquire(mem, operand) orig_atomic_fetch_and_acquire(mem, operand)
+#define atomic_fetch_and_release(mem, operand) orig_atomic_fetch_and_release(mem, operand)
+#define atomic_fetch_or_relaxed(mem, operand) orig_atomic_fetch_or_relaxed(mem, operand) 
+#define atomic_fetch_or_acquire(mem, operand) orig_atomic_fetch_or_acquire(mem, operand) 
+#define atomic_fetch_or_release(mem, operand) orig_atomic_fetch_or_release(mem, operand) 
+#define atomic_fetch_xor_release(mem, operand) orig_atomic_fetch_xor_release(mem, operand) 
+
+//
+// MVEE additions
+//
+#define THREAD_ATOMIC_GETMEM(descr, member) THREAD_GETMEM(descr, member)
+#define THREAD_ATOMIC_SETMEM(descr, member, val) THREAD_SETMEM(descr, member, val)
+
+
+#else // !IS_IN_rtld
+
+//
+// architecture-specific atomics (atomic-machine.h)
+//
+#define __arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_val_acq(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_val_rel(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_val_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_bool_acq(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_bool_rel(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_bool_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_exchange_acq(mem, newvalue)						\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);					\
+		____result = orig_atomic_exchange_acq(mem, newvalue);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_rel(mem, newvalue)						\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);					\
+		____result = orig_atomic_exchange_rel(mem, newvalue);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add(mem, value)						\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_atomic_exchange_and_add(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add_acq(mem, value)					\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_atomic_exchange_and_add_acq(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add_rel(mem, value)					\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_atomic_exchange_and_add_rel(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_add(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_ADD, mem, 1);			\
+		orig_atomic_add(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_increment(mem)					\
+	({											\
+		MVEE_PREOP(ATOMIC_INCREMENT, mem, 1);	\
+		orig_atomic_increment(mem);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_increment_and_test(mem)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_INCREMENT_AND_TEST, mem, 1);		\
+		____result = orig_atomic_increment_and_test(mem);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_increment_val(mem)				\
+	({											\
+		typeof(*mem) ____result;				\
+		MVEE_PREOP(ATOMIC_INCREMENT, mem, 1);	\
+		____result = orig_atomic_increment_val(mem);	\
+		MVEE_POSTOP();							\
+		____result;								\
+	})
+
+#define atomic_decrement(mem)					\
+	({											\
+		MVEE_PREOP(ATOMIC_DECREMENT, mem, 1);	\
+		orig_atomic_decrement(mem);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_decrement_and_test(mem)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_DECREMENT_AND_TEST, mem, 1);		\
+		____result = orig_atomic_decrement_and_test(mem);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_decrement_val(mem)				\
+	({											\
+		typeof(*mem) ____result;				\
+		MVEE_PREOP(ATOMIC_DECREMENT, mem, 1);	\
+		____result = orig_atomic_decrement_val(mem);	\
+		MVEE_POSTOP();							\
+		____result;								\
+	})
+
+#define atomic_add_negative(mem, value)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_ADD, mem, 1);						\
+		____result = orig_atomic_add_negative(mem, value);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_add_zero(mem, value)						\
+	({													\
+		unsigned char ____result;						\
+		MVEE_PREOP(ATOMIC_ADD_ZERO, mem, 1);			\
+		____result = orig_atomic_add_zero(mem, value);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+#define atomic_bit_set(mem, bit)				\
+	({											\
+		MVEE_PREOP(ATOMIC_BIT_SET, mem, 1);		\
+		orig_atomic_bit_set(mem, bit);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_bit_test_set(mem, bit)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_BIT_TEST_SET, mem, 1);			\
+		____result = orig_atomic_bit_test_set(mem, bit);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_and(mem, mask)					\
+	({											\
+		MVEE_PREOP(ATOMIC_AND, mem, 1);			\
+		orig_atomic_and(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_or(mem, mask)					\
+	({											\
+		MVEE_PREOP(ATOMIC_OR, mem, 1);			\
+		orig_atomic_or(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_max(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_MAX, mem, 1);			\
+		orig_atomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_min(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_MIN, mem, 1);			\
+		orig_atomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_decrement_if_positive(mem)					\
+	({														\
+		__typeof(*mem) __result;							\
+		MVEE_PREOP(ATOMIC_DECREMENT_IF_POSITIVE, mem, 1);	\
+		__result = orig_atomic_decrement_if_positive(mem);	\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_and_val(mem, mask)							\
+	({														\
+		__typeof(*mem) __result;							\
+		MVEE_PREOP(ATOMIC_AND_VAL, mem, 1);					\
+		__result = orig_atomic_and_val(mem);				\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_or_val(mem, mask)							\
+	({														\
+		__typeof(*mem) __result;							\
+		MVEE_PREOP(ATOMIC_OR_VAL, mem, 1);					\
+		__result = orig_atomic_or_val(mem);					\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_forced_read(x)						\
+	({												\
+		typeof(x) ____result;						\
+		MVEE_PREOP(ATOMIC_FORCED_READ, &x, 0);		\
+		____result = orig_atomic_forced_read(x);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define catomic_compare_and_exchange_val_acq(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_val_rel(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_bool_acq(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_bool_rel(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_bool_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_exchange_and_add(mem, value)					\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(CATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_catomic_exchange_and_add(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define catomic_add(mem, value)					\
+	({											\
+		MVEE_PREOP(CATOMIC_ADD, mem, 1);		\
+		orig_catomic_add(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_increment(mem)					\
+	({											\
+		MVEE_PREOP(CATOMIC_INCREMENT, mem, 1);	\
+		orig_catomic_increment(mem);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_increment_val(mem)						\
+	({													\
+		typeof(*mem) ____result;						\
+		MVEE_PREOP(CATOMIC_INCREMENT, mem, 1);			\
+		____result = orig_catomic_increment_val(mem);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+#define catomic_decrement(mem)					\
+	({											\
+		MVEE_PREOP(CATOMIC_DECREMENT, mem, 1);	\
+		orig_catomic_decrement(mem);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_decrement_val(mem)						\
+	({													\
+		typeof(*mem) ____result;						\
+		MVEE_PREOP(CATOMIC_DECREMENT, mem, 1);			\
+		____result = orig_catomic_decrement_val(mem);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+
+#define catomic_and(mem, mask)					\
+	({											\
+		MVEE_PREOP(CATOMIC_AND, mem, 1);		\
+		orig_catomic_and(mem, mask);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_or(mem, mask)					\
+	({											\
+		MVEE_PREOP(CATOMIC_OR, mem, 1);			\
+		orig_catomic_or(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_max(mem, value)					\
+	({											\
+		MVEE_PREOP(CATOMIC_MAX, mem, 1);		\
+		orig_catomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+
+//
+// generic C11-style atomics (include/atomic.h)
+//
+#define atomic_load_relaxed(mem)					\
+	({												\
+		__typeof(*mem) ____result;					\
+		MVEE_PREOP(ATOMIC_LOAD, mem, 0);			\
+		____result = orig_atomic_load_relaxed(mem);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define atomic_load_acquire(mem)					\
+	({												\
+		__typeof(*mem) ____result;					\
+		MVEE_PREOP(ATOMIC_LOAD, mem, 0);			\
+		____result = orig_atomic_load_acquire(mem);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define atomic_store_relaxed(mem, val)			\
+	(void)({									\
+		MVEE_PREOP(ATOMIC_STORE, mem, 1);		\
+		orig_atomic_store_relaxed(mem, val);	\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_store_release(mem, val)			\
+	(void)({									\
+		MVEE_PREOP(ATOMIC_STORE, mem, 1);		\
+		orig_atomic_store_release(mem, val);	\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_compare_exchange_weak_relaxed(mem, expected, desired)	\
+	({																	\
+		bool __result;													\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		__result = orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_compare_exchange_weak_acquire(mem, expected, desired)	\
+	({																	\
+		bool __result;													\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		__result = orig_atomic_compare_exchange_weak_acquire(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_compare_exchange_weak_release(mem, expected, desired)	\
+	({																	\
+		bool __result;													\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		__result = orig_atomic_compare_exchange_weak_release(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_exchange_relaxed(mem, desired)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		____result = orig_atomic_exchange_relaxed(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_exchange_acquire(mem, desired)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		____result = orig_atomic_exchange_acquire(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_exchange_release(mem, desired)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		____result = orig_atomic_exchange_release(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_relaxed(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_acquire(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_acq_rel(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_acq_rel(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_relaxed(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		____result = orig_atomic_fetch_and_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_acquire(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		____result = orig_atomic_fetch_and_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		____result = orig_atomic_fetch_and_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+
+#define atomic_fetch_or_relaxed(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		____result = orig_atomic_fetch_or_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_or_acquire(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		____result = orig_atomic_fetch_or_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_or_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		____result = orig_atomic_fetch_or_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_xor_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_XOR, mem, 1);						\
+		____result = orig_atomic_fetch_xor_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+//
+// MVEE additions
+//
+#define THREAD_ATOMIC_GETMEM(descr, member)			\
+	({												\
+		__typeof(descr->member) ____result;			\
+		MVEE_PREOP(ATOMIC_LOAD, &descr->member, 1);	\
+		____result = THREAD_GETMEM(descr, member);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define THREAD_ATOMIC_SETMEM(descr, member, val)			\
+	(void)({												\
+			MVEE_PREOP(ATOMIC_STORE, &descr->member, 1);	\
+			THREAD_SETMEM(descr, member, val);				\
+			MVEE_POSTOP();									\
+		})
+
+//
+// sys_futex with FUTEX_WAKE_OP usually overwrites the value of the futex.
+// We have to make sure that we include the futex write in our sync buf ordering
+//
+#define lll_futex_wake_unlock(futexp, nr_wake, nr_wake2, futexp2, private) \
+	({																	\
+		INTERNAL_SYSCALL_DECL (__err);									\
+		long int __ret;													\
+		MVEE_PREOP(___UNKNOWN_LOCK_TYPE___, futexp2, 1);				\
+		__ret = INTERNAL_SYSCALL (futex, __err, 6, (futexp),			\
+								  __lll_private_flag (FUTEX_WAKE_OP, private), \
+								  (nr_wake), (nr_wake2), (futexp2),		\
+								  FUTEX_OP_CLEAR_WAKE_IF_GT_ONE);		\
+		if (mvee_should_futex_unlock())									\
+		{																\
+			*futexp2 = 0;												\
+		}																\
+		MVEE_POSTOP();													\
+		INTERNAL_SYSCALL_ERROR_P (__ret, __err);						\
+	})
+
+#define arch_cpu_relax() __asm__ __volatile__("mov\tr0,r0\t@ nop\n\t");
+
+#endif // !IS_IN (rtld)
diff --git a/sysdeps/arm/mvee-totalpartial-agent.h b/sysdeps/arm/mvee-totalpartial-agent.h
new file mode 100644
index 0000000000..233ab331b3
--- /dev/null
+++ b/sysdeps/arm/mvee-totalpartial-agent.h
@@ -0,0 +1,119 @@
+//
+// MVEE_PARTIAL_ORDER_REPLICATION: when defined, slaves will use
+// queue projection to replay synchronization operations in
+// partial order rather than total order. In other words,
+// the slaves will only respect the order in which the master
+// has performed its synchronization operations on a per-word
+// basis
+//
+#define MVEE_PARTIAL_ORDER_REPLICATION
+//
+// MVEE_EXTENDED_QUEUE: when defined, the locking operation and
+// mutex pointer are also logged in the queue.
+//
+#define MVEE_EXTENDED_QUEUE
+//
+// MVEE_LOG_EIPS: when defined, libc logs return addresses for
+// all locking operations into a seperate queue
+//
+// WARNING: enabling EIP logging _CAN_ trigger crashes! We're
+// using __builtin_return_address(2) to fetch the eip of the 
+// caller of the locking function. Unfortunately, libc uses inline
+// __libc_lock_* operations every now and then. When it does, 
+// the __builtin_... call will return the wrong caller and in some
+// cases (e.g. in do_system) it might try to fetch the eip beyond
+// the end of the stack!
+//
+#define MVEE_LOG_EIPS
+#define MVEE_STACK_DEPTH 5
+//
+// MVEE_CHECK_LOCK_TYPE: if this is defined, the slave will check
+// whether or not it's replaying a lock of the same type
+// (only works with the extended queue)
+//
+#define MVEE_CHECK_LOCK_TYPE
+//
+// MVEE_DEBUG_MALLOC: if this is defined, the slaves will check whether
+// their malloc behavior is synced with the master
+//
+// #define MVEE_DEBUG_MALLOC
+
+#define DEFINE_MVEE_QUEUE(name, has_eip_queue)				\
+  static unsigned long             mvee_##name##_buffer_data_start  = 0; \
+  static volatile unsigned int*    mvee_##name##_buffer_pos         = 0; \
+  static volatile unsigned int*    mvee_##name##_buffer_lock        = 0; \
+  static volatile unsigned int*    mvee_##name##_buffer_flush_cnt   = 0; \
+  static volatile unsigned char*   mvee_##name##_buffer_flushing    = 0; \
+  static unsigned long             mvee_##name##_buffer_slots       = 0; \
+  static void*                     mvee_##name##_eip_buffer         = NULL; \
+  static unsigned char             mvee_##name##_buffer_log_eips    = has_eip_queue;
+
+// this is extremely wasteful but required to prevent false sharing in the producer...
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+# ifdef MVEE_EXTENDED_QUEUE
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE (sizeof(long) + sizeof(short) * (mvee_num_childs + 1))
+# else
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE (sizeof(long) + sizeof(short) + (mvee_num_childs - 1))
+# endif
+#else
+# ifdef MVEE_EXTENDED_QUEUE
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE (3*sizeof(long))
+# else
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE sizeof(short)
+# endif
+#endif
+#define mvee_lock_buffer_slot_size 64
+#define mvee_malloc_buffer_slot_size 64
+
+// In the new queue layout, we want each replica's lock, position, 
+// flush_cnt and flushing word on one and the same cache line
+// Therefore, we round up the buffer ptr to a multiple of 64 for the master replica.
+// Each subsequent replica has its variables aligned on the next cache line boundary
+
+#define INIT_MVEE_QUEUE(name, slot_size, queue_ident)			\
+  if (!mvee_##name##_buffer_data_start)					\
+    {									\
+      long tmp_id = syscall(MVEE_GET_SHARED_BUFFER, 0, queue_ident, &mvee_##name##_buffer_slots, MVEE_LOCK_QUEUE_SLOT_SIZE); \
+      mvee_##name##_buffer_slots      = (mvee_##name##_buffer_slots - mvee_num_childs * 64) / mvee_##name##_buffer_slot_size - 2; \
+      void* tmp_buffer                = (void*)syscall(__NR_shmat, tmp_id, NULL, 0); \
+      mvee_##name##_buffer_lock       = (volatile unsigned int*)  (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64); \
+      mvee_##name##_buffer_pos        = (volatile unsigned int*)  (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64 + sizeof(int)); \
+      mvee_##name##_buffer_flush_cnt  = (volatile unsigned int*)  (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64 + sizeof(int) * 2); \
+      mvee_##name##_buffer_flushing   = (volatile unsigned char*) (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64 + sizeof(int) * 3); \
+     *mvee_##name##_buffer_lock       = 1; \
+      mvee_##name##_buffer_data_start = MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_num_childs * 64; \
+      if (mvee_##name##_buffer_log_eips)				\
+	{								\
+	  long eip_buffer_id = syscall(MVEE_GET_SHARED_BUFFER, 1,	\
+				      queue_ident, NULL, mvee_num_childs * sizeof(long) * MVEE_STACK_DEPTH, MVEE_STACK_DEPTH); \
+	  mvee_##name##_eip_buffer = (void*)syscall(__NR_shmat, eip_buffer_id, NULL, 0); \
+	}								\
+    }									
+
+#define MVEE_LOG_QUEUE_DATA(name, pos, offset, data)			\
+  *(typeof(data)*)(mvee_##name##_buffer_data_start + mvee_##name##_buffer_slot_size * (pos) + offset) = data;
+
+#define MVEE_READ_QUEUE_DATA(name, pos, offset, result)			\
+  result = *(typeof(result)*)(mvee_##name##_buffer_data_start + mvee_##name##_buffer_slot_size * pos + offset);
+
+#define MVEE_LOG_STACK(name, start_depth, pos)				\
+  mvee_log_stack(mvee_##name##_eip_buffer, sizeof(long) * mvee_num_childs * MVEE_STACK_DEPTH, pos, start_depth);
+
+extern void mvee_invalidate_buffer      (void);
+extern void mvee_atomic_postop_internal (unsigned char preop_result);
+extern int  mvee_should_sync_tid        (void);
+extern int  mvee_all_heaps_aligned      (char* heap, unsigned long alloc_size); 
+extern void mvee_xcheck                 (unsigned long item);
+
+#define MVEE_POSTOP() \
+  mvee_atomic_postop_internal(__tmp_mvee_preop);
+
+#ifdef MVEE_EXTENDED_QUEUE
+ extern unsigned char     mvee_atomic_preop_internal             (unsigned char is_store, void* word_ptr, unsigned short op_type);
+# define MVEE_PREOP(op_type, mem, is_store)					\
+	register unsigned char __tmp_mvee_preop = mvee_atomic_preop_internal(is_store, (void*)mem, op_type);
+#else
+ extern unsigned char     mvee_atomic_preop_internal            (unsigned char is_store, void* word_ptr);
+# define MVEE_PREOP(op_type, mem, is_store) \
+	register unsigned char __tmp_mvee_preop = mvee_atomic_preop_internal(is_store, (void*)mem);
+#endif // !MVEE_EXTENDED_QUEUE
diff --git a/sysdeps/arm/mvee-woc-agent.h b/sysdeps/arm/mvee-woc-agent.h
new file mode 100644
index 0000000000..3769ec279b
--- /dev/null
+++ b/sysdeps/arm/mvee-woc-agent.h
@@ -0,0 +1,16 @@
+#define MVEE_MAX_COUNTERS 65536
+
+#define MVEE_MALLOC_HOOK(type, msg, sz, ar_ptr, chunk_ptr)
+
+extern void          mvee_atomic_postop_internal (unsigned char preop_result);
+extern unsigned char mvee_atomic_preop_internal  (volatile void* word_ptr);
+extern int           mvee_should_sync_tid        (void);
+extern int           mvee_all_heaps_aligned      (char* heap, unsigned long alloc_size); 
+extern void          mvee_invalidate_buffer      (void);
+extern unsigned char mvee_should_futex_unlock    (void);
+
+#define MVEE_POSTOP()								\
+	mvee_atomic_postop_internal(__tmp_mvee_preop);
+
+#define MVEE_PREOP(op_type, mem, is_store)								\
+	register unsigned char  __tmp_mvee_preop = mvee_atomic_preop_internal(mem);
diff --git a/sysdeps/arm/mvee_infinite_loop.S b/sysdeps/arm/mvee_infinite_loop.S
new file mode 100644
index 0000000000..39fee195e1
--- /dev/null
+++ b/sysdeps/arm/mvee_infinite_loop.S
@@ -0,0 +1,13 @@
+#include <sysdep.h>
+
+	.text
+	.syntax unified
+
+ENTRY(mvee_infinite_loop)
+	nop
+	nop
+	nop
+	nop
+	nop
+	b mvee_infinite_loop
+END(mvee_infinite_loop)
diff --git a/sysdeps/nptl/lowlevellock-futex.h b/sysdeps/nptl/lowlevellock-futex.h
index 746f56f80c..67dc58b791 100644
--- a/sysdeps/nptl/lowlevellock-futex.h
+++ b/sysdeps/nptl/lowlevellock-futex.h
@@ -132,7 +132,7 @@
 
 /* Wake up up to NR_WAKE waiters on FUTEXP and NR_WAKE2 on FUTEXP2.
    Returns non-zero if error happened, zero if success.  */
-# define lll_futex_wake_unlock(futexp, nr_wake, nr_wake2, futexp2, private) \
+# define orig_lll_futex_wake_unlock(futexp, nr_wake, nr_wake2, futexp2, private) \
   lll_futex_syscall (6, futexp,                                         \
 		     __lll_private_flag (FUTEX_WAKE_OP, private),       \
 		     nr_wake, nr_wake2, futexp2,                        \
diff --git a/sysdeps/nptl/stdio-lock.h b/sysdeps/nptl/stdio-lock.h
index 1d6af7f215..0037626f3b 100644
--- a/sysdeps/nptl/stdio-lock.h
+++ b/sysdeps/nptl/stdio-lock.h
@@ -32,7 +32,11 @@ typedef struct { int lock; int cnt; void *owner; } _IO_lock_t;
 #define _IO_lock_initializer { LLL_LOCK_INITIALIZER, 0, NULL }
 
 #define _IO_lock_init(_name) \
-  ((void) ((_name) = (_IO_lock_t) _IO_lock_initializer))
+  ({ \
+    atomic_store_relaxed(&((_name).lock), LLL_LOCK_INITIALIZER); \
+    (_name).cnt = 0; \
+    (_name).owner = NULL; \
+  })
 
 #define _IO_lock_fini(_name) \
   ((void) 0)
diff --git a/sysdeps/unix/sysv/linux/check_pf.c b/sysdeps/unix/sysv/linux/check_pf.c
index 5a00cddeb1..ae6aa6a2c8 100644
--- a/sysdeps/unix/sysv/linux/check_pf.c
+++ b/sysdeps/unix/sysv/linux/check_pf.c
@@ -269,7 +269,7 @@ make_request (int fd, pid_t pid)
   if (seen_ipv6 && result != NULL)
     {
       result->timestamp = get_nl_timestamp ();
-      result->usecnt = 2;
+      atomic_store_relaxed(&result->usecnt, 2);
       result->seen_ipv4 = seen_ipv4;
       result->seen_ipv6 = true;
       result->in6ailen = result_len;
@@ -348,7 +348,7 @@ __check_pf (bool *seen_ipv4, bool *seen_ipv6,
       *in6ailen = data->in6ailen;
       *in6ai = data->in6ai;
 
-      if (olddata != NULL && olddata->usecnt > 0
+      if (olddata != NULL && atomic_load_relaxed(&olddata->usecnt) > 0
 	  && atomic_add_zero (&olddata->usecnt, -1))
 	free (olddata);
 
@@ -381,7 +381,7 @@ __free_in6ai (struct in6addrinfo *ai)
 	{
 	  __libc_lock_lock (lock);
 
-	  if (data->usecnt == 0)
+	  if (atomic_load_relaxed(&data->usecnt) == 0)
 	    /* Still unused.  */
 	    free (data);
 
diff --git a/sysdeps/unix/sysv/linux/x86/elision-lock.c b/sysdeps/unix/sysv/linux/x86/elision-lock.c
index f22e8aac72..947df4c751 100644
--- a/sysdeps/unix/sysv/linux/x86/elision-lock.c
+++ b/sysdeps/unix/sysv/linux/x86/elision-lock.c
@@ -99,7 +99,7 @@ __lll_lock_elision (int *futex, short *adapt_count, EXTRAARG int private)
       /* Use a normal lock until the threshold counter runs out.
 	 Lost updates possible.  */
       atomic_store_relaxed (adapt_count,
-	  atomic_load_relaxed (adapt_count) - 1);
+	  orig_atomic_load_relaxed (adapt_count) - 1);
     }
 
   /* Use a normal lock here.  */
diff --git a/sysdeps/unix/sysv/linux/x86/elision-trylock.c b/sysdeps/unix/sysv/linux/x86/elision-trylock.c
index 67939d98a4..b1e91c1dba 100644
--- a/sysdeps/unix/sysv/linux/x86/elision-trylock.c
+++ b/sysdeps/unix/sysv/linux/x86/elision-trylock.c
@@ -68,7 +68,7 @@ __lll_trylock_elision (int *futex, short *adapt_count)
     {
       /* Lost updates are possible but harmless (see above).  */
       atomic_store_relaxed (adapt_count,
-	  atomic_load_relaxed (adapt_count) - 1);
+	  orig_atomic_load_relaxed (adapt_count) - 1);
     }
 
   return lll_trylock (*futex);
diff --git a/sysdeps/unix/sysv/linux/x86/elision-unlock.c b/sysdeps/unix/sysv/linux/x86/elision-unlock.c
index 0153a9adb9..38c1b91631 100644
--- a/sysdeps/unix/sysv/linux/x86/elision-unlock.c
+++ b/sysdeps/unix/sysv/linux/x86/elision-unlock.c
@@ -25,7 +25,7 @@ __lll_unlock_elision(int *lock, int private)
 {
   /* When the lock was free we're in a transaction.
      When you crash here you unlocked a free lock.  */
-  if (*lock == 0)
+	if (atomic_load_relaxed(lock) == 0)
     _xend();
   else
     lll_unlock ((*lock), private);
diff --git a/sysdeps/unix/sysv/linux/x86_64/cancellation.S b/sysdeps/unix/sysv/linux/x86_64/cancellation.S
deleted file mode 100644
index 256529c9f6..0000000000
--- a/sysdeps/unix/sysv/linux/x86_64/cancellation.S
+++ /dev/null
@@ -1,104 +0,0 @@
-/* Copyright (C) 2009-2020 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2009.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <https://www.gnu.org/licenses/>.  */
-
-#include <sysdep.h>
-#include <tcb-offsets.h>
-#include <kernel-features.h>
-#include <lowlevellock-futex.h>
-
-#define PTHREAD_UNWIND JUMPTARGET(__pthread_unwind)
-#if IS_IN (libpthread)
-# if defined SHARED && !defined NO_HIDDEN
-#  undef PTHREAD_UNWIND
-#  define PTHREAD_UNWIND __GI___pthread_unwind
-# endif
-#else
-# ifndef SHARED
-	.weak __pthread_unwind
-# endif
-#endif
-
-
-#define LOAD_PRIVATE_FUTEX_WAIT(reg) \
-	movl	$(FUTEX_WAIT | FUTEX_PRIVATE_FLAG), reg
-
-/* It is crucial that the functions in this file don't modify registers
-   other than %rax and %r11.  The syscall wrapper code depends on this
-   because it doesn't explicitly save the other registers which hold
-   relevant values.  */
-	.text
-
-	.hidden __pthread_enable_asynccancel
-ENTRY(__pthread_enable_asynccancel)
-	movl	%fs:CANCELHANDLING, %eax
-2:	movl	%eax, %r11d
-	orl	$TCB_CANCELTYPE_BITMASK, %r11d
-	cmpl	%eax, %r11d
-	je	1f
-
-	lock
-	cmpxchgl %r11d, %fs:CANCELHANDLING
-	jnz	2b
-
-	andl	$(TCB_CANCELSTATE_BITMASK|TCB_CANCELTYPE_BITMASK|TCB_CANCELED_BITMASK|TCB_EXITING_BITMASK|TCB_CANCEL_RESTMASK|TCB_TERMINATED_BITMASK), %r11d
-	cmpl	$(TCB_CANCELTYPE_BITMASK|TCB_CANCELED_BITMASK), %r11d
-	je	3f
-
-1:	ret
-
-3:	subq	$8, %rsp
-	cfi_adjust_cfa_offset(8)
-	LP_OP(mov) $TCB_PTHREAD_CANCELED, %fs:RESULT
-	lock
-	orl	$TCB_EXITING_BITMASK, %fs:CANCELHANDLING
-	mov	%fs:CLEANUP_JMP_BUF, %RDI_LP
-	call	PTHREAD_UNWIND
-	hlt
-END(__pthread_enable_asynccancel)
-
-
-	.hidden __pthread_disable_asynccancel
-ENTRY(__pthread_disable_asynccancel)
-	testl	$TCB_CANCELTYPE_BITMASK, %edi
-	jnz	1f
-
-	movl	%fs:CANCELHANDLING, %eax
-2:	movl	%eax, %r11d
-	andl	$~TCB_CANCELTYPE_BITMASK, %r11d
-	lock
-	cmpxchgl %r11d, %fs:CANCELHANDLING
-	jnz	2b
-
-	movl	%r11d, %eax
-3:	andl	$(TCB_CANCELING_BITMASK|TCB_CANCELED_BITMASK), %eax
-	cmpl	$TCB_CANCELING_BITMASK, %eax
-	je	4f
-1:	ret
-
-	/* Performance doesn't matter in this loop.  We will
-	   delay until the thread is canceled.  And we will unlikely
-	   enter the loop twice.  */
-4:	mov	%fs:0, %RDI_LP
-	movl	$__NR_futex, %eax
-	xorq	%r10, %r10
-	addq	$CANCELHANDLING, %rdi
-	LOAD_PRIVATE_FUTEX_WAIT (%esi)
-	syscall
-	movl	%fs:CANCELHANDLING, %eax
-	jmp	3b
-END(__pthread_disable_asynccancel)
diff --git a/sysdeps/unix/sysv/linux/x86_64/libc-cancellation.S b/sysdeps/unix/sysv/linux/x86_64/libc-cancellation.S
deleted file mode 100644
index 6c6b2a814d..0000000000
--- a/sysdeps/unix/sysv/linux/x86_64/libc-cancellation.S
+++ /dev/null
@@ -1,21 +0,0 @@
-/* Copyright (C) 2009-2020 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2009.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <https://www.gnu.org/licenses/>.  */
-
-#define __pthread_enable_asynccancel __libc_enable_asynccancel
-#define __pthread_disable_asynccancel __libc_disable_asynccancel
-#include "cancellation.S"
diff --git a/sysdeps/unix/sysv/linux/x86_64/librt-cancellation.S b/sysdeps/unix/sysv/linux/x86_64/librt-cancellation.S
deleted file mode 100644
index 71ff1ca8ea..0000000000
--- a/sysdeps/unix/sysv/linux/x86_64/librt-cancellation.S
+++ /dev/null
@@ -1,21 +0,0 @@
-/* Copyright (C) 2009-2020 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2009.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <https://www.gnu.org/licenses/>.  */
-
-#define __pthread_enable_asynccancel __librt_enable_asynccancel
-#define __pthread_disable_asynccancel __librt_disable_asynccancel
-#include "cancellation.S"
diff --git a/sysdeps/x86/atomic-machine.h b/sysdeps/x86/atomic-machine.h
index bb49648374..c0789554ed 100644
--- a/sysdeps/x86/atomic-machine.h
+++ b/sysdeps/x86/atomic-machine.h
@@ -74,13 +74,13 @@ typedef uintmax_t uatomic_max_t;
 #endif
 #define ATOMIC_EXCHANGE_USES_CAS	0
 
-#define atomic_compare_and_exchange_val_acq(mem, newval, oldval) \
+#define orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval) \
   __sync_val_compare_and_swap (mem, oldval, newval)
-#define atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
+#define orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
   (! __sync_bool_compare_and_swap (mem, oldval, newval))
 
 
-#define __arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval) \
+#define orig___arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval) \
   ({ __typeof (*mem) ret;						      \
      __asm __volatile ("cmpl $0, %%" SEG_REG ":%P5\n\t"			      \
 		       "je 0f\n\t"					      \
@@ -91,7 +91,7 @@ typedef uintmax_t uatomic_max_t;
 			 "i" (offsetof (tcbhead_t, multiple_threads)));	      \
      ret; })
 
-#define __arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval) \
+#define orig___arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval) \
   ({ __typeof (*mem) ret;						      \
      __asm __volatile ("cmpl $0, %%" SEG_REG ":%P5\n\t"			      \
 		       "je 0f\n\t"					      \
@@ -102,7 +102,7 @@ typedef uintmax_t uatomic_max_t;
 			 "i" (offsetof (tcbhead_t, multiple_threads)));	      \
      ret; })
 
-#define __arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval) \
+#define orig___arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval) \
   ({ __typeof (*mem) ret;						      \
      __asm __volatile ("cmpl $0, %%" SEG_REG ":%P5\n\t"			      \
 		       "je 0f\n\t"					      \
@@ -114,7 +114,7 @@ typedef uintmax_t uatomic_max_t;
      ret; })
 
 #ifdef __x86_64__
-# define __arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval) \
+# define orig___arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval) \
   ({ __typeof (*mem) ret;						      \
      __asm __volatile ("cmpl $0, %%fs:%P5\n\t"				      \
 		       "je 0f\n\t"					      \
@@ -176,7 +176,7 @@ typedef uintmax_t uatomic_max_t;
 
 
 /* Note that we need no lock prefix.  */
-#define atomic_exchange_acq(mem, newvalue) \
+#define orig_atomic_exchange_acq(mem, newvalue) \
   ({ __typeof (*mem) result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile ("xchgb %b0, %1"				      \
@@ -203,7 +203,7 @@ typedef uintmax_t uatomic_max_t;
      result; })
 
 
-#define __arch_exchange_and_add_body(lock, pfx, mem, value) \
+#define orig___arch_exchange_and_add_body(lock, pfx, mem, value) \
   ({ __typeof (*mem) __result;						      \
      __typeof (value) __addval = (value);				      \
      if (sizeof (*mem) == 1)						      \
@@ -231,18 +231,18 @@ typedef uintmax_t uatomic_max_t;
        __result = do_exchange_and_add_val_64_acq (pfx, (mem), __addval);      \
      __result; })
 
-#define atomic_exchange_and_add(mem, value) \
+#define orig_atomic_exchange_and_add(mem, value) \
   __sync_fetch_and_add (mem, value)
 
 #define __arch_exchange_and_add_cprefix \
   "cmpl $0, %%" SEG_REG ":%P4\n\tje 0f\n\tlock\n0:\t"
 
-#define catomic_exchange_and_add(mem, value) \
-  __arch_exchange_and_add_body (__arch_exchange_and_add_cprefix, __arch_c,    \
+#define orig_catomic_exchange_and_add(mem, value) \
+  orig___arch_exchange_and_add_body (__arch_exchange_and_add_cprefix, __arch_c,    \
 				mem, value)
 
 
-#define __arch_add_body(lock, pfx, apfx, mem, value) \
+#define orig___arch_add_body(lock, pfx, apfx, mem, value) \
   do {									      \
     if (__builtin_constant_p (value) && (value) == 1)			      \
       pfx##_increment (mem);						      \
@@ -273,17 +273,17 @@ typedef uintmax_t uatomic_max_t;
       do_add_val_64_acq (apfx, (mem), (value));				      \
   } while (0)
 
-# define atomic_add(mem, value) \
-  __arch_add_body (LOCK_PREFIX, atomic, __arch, mem, value)
+# define orig_atomic_add(mem, value) \
+  orig___arch_add_body (LOCK_PREFIX, orig_atomic, __arch, mem, value)
 
 #define __arch_add_cprefix \
   "cmpl $0, %%" SEG_REG ":%P3\n\tje 0f\n\tlock\n0:\t"
 
-#define catomic_add(mem, value) \
-  __arch_add_body (__arch_add_cprefix, atomic, __arch_c, mem, value)
+#define orig_catomic_add(mem, value) \
+  orig___arch_add_body (__arch_add_cprefix, orig_atomic, __arch_c, mem, value)
 
 
-#define atomic_add_negative(mem, value) \
+#define orig_atomic_add_negative(mem, value) \
   ({ unsigned char __result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile (LOCK_PREFIX "addb %b2, %0; sets %1"		      \
@@ -307,7 +307,7 @@ typedef uintmax_t uatomic_max_t;
      __result; })
 
 
-#define atomic_add_zero(mem, value) \
+#define orig_atomic_add_zero(mem, value) \
   ({ unsigned char __result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile (LOCK_PREFIX "addb %b2, %0; setz %1"		      \
@@ -331,7 +331,7 @@ typedef uintmax_t uatomic_max_t;
      __result; })
 
 
-#define __arch_increment_body(lock, pfx, mem) \
+#define orig___arch_increment_body(lock, pfx, mem) \
   do {									      \
     if (sizeof (*mem) == 1)						      \
       __asm __volatile (lock "incb %b0"					      \
@@ -357,16 +357,16 @@ typedef uintmax_t uatomic_max_t;
       do_add_val_64_acq (pfx, mem, 1);					      \
   } while (0)
 
-#define atomic_increment(mem) __arch_increment_body (LOCK_PREFIX, __arch, mem)
+#define orig_atomic_increment(mem) orig___arch_increment_body (LOCK_PREFIX, __arch, mem)
 
 #define __arch_increment_cprefix \
   "cmpl $0, %%" SEG_REG ":%P2\n\tje 0f\n\tlock\n0:\t"
 
-#define catomic_increment(mem) \
-  __arch_increment_body (__arch_increment_cprefix, __arch_c, mem)
+#define orig_catomic_increment(mem) \
+  orig___arch_increment_body (__arch_increment_cprefix, __arch_c, mem)
 
 
-#define atomic_increment_and_test(mem) \
+#define orig_atomic_increment_and_test(mem) \
   ({ unsigned char __result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile (LOCK_PREFIX "incb %b0; sete %b1"		      \
@@ -389,7 +389,7 @@ typedef uintmax_t uatomic_max_t;
      __result; })
 
 
-#define __arch_decrement_body(lock, pfx, mem) \
+#define orig___arch_decrement_body(lock, pfx, mem) \
   do {									      \
     if (sizeof (*mem) == 1)						      \
       __asm __volatile (lock "decb %b0"					      \
@@ -415,16 +415,16 @@ typedef uintmax_t uatomic_max_t;
       do_add_val_64_acq (pfx, mem, -1);					      \
   } while (0)
 
-#define atomic_decrement(mem) __arch_decrement_body (LOCK_PREFIX, __arch, mem)
+#define orig_atomic_decrement(mem) orig___arch_decrement_body (LOCK_PREFIX, __arch, mem)
 
 #define __arch_decrement_cprefix \
   "cmpl $0, %%" SEG_REG ":%P2\n\tje 0f\n\tlock\n0:\t"
 
-#define catomic_decrement(mem) \
-  __arch_decrement_body (__arch_decrement_cprefix, __arch_c, mem)
+#define orig_catomic_decrement(mem) \
+  orig___arch_decrement_body (__arch_decrement_cprefix, __arch_c, mem)
 
 
-#define atomic_decrement_and_test(mem) \
+#define orig_atomic_decrement_and_test(mem) \
   ({ unsigned char __result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile (LOCK_PREFIX "decb %b0; sete %1"		      \
@@ -445,7 +445,7 @@ typedef uintmax_t uatomic_max_t;
      __result; })
 
 
-#define atomic_bit_set(mem, bit) \
+#define orig_atomic_bit_set(mem, bit) \
   do {									      \
     if (sizeof (*mem) == 1)						      \
       __asm __volatile (LOCK_PREFIX "orb %b2, %0"			      \
@@ -472,7 +472,7 @@ typedef uintmax_t uatomic_max_t;
   } while (0)
 
 
-#define atomic_bit_test_set(mem, bit) \
+#define orig_atomic_bit_test_set(mem, bit) \
   ({ unsigned char __result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile (LOCK_PREFIX "btsb %3, %1; setc %0"		      \
@@ -495,7 +495,7 @@ typedef uintmax_t uatomic_max_t;
      __result; })
 
 
-#define __arch_and_body(lock, mem, mask) \
+#define orig___arch_and_body(lock, mem, mask) \
   do {									      \
     if (sizeof (*mem) == 1)						      \
       __asm __volatile (lock "andb %b1, %0"				      \
@@ -524,12 +524,12 @@ typedef uintmax_t uatomic_max_t;
 #define __arch_cprefix \
   "cmpl $0, %%" SEG_REG ":%P3\n\tje 0f\n\tlock\n0:\t"
 
-#define atomic_and(mem, mask) __arch_and_body (LOCK_PREFIX, mem, mask)
+#define orig_atomic_and(mem, mask) orig___arch_and_body (LOCK_PREFIX, mem, mask)
 
-#define catomic_and(mem, mask) __arch_and_body (__arch_cprefix, mem, mask)
+#define orig_catomic_and(mem, mask) orig___arch_and_body (__arch_cprefix, mem, mask)
 
 
-#define __arch_or_body(lock, mem, mask) \
+#define orig___arch_or_body(lock, mem, mask) \
   do {									      \
     if (sizeof (*mem) == 1)						      \
       __asm __volatile (lock "orb %b1, %0"				      \
@@ -555,9 +555,9 @@ typedef uintmax_t uatomic_max_t;
       __atomic_link_error ();						      \
   } while (0)
 
-#define atomic_or(mem, mask) __arch_or_body (LOCK_PREFIX, mem, mask)
+#define orig_atomic_or(mem, mask) orig___arch_or_body (LOCK_PREFIX, mem, mask)
 
-#define catomic_or(mem, mask) __arch_or_body (__arch_cprefix, mem, mask)
+#define orig_catomic_or(mem, mask) orig___arch_or_body (__arch_cprefix, mem, mask)
 
 /* We don't use mfence because it is supposedly slower due to having to
    provide stronger guarantees (e.g., regarding self-modifying code).  */
@@ -568,4 +568,818 @@ typedef uintmax_t uatomic_max_t;
 
 #define atomic_spin_nop() __asm ("pause")
 
+/*--------------------------------------------------------------------------------
+                                  MVEE PATCHES
+--------------------------------------------------------------------------------*/
+#define USE_MVEE_LIBC
+
+#ifdef USE_MVEE_LIBC
+#define MVEE_FAKE_SYSCALL_BASE          0x6FFFFFFF
+#define MVEE_GET_MASTERTHREAD_ID        MVEE_FAKE_SYSCALL_BASE + 3
+#define MVEE_GET_SHARED_BUFFER          MVEE_FAKE_SYSCALL_BASE + 4
+#define MVEE_FLUSH_SHARED_BUFFER        MVEE_FAKE_SYSCALL_BASE + 5
+#define MVEE_SET_INFINITE_LOOP_PTR      MVEE_FAKE_SYSCALL_BASE + 6
+#define MVEE_TOGGLESYNC                 MVEE_FAKE_SYSCALL_BASE + 7
+#define MVEE_SET_SHARED_BUFFER_POS_PTR  MVEE_FAKE_SYSCALL_BASE + 8
+#define MVEE_RUNS_UNDER_MVEE_CONTROL    MVEE_FAKE_SYSCALL_BASE + 9
+#define MVEE_GET_THREAD_NUM             MVEE_FAKE_SYSCALL_BASE + 10
+#define MVEE_SET_SYNC_PRIMITIVES_PTR    MVEE_FAKE_SYSCALL_BASE + 12
+#define MVEE_ALL_HEAPS_ALIGNED          MVEE_FAKE_SYSCALL_BASE + 13
+#define MVEE_GET_VIRTUALIZED_ARGV0      MVEE_FAKE_SYSCALL_BASE + 17
+#define MVEE_LIBC_LOCK_BUFFER           3
+#define MVEE_LIBC_MALLOC_DEBUG_BUFFER   11
+#define MVEE_LIBC_ATOMIC_BUFFER         13
+#define MVEE_LIBC_LOCK_BUFFER_PARTIAL   16
+#define MVEE_FUTEX_WAIT_TID             30
+
+enum mvee_alloc_types
+  {
+  LIBC_MALLOC,
+  LIBC_FREE,
+  LIBC_REALLOC,
+  LIBC_MEMALIGN,
+  LIBC_CALLOC,
+  MALLOC_TRIM,
+  HEAP_TRIM,
+  MALLOC_CONSOLIDATE,
+  ARENA_GET2,
+  _INT_MALLOC,
+  _INT_FREE,
+  _INT_REALLOC
+  };
+
+//
+// Atomic operations list. Keep this in sync with MVEE/Inc/MVEE_shm.h
+//
+enum mvee_base_atomics
+{
+    // LOAD OPERATIONS FIRST!!! DO NOT CHANGE THIS CONVENTION
+    ATOMIC_FORCED_READ,
+    ATOMIC_LOAD,
+    // THE FOLLOWING IS NOT AN ACTUAL ATOMIC OPERATION, IT JUST DENOTES THE END OF THE LOAD-ONLY ATOMICS!!!
+    ATOMIC_LOAD_MAX,
+    // STORES AFTER LOADS
+    CATOMIC_AND,
+    CATOMIC_OR,
+    CATOMIC_EXCHANGE_AND_ADD,
+    CATOMIC_ADD,
+    CATOMIC_INCREMENT,
+    CATOMIC_DECREMENT,
+    CATOMIC_MAX,
+    ATOMIC_COMPARE_AND_EXCHANGE_VAL,
+    ATOMIC_COMPARE_AND_EXCHANGE_BOOL,
+    ATOMIC_EXCHANGE,
+    ATOMIC_EXCHANGE_AND_ADD,
+    ATOMIC_INCREMENT_AND_TEST,
+    ATOMIC_DECREMENT_AND_TEST,
+	ATOMIC_ADD_NEGATIVE,
+    ATOMIC_ADD_ZERO,
+    ATOMIC_ADD,
+	ATOMIC_OR,
+	ATOMIC_OR_VAL,
+    ATOMIC_INCREMENT,
+    ATOMIC_DECREMENT,
+    ATOMIC_BIT_TEST_SET,
+    ATOMIC_BIT_SET,
+    ATOMIC_AND,
+	ATOMIC_AND_VAL,
+    ATOMIC_STORE,
+	ATOMIC_MIN,
+    ATOMIC_MAX,
+    ATOMIC_DECREMENT_IF_POSITIVE,
+	ATOMIC_FETCH_ADD,
+	ATOMIC_FETCH_AND,
+	ATOMIC_FETCH_OR,
+	ATOMIC_FETCH_XOR,
+    ___UNKNOWN_LOCK_TYPE___,
+    __MVEE_BASE_ATOMICS_MAX__
+};
+
+enum mvee_extended_atomics {
+  mvee_atomic_load_n,
+  mvee_atomic_load,
+  mvee_atomic_store_n,
+  mvee_atomic_store,
+  mvee_atomic_exchange_n,
+  mvee_atomic_exchange,
+  mvee_atomic_compare_exchange_n,
+  mvee_atomic_compare_exchange,
+  mvee_atomic_add_fetch,
+  mvee_atomic_sub_fetch,
+  mvee_atomic_and_fetch,
+  mvee_atomic_xor_fetch,
+  mvee_atomic_or_fetch,
+  mvee_atomic_nand_fetch,
+  mvee_atomic_fetch_add,
+  mvee_atomic_fetch_sub,
+  mvee_atomic_fetch_and,
+  mvee_atomic_fetch_xor,
+  mvee_atomic_fetch_or,
+  mvee_atomic_fetch_nand,
+  mvee_atomic_test_and_set,
+  mvee_atomic_clear,
+  mvee_sync_fetch_and_add,
+  mvee_sync_fetch_and_sub,
+  mvee_sync_fetch_and_or,
+  mvee_sync_fetch_and_and,
+  mvee_sync_fetch_and_xor,
+  mvee_sync_fetch_and_nand,
+  mvee_sync_add_and_fetch,
+  mvee_sync_sub_and_fetch,
+  mvee_sync_or_and_fetch,
+  mvee_sync_and_and_fetch,
+  mvee_sync_xor_and_fetch,
+  mvee_sync_nand_and_fetch,
+  mvee_sync_bool_compare_and_swap,
+  mvee_sync_val_compare_and_swap,
+  mvee_sync_lock_test_and_set,
+  mvee_sync_lock_release,
+  mvee_atomic_ops_max
+};
+
+#define MVEE_ROUND_UP(x, multiple)		\
+  ((x + (multiple - 1)) & ~(multiple -1))
+
+#define MVEE_MIN(a, b) ((a > b) ? (b) : (a))
+
+# ifdef MVEE_USE_TOTALPARTIAL_AGENT
+#  include "mvee-totalpartial-agent.h"
+# else
+#  include "mvee-woc-agent.h"
+# endif
+
+#endif
+
+// We don't use our sync agent in the dynamic loader so just use the original atomics everywhere
+#if IS_IN (rtld) || !defined USE_MVEE_LIBC
+
+//
+// generic atomics (include/atomic.h and sysdeps/arch/atomic-machine.h)
+//
+#define __arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_val_acq(mem, newval, oldval) orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_val_rel(mem, newval, oldval) orig_atomic_compare_and_exchange_val_rel(mem, newval, oldval)
+#define atomic_compare_and_exchange_bool_acq(mem, newval, oldval) orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_bool_rel(mem, newval, oldval) orig_atomic_compare_and_exchange_bool_rel(mem, newval, oldval)
+#define atomic_exchange_acq(mem, newvalue) orig_atomic_exchange_acq(mem, newvalue)
+#define atomic_exchange_rel(mem, newvalue) orig_atomic_exchange_rel(mem, newvalue)
+#define atomic_exchange_and_add(mem, value) orig_atomic_exchange_and_add(mem, value)
+#define atomic_exchange_and_add_acq(mem, value) orig_atomic_exchange_and_add_acq(mem, value)
+#define atomic_exchange_and_add_rel(mem, value) orig_atomic_exchange_and_add_rel(mem, value)
+#define atomic_add(mem, value) orig_atomic_add(mem, value)
+#define atomic_increment(mem) orig_atomic_increment(mem)
+#define atomic_increment_and_test(mem) orig_atomic_increment_and_test(mem)
+#define atomic_increment_val(mem) orig_atomic_increment_val(mem)
+#define atomic_decrement(mem) orig_atomic_decrement(mem)
+#define atomic_decrement_and_test(mem) orig_atomic_decrement_and_test(mem)
+#define atomic_decrement_val(mem) orig_atomic_decrement_val(mem)
+#define atomic_add_negative(mem, value) orig_atomic_add_negative(mem, value)
+#define atomic_add_zero(mem, value) orig_atomic_add_zero(mem, value)
+#define atomic_bit_set(mem, bit) orig_atomic_bit_set(mem, bit)
+#define atomic_bit_test_set(mem, bit) orig_atomic_bit_test_set(mem, bit)
+#define atomic_and(mem, mask) orig_atomic_and(mem, mask)
+#define atomic_or(mem, mask) orig_atomic_or(mem, mask)
+#define atomic_max(mem, value) orig_atomic_max(mem, value)
+#define atomic_min(mem, value) orig_atomic_min(mem, value)
+#define atomic_decrement_if_positive(mem) orig_atomic_decrement_if_positive(mem)
+#define atomic_and_val(mem, mask) orig_atomic_and_val(mem, mask)
+#define atomic_or_val(mem, mask) orig_atomic_or_val(mem, mask)
+#define atomic_forced_read(x) orig_atomic_forced_read(x)
+#define catomic_compare_and_exchange_val_acq(mem, newval, oldval) orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval)
+#define catomic_compare_and_exchange_val_rel(mem, newval, oldval) orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval)
+#define catomic_compare_and_exchange_bool_acq(mem, newval, oldval) orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval)
+#define catomic_compare_and_exchange_bool_rel(mem, newval, oldval) orig_catomic_compare_and_exchange_bool_rel(mem, newval, oldval)
+#define catomic_exchange_and_add(mem, value) orig_catomic_exchange_and_add(mem, value)
+#define catomic_add(mem, value) orig_catomic_add(mem, value)
+#define catomic_increment(mem) orig_catomic_increment(mem)
+#define catomic_increment_val(mem) orig_catomic_increment_val(mem)
+#define catomic_decrement(mem) orig_catomic_decrement(mem)
+#define catomic_decrement_val(mem) orig_catomic_decrement_val(mem)
+#define catomic_and(mem, mask) orig_catomic_and(mem, mask)
+#define catomic_or(mem, mask) orig_catomic_or(mem, mask)
+#define catomic_max(mem, value) orig_catomic_max(mem, value)
+
+//
+// C11-style atomics (include/atomic.h)
+//
+#define atomic_load_relaxed(mem) orig_atomic_load_relaxed(mem)
+#define atomic_load_acquire(mem) orig_atomic_load_acquire(mem)
+#define atomic_store_relaxed(mem, val) orig_atomic_store_relaxed(mem, val)
+#define atomic_store_release(mem, val) orig_atomic_store_release(mem, val)
+#define atomic_compare_exchange_weak_relaxed(mem, expected, desired) orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired)
+#define atomic_compare_exchange_weak_acquire(mem, expected, desired) orig_atomic_compare_exchange_weak_acquire(mem, expected, desired) 
+#define atomic_compare_exchange_weak_release(mem, expected, desired) orig_atomic_compare_exchange_weak_release(mem, expected, desired)
+#define atomic_exchange_relaxed(mem, desired) orig_atomic_exchange_relaxed(mem, desired)
+#define atomic_exchange_acquire(mem, desired) orig_atomic_exchange_acquire(mem, desired)
+#define atomic_exchange_release(mem, desired) orig_atomic_exchange_release(mem, desired)
+#define atomic_fetch_add_relaxed(mem, operand) orig_atomic_fetch_add_relaxed(mem, operand)
+#define atomic_fetch_add_acquire(mem, operand) orig_atomic_fetch_add_acquire(mem, operand)
+#define atomic_fetch_add_release(mem, operand) orig_atomic_fetch_add_release(mem, operand)
+#define atomic_fetch_add_acq_rel(mem, operand) orig_atomic_fetch_add_acq_rel(mem, operand)
+#define atomic_fetch_and_relaxed(mem, operand) orig_atomic_fetch_and_relaxed(mem, operand)
+#define atomic_fetch_and_acquire(mem, operand) orig_atomic_fetch_and_acquire(mem, operand)
+#define atomic_fetch_and_release(mem, operand) orig_atomic_fetch_and_release(mem, operand)
+#define atomic_fetch_or_relaxed(mem, operand) orig_atomic_fetch_or_relaxed(mem, operand) 
+#define atomic_fetch_or_acquire(mem, operand) orig_atomic_fetch_or_acquire(mem, operand) 
+#define atomic_fetch_or_release(mem, operand) orig_atomic_fetch_or_release(mem, operand) 
+#define atomic_fetch_xor_release(mem, operand) orig_atomic_fetch_xor_release(mem, operand) 
+
+//
+// MVEE additions
+//
+#define THREAD_ATOMIC_GETMEM(descr, member) THREAD_GETMEM(descr, member)
+#define THREAD_ATOMIC_SETMEM(descr, member, val) THREAD_SETMEM(descr, member, val)
+
+
+#else // !IS_IN_rtld
+
+//
+// architecture-specific atomics (atomic-machine.h)
+//
+#define __arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval)	\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		typeof(*mem) ____result = orig___arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval)	\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		typeof(*mem) ____result = orig___arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval)	\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		typeof(*mem) ____result = orig___arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval)	\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		typeof(*mem) ____result = orig___arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_val_acq(mem, newval, oldval)		\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		typeof(*mem) ____result = orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_val_rel(mem, newval, oldval)		\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		typeof(*mem) ____result = orig_atomic_compare_and_exchange_val_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_bool_acq(mem, newval, oldval)		\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		bool ____result = orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_bool_rel(mem, newval, oldval)		\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		bool ____result = orig_atomic_compare_and_exchange_bool_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_exchange_acq(mem, newvalue)						\
+	({															\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);					\
+		typeof(*mem) ____result = orig_atomic_exchange_acq(mem, newvalue);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_rel(mem, newvalue)						\
+	({															\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);					\
+		typeof(*mem) ____result = orig_atomic_exchange_rel(mem, newvalue);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add(mem, value)						\
+	({															\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		typeof(*mem) ____result = orig_atomic_exchange_and_add(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add_acq(mem, value)					\
+	({															\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		typeof(*mem) ____result = orig_atomic_exchange_and_add_acq(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add_rel(mem, value)					\
+	({															\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		typeof(*mem) ____result = orig_atomic_exchange_and_add_rel(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_add(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_ADD, mem, 1);			\
+		orig_atomic_add(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_increment(mem)					\
+	({											\
+		MVEE_PREOP(ATOMIC_INCREMENT, mem, 1);	\
+		orig_atomic_increment(mem);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_increment_and_test(mem)						\
+	({														\
+		MVEE_PREOP(ATOMIC_INCREMENT_AND_TEST, mem, 1);		\
+		unsigned char ____result = orig_atomic_increment_and_test(mem);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_increment_val(mem)				\
+	({											\
+		MVEE_PREOP(ATOMIC_INCREMENT, mem, 1);	\
+		typeof(*mem) ____result = orig_atomic_increment_val(mem);	\
+		MVEE_POSTOP();							\
+		____result;								\
+	})
+
+#define atomic_decrement(mem)					\
+	({											\
+		MVEE_PREOP(ATOMIC_DECREMENT, mem, 1);	\
+		orig_atomic_decrement(mem);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_decrement_and_test(mem)						\
+	({														\
+		MVEE_PREOP(ATOMIC_DECREMENT_AND_TEST, mem, 1);		\
+		unsigned char ____result = orig_atomic_decrement_and_test(mem);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_decrement_val(mem)				\
+	({											\
+		MVEE_PREOP(ATOMIC_DECREMENT, mem, 1);	\
+		typeof(*mem) ____result = orig_atomic_decrement_val(mem);	\
+		MVEE_POSTOP();							\
+		____result;								\
+	})
+
+#define atomic_add_negative(mem, value)						\
+	({														\
+		MVEE_PREOP(ATOMIC_ADD, mem, 1);						\
+		unsigned char ____result = orig_atomic_add_negative(mem, value);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_add_zero(mem, value)						\
+	({													\
+		MVEE_PREOP(ATOMIC_ADD_ZERO, mem, 1);			\
+		unsigned char ____result = orig_atomic_add_zero(mem, value);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+#define atomic_bit_set(mem, bit)				\
+	({											\
+		MVEE_PREOP(ATOMIC_BIT_SET, mem, 1);		\
+		orig_atomic_bit_set(mem, bit);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_bit_test_set(mem, bit)						\
+	({														\
+		MVEE_PREOP(ATOMIC_BIT_TEST_SET, mem, 1);			\
+		unsigned char ____result = orig_atomic_bit_test_set(mem, bit);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_and(mem, mask)					\
+	({											\
+		MVEE_PREOP(ATOMIC_AND, mem, 1);			\
+		orig_atomic_and(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_or(mem, mask)					\
+	({											\
+		MVEE_PREOP(ATOMIC_OR, mem, 1);			\
+		orig_atomic_or(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_max(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_MAX, mem, 1);			\
+		orig_atomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_min(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_MIN, mem, 1);			\
+		orig_atomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_decrement_if_positive(mem)					\
+	({														\
+		MVEE_PREOP(ATOMIC_DECREMENT_IF_POSITIVE, mem, 1);	\
+		__typeof(*mem) __result = orig_atomic_decrement_if_positive(mem);	\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_and_val(mem, mask)							\
+	({														\
+		MVEE_PREOP(ATOMIC_AND_VAL, mem, 1);					\
+		__typeof(*mem) __result = orig_atomic_and_val(mem);				\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_or_val(mem, mask)							\
+	({														\
+		MVEE_PREOP(ATOMIC_OR_VAL, mem, 1);					\
+		__typeof(*mem) __result = orig_atomic_or_val(mem);					\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_forced_read(x)						\
+	({												\
+		MVEE_PREOP(ATOMIC_FORCED_READ, &x, 0);		\
+		typeof(x) ____result = orig_atomic_forced_read(x);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define catomic_compare_and_exchange_val_acq(mem, newval, oldval)		\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		typeof(*mem) ____result = orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_val_rel(mem, newval, oldval)		\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		typeof(*mem) ____result = orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_bool_acq(mem, newval, oldval)		\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		bool ____result = orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_bool_rel(mem, newval, oldval)		\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		bool ____result = orig_catomic_compare_and_exchange_bool_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_exchange_and_add(mem, value)					\
+	({															\
+		MVEE_PREOP(CATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		typeof(*mem) ____result = orig_catomic_exchange_and_add(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define catomic_add(mem, value)					\
+	({											\
+		MVEE_PREOP(CATOMIC_ADD, mem, 1);		\
+		orig_catomic_add(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_increment(mem)					\
+	({											\
+		MVEE_PREOP(CATOMIC_INCREMENT, mem, 1);	\
+		orig_catomic_increment(mem);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_increment_val(mem)						\
+	({													\
+		MVEE_PREOP(CATOMIC_INCREMENT, mem, 1);			\
+		typeof(*mem) ____result = orig_catomic_increment_val(mem);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+#define catomic_decrement(mem)					\
+	({											\
+		MVEE_PREOP(CATOMIC_DECREMENT, mem, 1);	\
+		orig_catomic_decrement(mem);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_decrement_val(mem)						\
+	({													\
+		MVEE_PREOP(CATOMIC_DECREMENT, mem, 1);			\
+		typeof(*mem) ____result = orig_catomic_decrement_val(mem);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+
+#define catomic_and(mem, mask)					\
+	({											\
+		MVEE_PREOP(CATOMIC_AND, mem, 1);		\
+		orig_catomic_and(mem, mask);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_or(mem, mask)					\
+	({											\
+		MVEE_PREOP(CATOMIC_OR, mem, 1);			\
+		orig_catomic_or(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_max(mem, value)					\
+	({											\
+		MVEE_PREOP(CATOMIC_MAX, mem, 1);		\
+		orig_catomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+
+//
+// generic C11-style atomics (include/atomic.h)
+//
+#define atomic_load_relaxed(mem)					\
+	({												\
+		MVEE_PREOP(ATOMIC_LOAD, mem, 0);			\
+		__typeof(*mem) ____result = orig_atomic_load_relaxed(mem);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define atomic_load_acquire(mem)					\
+	({												\
+		MVEE_PREOP(ATOMIC_LOAD, mem, 0);			\
+		__typeof(*mem) ____result = orig_atomic_load_acquire(mem);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define atomic_store_relaxed(mem, val)			\
+	(void)({									\
+		MVEE_PREOP(ATOMIC_STORE, mem, 1);		\
+		orig_atomic_store_relaxed(mem, val);	\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_store_release(mem, val)			\
+	(void)({									\
+		MVEE_PREOP(ATOMIC_STORE, mem, 1);		\
+		orig_atomic_store_release(mem, val);	\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_compare_exchange_weak_relaxed(mem, expected, desired)	\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		bool __result = orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_compare_exchange_weak_acquire(mem, expected, desired)	\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		bool __result = orig_atomic_compare_exchange_weak_acquire(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_compare_exchange_weak_release(mem, expected, desired)	\
+	({																	\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		bool __result = orig_atomic_compare_exchange_weak_release(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_exchange_relaxed(mem, desired)						\
+	({																\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_exchange_relaxed(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_exchange_acquire(mem, desired)						\
+	({																\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_exchange_acquire(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_exchange_release(mem, desired)						\
+	({																\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_exchange_release(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_relaxed(mem, operand)						\
+	({																\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_fetch_add_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_acquire(mem, operand)						\
+	({																\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_fetch_add_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_release(mem, operand)						\
+	({																\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_fetch_add_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_acq_rel(mem, operand)						\
+	({																\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_fetch_add_acq_rel(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_relaxed(mem, operand)						\
+	({																\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_fetch_and_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_acquire(mem, operand)						\
+	({																\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_fetch_and_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_release(mem, operand)						\
+	({																\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_fetch_and_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+
+#define atomic_fetch_or_relaxed(mem, operand)						\
+	({																\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_fetch_or_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_or_acquire(mem, operand)						\
+	({																\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_fetch_or_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_or_release(mem, operand)						\
+	({																\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_fetch_or_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_xor_release(mem, operand)						\
+	({																\
+		MVEE_PREOP(ATOMIC_FETCH_XOR, mem, 1);						\
+		typeof(*mem) ____result = orig_atomic_fetch_xor_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+//
+// MVEE additions
+//
+#define THREAD_ATOMIC_GETMEM(descr, member)			\
+	({												\
+		MVEE_PREOP(ATOMIC_LOAD, &descr->member, 1);	\
+		__typeof(descr->member) ____result = THREAD_GETMEM(descr, member);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define THREAD_ATOMIC_SETMEM(descr, member, val)			\
+	(void)({												\
+			MVEE_PREOP(ATOMIC_STORE, &descr->member, 1);	\
+			THREAD_SETMEM(descr, member, val);				\
+			MVEE_POSTOP();									\
+		})
+
+//
+// sys_futex with FUTEX_WAKE_OP usually overwrites the value of the futex.
+// We have to make sure that we include the futex write in our sync buf ordering
+//
+#define lll_futex_wake_unlock(futexp, nr_wake, nr_wake2, futexp2, private) \
+	({																	\
+		long int __ret;													\
+		MVEE_PREOP(___UNKNOWN_LOCK_TYPE___, futexp2, 1);				\
+		__ret = orig_lll_futex_wake_unlock (futexp, nr_wake,    \
+			nr_wake2, futexp2, private);  \
+		if (mvee_should_futex_unlock())									\
+		{																\
+			*futexp2 = 0;												\
+		}																\
+		MVEE_POSTOP();													\
+		__ret;									\
+	})
+
+# define lll_mvee_futex_timed_wait(futexp, val, timeout, private)     \
+  lll_futex_syscall (4, futexp,                                 \
+		     __lll_private_flag (mvee_should_sync_tid() ? MVEE_FUTEX_WAIT_TID : FUTEX_WAIT, private),  \
+		     val, timeout)
+
+/* Wait while *FUTEXP == VAL for an lll_futex_wake call on FUTEXP.  */
+# define lll_mvee_futex_wait(futexp, val, private) \
+  lll_mvee_futex_timed_wait (futexp, val, NULL, private)
+
+/* Like lll_futex_wait, but acting as a cancellable entrypoint.  */
+# define lll_futex_mvee_wait_cancel(futexp, val, private) \
+  ({                                                                   \
+    int __oldtype = CANCEL_ASYNC ();				       \
+    long int __err = lll_mvee_futex_wait (futexp, val, LLL_SHARED);	       \
+    CANCEL_RESET (__oldtype);					       \
+    __err;							       \
+  })
+
+/* Like lll_futex_timed_wait, but acting as a cancellable entrypoint.  */
+# define lll_futex_mvee_timed_wait_cancel(futexp, val, timeout, private) \
+  ({									   \
+    int __oldtype = CANCEL_ASYNC ();				       	   \
+    long int __err = lll_mvee_futex_timed_wait (futexp, val, timeout, private); \
+    CANCEL_RESET (__oldtype);						   \
+    __err;								   \
+  })
+
+#endif // !IS_IN (rtld)
+
 #endif /* atomic-machine.h */
diff --git a/sysdeps/x86_64/multiarch/Makefile b/sysdeps/x86_64/multiarch/Makefile
index 395e432c09..3d388036c5 100644
--- a/sysdeps/x86_64/multiarch/Makefile
+++ b/sysdeps/x86_64/multiarch/Makefile
@@ -1,5 +1,6 @@
 ifeq ($(subdir),csu)
 tests += test-multiarch
+sysdep_routines += mvee_infinite_loop
 endif
 
 ifeq ($(subdir),string)
diff --git a/sysdeps/x86_64/mvee-totalpartial-agent.h b/sysdeps/x86_64/mvee-totalpartial-agent.h
new file mode 100644
index 0000000000..2381e5e480
--- /dev/null
+++ b/sysdeps/x86_64/mvee-totalpartial-agent.h
@@ -0,0 +1,109 @@
+//
+// MVEE_PARTIAL_ORDER_REPLICATION: when defined, slaves will use
+// queue projection to replay synchronization operations in
+// partial order rather than total order. In other words,
+// the slaves will only respect the order in which the master
+// has performed its synchronization operations on a per-word
+// basis
+//
+#define MVEE_PARTIAL_ORDER_REPLICATION
+//
+// MVEE_LOG_EIPS: when defined, libc logs return addresses for
+// all locking operations into a seperate queue
+//
+// WARNING: enabling EIP logging _CAN_ trigger crashes! We're
+// using __builtin_return_address(2) to fetch the eip of the 
+// caller of the locking function. Unfortunately, libc uses inline
+// __libc_lock_* operations every now and then. When it does, 
+// the __builtin_... call will return the wrong caller and in some
+// cases (e.g. in do_system) it might try to fetch the eip beyond
+// the end of the stack!
+//
+#define MVEE_LOG_EIPS
+#define MVEE_STACK_DEPTH 5
+//
+// MVEE_CHECK_LOCK_TYPE: if this is defined, the slave will check
+// whether or not it's replaying a lock of the same type
+// (only works with the extended queue)
+//
+#define MVEE_CHECK_LOCK_TYPE
+//
+// MVEE_DEBUG_MALLOC: if this is defined, the slaves will check whether
+// their malloc behavior is synced with the master
+//
+#define MVEE_DEBUG_MALLOC
+
+//
+// Latest version of the replication buffer layout:
+//
+// struct mvee_buffer_info for variant <0>
+// ...
+// struct mvee_buffer_info for variant <N>
+// struct mvee_buffer_entry for replicated operation <0>
+// ...
+// struct mvee_buffer_entry for replication operation <number of requested slots - 1>
+//
+
+//
+// The callstack buffer is separate and is just laid out like:
+//
+// struct mvee_callstack_entry for replicated operation <0> in variant <0>
+// ...
+// struct mvee_callstack_entry for replicated operation <0> in variant <N>
+// ...
+// ...
+// struct mvee_callstack_entry for replicated operation <number of requested slots - 1> in variant <0>
+// ...
+// struct mvee_callstack_entry for replicated operation <number of requested slots - 1> in variant <N>
+//
+
+struct mvee_buffer_info
+{
+	// The master must acquire this lock before writing into the buffer
+	volatile int lock;
+    // In the master, pos is the index of the next element we're going to write
+    // In the slave, pos is the index of the first element that hasn't been replicated yet
+	volatile unsigned int pos;
+	// How many elements fit inside the buffer?
+	// This does not include the position entries
+	unsigned int size;
+    // How many times has the buffer been flushed?
+    volatile unsigned int flush_cnt;
+    // Are we flushing the buffer right now?
+    volatile unsigned char flushing;
+	// Type of the buffer. Must be MVEE_LIBC_LOCK_BUFFER or MVEE_LIBC_LOCK_BUFFER_PARTIAL
+	unsigned char buffer_type;
+	// Pad to the next cache line boundary
+	unsigned char padding[64 - sizeof(int) * 4 - sizeof(unsigned char) * 2];
+};
+
+struct mvee_buffer_entry
+{
+	// the memory location that is being accessed atomically
+	unsigned long word_ptr;
+	// the thread id of the master variant thread that accessed the field
+	unsigned int master_thread_id;
+	// type of the operation
+	unsigned short operation_type;
+	// Pad to the next cache line boundary. We use this to write tags in the partial order buffer
+	unsigned char tags[64 - sizeof(long) - sizeof(int) - sizeof(short)];
+};
+
+struct mvee_callstack_entry
+{
+    // might be zero
+    unsigned long callee[MVEE_STACK_DEPTH];
+};
+
+extern void mvee_invalidate_buffer      (void);
+extern void mvee_atomic_postop_internal (unsigned char preop_result);
+extern int  mvee_should_sync_tid        (void);
+extern int  mvee_all_heaps_aligned      (char* heap, unsigned long alloc_size); 
+extern void mvee_xcheck                 (unsigned long item);
+
+#define MVEE_POSTOP() \
+  mvee_atomic_postop_internal(__tmp_mvee_preop);
+
+extern unsigned char     mvee_atomic_preop_internal (unsigned short op_type, void* word_ptr);
+#define MVEE_PREOP(op_type, mem, is_store)					\
+	register unsigned char __tmp_mvee_preop = mvee_atomic_preop_internal(op_type, (void*)mem);
diff --git a/sysdeps/x86_64/mvee-woc-agent.h b/sysdeps/x86_64/mvee-woc-agent.h
new file mode 100644
index 0000000000..8d3ffa1ff8
--- /dev/null
+++ b/sysdeps/x86_64/mvee-woc-agent.h
@@ -0,0 +1,16 @@
+#define MVEE_MAX_COUNTERS 65536
+
+#define MVEE_MALLOC_HOOK(type, msg, sz, ar_ptr, chunk_ptr)
+
+extern void          mvee_atomic_postop_internal (unsigned char preop_result);
+extern unsigned char mvee_atomic_preop_internal  (volatile void* word_ptr);
+extern int           mvee_should_sync_tid        (void);
+extern int           mvee_all_heaps_aligned      (char* heap, unsigned long alloc_size); 
+extern void          mvee_invalidate_buffer      (void);
+extern unsigned char mvee_should_futex_unlock    (void);
+
+#define MVEE_POSTOP()								\
+	mvee_atomic_postop_internal(__tmp_mvee_preop);
+
+#define MVEE_PREOP(op_type, mem, is_store)								\
+	register unsigned char  __tmp_mvee_preop = mvee_atomic_preop_internal((volatile void*)mem);
diff --git a/sysdeps/x86_64/mvee_infinite_loop.S b/sysdeps/x86_64/mvee_infinite_loop.S
new file mode 100644
index 0000000000..4c1e226e0e
--- /dev/null
+++ b/sysdeps/x86_64/mvee_infinite_loop.S
@@ -0,0 +1,39 @@
+/* 
+ * mvee_infinite_loop:
+ * this function is used for both thread transfering and signal delivery 
+ * 
+ * 1) to transfer threads to a new monitor, the original monitor (i.e. the 
+ * monitor that monitors the childs that instigated the fork event) needs to
+ * detach from the threads first. While the threads are detached, they can
+ * run freely, without the intervention of a debugger.
+ * As such, we have to move the program counter to an infinite loop while
+ * the threads are detached. This way, the threads will all be in an equivalent
+ * state when the new monitor attaches to them.
+ * Because we're going to replace the registers by their original contents
+ * when the new monitor attaches, we can use sys_pause calls in the infinite
+ * loop.
+ * 
+ * 2) delivering signals through the ptrace API happens asynchronously 
+ * (I found out the hard way). As such, we should wait for the threads to be
+ * in equivalent states (e.g. stopped on the same syscall). Then the registers
+ * should be backed up and the syscall nr should be replaced by a harmless
+ * syscall that doesn't modify the program state. We use sys_getpid for this
+ * purpose. When that replaced syscall returns, we change the pc to this
+ * infinite loop while we wait for async signal delivery.
+ * We probably cannot use syscalls while waiting for signal delivery. 
+ * One possible exception is sys_sched_yield. Our modified MVEE kernel does
+ * not report this syscall to the ptracer
+ * 
+ * the with_syscalls parameter is passed through the ecx register!
+ */
+
+		.globl mvee_infinite_loop
+		.type mvee_infinite_loop,@function
+
+mvee_infinite_loop:
+		nop
+		nop
+		nop
+		nop
+		nop
+		jmp mvee_infinite_loop
diff --git a/sysdeps/x86_64/nptl/pthread_spin_lock.S b/sysdeps/x86_64/nptl/pthread_spin_lock.S
deleted file mode 100644
index 1476040ce8..0000000000
--- a/sysdeps/x86_64/nptl/pthread_spin_lock.S
+++ /dev/null
@@ -1,34 +0,0 @@
-/* Copyright (C) 2012-2020 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <https://www.gnu.org/licenses/>.  */
-
-#include <lowlevellock.h>
-#include <sysdep.h>
-
-ENTRY(pthread_spin_lock)
-1:	LOCK
-	decl	0(%rdi)
-	jne	2f
-	xor	%eax, %eax
-	ret
-
-	.align	16
-2:	rep
-	nop
-	cmpl	$0, 0(%rdi)
-	jg	1b
-	jmp	2b
-END(pthread_spin_lock)
diff --git a/sysdeps/x86_64/nptl/pthread_spin_trylock.S b/sysdeps/x86_64/nptl/pthread_spin_trylock.S
deleted file mode 100644
index 24981fe717..0000000000
--- a/sysdeps/x86_64/nptl/pthread_spin_trylock.S
+++ /dev/null
@@ -1,37 +0,0 @@
-/* Copyright (C) 2002-2020 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <https://www.gnu.org/licenses/>.  */
-
-#include <pthread-errnos.h>
-#include <sysdep.h>
-
-
-#ifdef UP
-# define LOCK
-#else
-# define LOCK lock
-#endif
-
-ENTRY(pthread_spin_trylock)
-	movl	$1, %eax
-	xorl	%ecx, %ecx
-	LOCK
-	cmpxchgl %ecx, (%rdi)
-	movl	$EBUSY, %eax
-	cmovel	%ecx, %eax
-	retq
-END(pthread_spin_trylock)
diff --git a/sysdeps/x86_64/nptl/pthread_spin_unlock.S b/sysdeps/x86_64/nptl/pthread_spin_unlock.S
deleted file mode 100644
index ae5c4d889d..0000000000
--- a/sysdeps/x86_64/nptl/pthread_spin_unlock.S
+++ /dev/null
@@ -1,29 +0,0 @@
-/* Copyright (C) 2002-2020 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <https://www.gnu.org/licenses/>.  */
-
-#include <sysdep.h>
-
-ENTRY(pthread_spin_unlock)
-	movl	$1, (%rdi)
-	xorl	%eax, %eax
-	retq
-END(pthread_spin_unlock)
-
-	/* The implementation of pthread_spin_init is identical.  */
-	.globl	pthread_spin_init
-pthread_spin_init = pthread_spin_unlock
